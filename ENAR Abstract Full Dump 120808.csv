username,salutation,realname,contacttitle,workaffiliation,address1,phone,fax,email,Talk_Title,presentingauthor,presenter1Firstname,presenter1middle,presenter1lastname,presenter1affiliation,presenter2Firstname,presenter2middle,presenter2lastname,presenter2affiliation,presenter3Firstname,presenter3middle,presenter3lastname,presenter3affiliation,presenter4Firstname,presenter4middle,presenter4lastname,presenter4affiliation,presenter5Firstname,presenter5middle,presenter5lastname,presenter5affiliation,presenter6Firstname,presenter6middle,presenter6lastname,presenter6affiliation,presenter7Firstname,presenter7middle,presenter7lastname,presenter7affiliation,presenter8Firstname,presenter8middle,presenter8lastname,presenter8affiliation,presenter9Firstname,presenter9middle,presenter9lastname,presenter9affiliation,presenter10Firstname,presenter10middle,presenter10lastname,presenter10affiliation,abstract,IMS_Session,ENAR_ASA_IMS_Conflict,TutorialSpecified,COPSS_Conflict,SC_T_R_Conflict,OtherConflict,OtherConflictText,category,student_oral_poster,invitedpaper_organizer,specialcontributedpaper_organizer,overheadprojector,slideprojector_35mm,no_equipment,willing_to_serve_as_a_chair_for_a_session,FirstCategory,SecondCategory,OtherSpecifydon@podi.com,,test,test,test,test,test,test,don@podi.com,test,1,test,,test,testtesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttest,testtest,,test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"testtesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttesttest  testtesttesttesttesttest",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Multivariate methods,Clustered data methods,mgt26@georgetown.edu,,Mahlet G. Tadesse,Assistant Professor,Georgetown University,Department of Mathematics,202-687-1871,202-687-6067,mgt26@georgetown.edu,Identifying cluster structure and relevant variables in high-dimensional data sets,1,Mahlet,G,Tadesse,Georgetown University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the analysis of high-dimensional data there is often interest in uncovering cluster structure and identifying discriminating variables.  For example, the goal may be to use gene expression data to discover new disease subtypes and to determine genes with different expression levels between classes.  Another research question that is receiving increased attention is the problem of relating genomic data sets from various sources.  For instance, the goal may be to identify subsets of DNA sequence variations from SNP arrays or array CGH that are associated with changes in mRNA transcript abundance in a set of correlated genes.  We have proposed mixture models with variable selection to address these problems.  I will present the methods and illustrate their applications on various genomic data sets.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,TRUE,Tutorial: Mass spectrometry-based proteomics,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Genomics,marie@podi.com,,Marie,,PODI,jhg,3333333333,,marie@podi.com,Marie's Test,1,Marie,,Test,PODI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,My Abstract,FALSE,FALSE,T3: Statistical Analysis of Cost Effectiveness Data,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,"Biologics, pharmaceuticals, medical devices",Biomarkers/surrogate markers,info@podi.com,,test2,test2,test2,test2,2222222222,2222222222,info@podi.com,test2,1,yada,,yada,yada,yada,yada,yada,yada,yada,,,yada,yada,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda yadda ",TRUE,FALSE,,FALSE,FALSE,TRUE, yadda yadda,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Agreement,podi@podi.com,,test2,test2,test2,test2,2222222222,2222222222,podi@podi.com,test2,1,test2,,test2,test2test2test2,test2test2,test2,test2,test2test2test2test2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"test2test2test2test2test2test2test2test2test2test2test2test2test2test2",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Measurement error,Biostatistics Education,fake@podi.com,,fake,fake,fake,fake,2222222222,,fake@podi.com,fake,1,fake,fake,fake,fakefakefakefakefakefake,fakefake,fakefake,fakefake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"fakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefakefake",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Joint models for longitudinal and survival data,Adaptive design/adaptive randomization,meiyang@bu.edu,,Mei Yang,,Student,75 Gardner St. Apt #22,857-472-0072,,meiyang@bu.edu,Pharmacogenomics Meta-analysis - A Simulation Study,1,Mei,,Yang,"Department of Biostatistics, Boston University,Boston, MA",Meng,,Chen,"Translational Technology Statistics,Pfizer Global Research & Development,New London, CT",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For most currently performed pharmacogenomics studies, trials tend to be small with low power. Meta-analysis is often used to borrow information across related trials to dependably detect genetic associations. We studied the operating characteristics of different meta-analysis approaches in detecting genetic signals for binary safety outcomes under the dominant model. Data were simulated under three scenarios to represent circumstances with different levels of heterogeneity. Frequentist approaches (i.e. fixed/random effect model and cumulative meta-analysis) and Bayesian approaches (i.e. full Bayesian and empirical Bayes) were performed on each scenario. Under the moderate heterogeneity scenario, which is most common, we found Bayesian model gave narrower width of interval estimate for the genetic effect compared with the random effect model. And effect estimate for each trial was shrunken towards the overall mean. Furthermore, Bayesian models provide us the options to model early stage trials. If they are exchangeable to the later ones, Bayesian model with non-informative priors on all trials is more appropriate; if they provide useful information that needs to be incorporated, empirical Bayes model with priors formed from early trials is more appropriate; if they are totally irrelevant, Bayesian model with non-informative priors on later trials is more appropriate.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Statistical genetics,ronald@biostat.wisc.edu,,Ronald Gangnon,Asst Professor,University of Wisconsin,603 WARF Office Building,608-265-0688,,ronald@biostat.wisc.edu,Adjustments for Local Multiplicity with Scan Statistics,1,Ronald,E,Gangnon,University of Wisconsin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In most cluster detection problems, there are local differences in theextent of the multiplicity problem across the study region.  Forexample, using a fixed maximum geographic radius for clusters, urbanareas typically have many overlapping clusters, while rural areas haverelatively few.  The spatial scan statistic does not account for thsilocal multiplicity problem.  We describe two new spatially-varyingmultiplicity adjustments for spatial cluster detection, one based on anested Bonferroni adjustment (LASS-B) and one based on a local Gumbeldistribution approximation (LASS-G).  The performance of the spatialscan statistic, LASS-B and LASS-G in terms of unbiased clusterdetection under the null hypothesis is evaluated throughsimulation. These methods are then applied to both the well-known NewYork leukemia data and data on breast cancer incidence in Wisconsin.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Epidemiologic methods,pararaim@iup.edu,,Mavis Pararai,Assistant Professor,Indiana University of Pennsylvania,"Mathematics Department, Stright Hall",724-357-1281,724-357-7908,pararaim@iup.edu,Underreporting in the Generalized Poisson Regresssion Model,1,Mavis,,Pararai,Indiana University of Pennsylvania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Mavis PararaiIndiana University of PennsylvaniaUnderreporting in the Generalized Poisson Regression ModelABSTRACTThe generalized Poisson regression model has been used to model equi-, over- and under-dispersed count data. In many of these situations the assumption is that the response, the count, is reported without error. It is possible that the count maybe underreported or overreported. The Poisson regression model and the negative binomial regression model have been modified and used in modeling count data that is underreported. In this paper, the generalized Poisson regression model for underreported counts is developed. The parameters of the proposed model are estimated by the maximum likelihood method. We propose a score test to determine whether there is significant underreporting in the data in order to use of the generalized Poisson regression model for underreported counts as opposed to the ordinary generalized Poisson regression model. The generalized Poisson regression model is applied to data on number of sexual partners. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Generalized linear models,Measurement error,yangfeng@princeton.edu,,Yang Feng,,Princeton University,ORFE Building,6093566946,,yangfeng@princeton.edu,NETWORK EXPLORATION VIA THE ADAPTIVE LASSO AND SCAD PENALTIES,2,Jianqing,,Fan,Princeton University,Yang,,Feng,Princeton University,Yichao,,Wu,North Carolina State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,jiashun@stat.cmu.edu,,Jiashun Jin,,Carnegie Mellon University,Department of Statistics,4122689551,,jiashun@stat.cmu.edu,Higher Criticism Thresholding: optimal feature selection when features are week and rare,1,Jiashun,,Jin,Carnegie Mellon University ,David,L,Donoho,Stanford University ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For high-dimensional classification, an important approach is to use thresholding to select features.  However, it remains an open problem how to set the threshold.      In this paper,  we propose a new approach which sets the threshold  by Higher Criticism,  a recent  statistic  proposed  in Donoho and Jin (2004). We show that the Higher Criticism thresholding  is optimal for many classifiers and for many circumstances.  Comparison to recent classification methods (including the Least Centroid Shrunk and False Discovery Rate Thresholding (FDRT)) are investigated  both with simulated data and with microarray data. ",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Statistical genetics,fuentes@stat.ncsu.edu,,Montserrat Fuentes,Professor of Statistics,North Carolina State University,"Statistics Department, Box 8203 NCSU Box",9195151921,919 515 1169,fuentes@stat.ncsu.edu,Bayesian Variable Selection for Multivariate Spatially-Varying Coefficient Regression: application to physical activity during pregnancy,1,Montserrat,,Fuentes,NCSU,Brian,,Reich,NCSU,Amy,,Herring,UNC,,,,,,,,,,,,,,,,,,,,,,,,,,,,," For pregnant women, the American College of Obstetricians and Gynecologists currently recommends 30 minutes of moderate exercise on most days.  Epidemiologists, policy makers, and city planners are interested in whether characteristics ofthe physical environment in which women live and work have influence on physical activity levels during pregnancy.  We study the associations between physical activity and several factors including personal characteristics, meteorological/air quality variables, and neighborhood characteristics in pregnant women in four counties of North Carolina.  We simultaneously analyze six types of physical activity and investigate cross-dependencies between these activity types. Exploratory analysis suggests that the associations are different in different regions. Therefore we use a multivariate regression model with spatially-varying regression coefficients. This model includes a regression parameter for each covariate at each spatial location. For ourdata with many predictors, some form of dimension reduction is clearly needed. We introduce a spatial Bayesian variable selection procedure to identify subsets of important variables. Our stochastic search algorithm determines the probabilities that each covariate's effect is null, non-null but constant across space, and spatially-varying.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Bayesian methods,tebbs@stat.sc.edu,,Joshua M. Tebbs,Associate Professor,University of South Carolina,Department of Statistics,803-777-5163,803-777-4048,tebbs@stat.sc.edu,Inference for variance components in generalized linear mixed models for pooled binary response,1,Joshua,M,Tebbs,"Department of Statistics, University of South Carolina",Peng,,Chen,"Department of Statistics, University of South Carolina",Christopher,R,Bilder,"Department of Statistics, University of Nebraska-Lincoln",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We investigate likelihood-based tests for variance components in generalized linear mixed models for pooled binary response. Pooled binary responses are commonly observed in group testing applications, where individual specimens (such as blood or urine samples) are tested in pools. Our variance component tests can be used to assess heterogeneity among clusters (e.g., testing sites), while preserving the anonymity of individual subjects. We illustrate our methods using chlamydia and gonorrhea data collected by the state of Nebraska as part of the Infertility Prevention Project.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Generalized linear models,Clustered data methods,hming@umich.edu,,Ming Hu,,"Department of Biostatistics, University of Michiga","Department of Biostatistics, University of Michigan",734-763-4803,,hming@umich.edu,Query large scale microarray compendium datasets using a model-based Bayesian approach with variable selection,1,Ming,,Hu,"Center for Statistical Genetics, Department of Biostatistics, School of Public Health, University of Michigan, Ann Arbor, MI 48109-2029",Zhaohui,,Qin,"Center for Statistical Genetics, Department of Biostatistics, School of Public Health, University of Michigan, Ann Arbor, MI 48109-2029",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In microarray gene expression data analysis, it is often of interest to identify genes that share similar expression profiles with a particular gene such as a key regulatory protein. Multiple studies have been conducted using various correlation measures to identify co-expressed genes. While working well for small datasets, the heterogeneity introduced from increased sample size inevitably reduces the sensitivity and specificity of these approaches. We develop a model-based gene expression query algorithm built under the Bayesian model selection framework. It is capable of detecting co-expression profiles under a subset of samples/experimental conditions. In addition, it allows linearly transformed expression patterns to be recognized and is robust against sporadic outliers in the data. Both features are critically important for increasing the power of identifying co-expressed genes in large scale gene expression datasets. Our simulation studies suggest that this method outperforms existing correlation coefficients or mutual information-based query tools. When we apply this new method to the Escherichia coli compendium data, it identifies a majority of known regulons as well as novel potential target genes of numerous key transcription factors.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Microarray analysis,data miningcasanova@wfubmc.edu,,Ramon Casanova,Assistant Professor,Wake Forest University,Medical Center Blvd WC 2336,336-7168309,,casanova@wfubmc.edu,An approach to detect gene-gene interactions in SNP data based on probabilistic measures,1,Ramon,,Casanova,"Section on Statistical Genetic and Bioinformatics, Dept of Biostatistical Sciences, Wake Forest University Health  ",Josh,D.,Grab,"Section on Statistical Genetic and Bioinformatics, Dept of Biostatistical Sciences, Wake Forest University Health ",Miranda,C.,Marion,"Section on Statistical Genetic and Bioinformatics, Dept of Biostatistical Sciences, Wake Forest University Health ",Paula,S.,Ramos,"Section on Statistical Genetic and Bioinformatics, Dept of Biostatistical Sciences, Wake Forest University Health ",Jasmin,,Divers,"Section on Statistical Genetic and Bioinformatics, Dept of Biostatistical Sciences, Wake Forest University Health ",Carl,D.,Langefeld,"Section on Statistical Genetic and Bioinformatics, Dept of Biostatistical Sciences, Wake Forest University Health ",,,,,,,,,,,,,,,,,"There is increasing evidence that the origin of complex diseases is related to multiple genes, gene-gene interactions and gene-environment interactions. This has motivated the rapid development of analytic tools to efficiently detect these interactions.  This is a problem compounded by the huge size of genomic data sets and different sources of noise such as missing data, phenotypes, genetic heterogeneity, etc.  To deal with such a degree of complexity, methods from machine learning and information theory are becoming more popular in the field. Here we propose a new approach for detection of gene-gene interactions in SNP data based on information theory and probabilistic measures. The key idea behind our approach is to probe the data for correlation changes between cases and controls that may be related to the disease. We show using case-control simulated data, that these measures in some situations could outperform a nonlinear support vector machine (SVM) classifier. These techniques are easy to implement and computationally very efficient.  Applications to the large genome-wide association study and replication study in lupus (www.SLEGEN.org) will be discussed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Machine learning,psaha@u.washington.edu,,Paramita Saha,,"Department of Biostatistics, University of Washing",F-600 Health Sciences Building,206-225-3843,,psaha@u.washington.edu,Time-dependent Predictive Accuracy in the Presence of Competing Risks,1,Paramita,,Saha,"Department of Biostatistics, University of Washington",Patrick,J,Heagerty,"Department of Biostatistics, University of Washington",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Competing risks arise naturally in time-to-event studies.In this article we propose time-dependent accuracy measures for amarker when we have censored survival times and competing risks. Time-dependent versions of sensitivity or True Positive (TP) fractionnaturally correspond to consideration of either cumulative (orprevalent) cases that accrue over a fixed time period, oralternatively to incident cases that are observed among event-freesubjects at any select time. Time-dependent (dynamic) specificity (1 - False Positive  (FP)) can bebased on the marker distribution among event-free subjects. We extendthese definitions to incorporate cause of failure for competing risksoutcomes.  The proposed estimation for cause-specific cumulative TP/dynamic FP is based on the nearest neighbor estimation of bivariate distribution function of the marker and the event time. On the other hand,incident  TP/dynamic FPcan be estimated using a possibly non-proportional hazards Cox modelfor the cause-specific hazards and riskset reweighting of the markerdistribution. The proposed methods extend the time-dependentpredictive accuracy measures of Heagerty, Lumley, and Pepe (2000) andHeagerty and Zheng (2005).",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,ROC analysis,Survival analysis,fangx@ecu.edu,,Xiangming Fang,,East Carolina University,"Department of Bisotatistics, College of Allied Health Sciences",(252)744-6041,,fangx@ecu.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,liulei@virginia.edu,,Lei Liu,Assistant Professor,University of Virginia,3181 Hospital West,434-982-3364,434-243-5787,liulei@virginia.edu,A Flexible Two-Part Random Effects Model for Correlated Medical Costs,1,Lei,,Liu,"Division of Biostatistics and Epidemiology,Division of Public Health Sciences,University of Virginia",Mark,,Cowen,"Quality Institute, St. Joseph Mercy Health System",Robert,,Strawderman,"Department of Biological Statistics and Computational Biology, Cornell University",Tina,,Shih,"Section of Health Service Research, Department of Biostatistics, Division of Quantitative Sciences, M. D. Anderson Cancer Center, University of Texas",,,,,,,,,,,,,,,,,,,,,,,,,"In this paper, we propose a 'two-part' random effects model (Olsen and Schafer 2001; Tooze, Grunwald, and Jones 2002) for correlated medical cost data. Typically, medical cost data are right-skewed, involve a substantial proportion of zero values, and may exhibit heteroscedasticity. The proposed model specification consists of two generalized linear mixed models, linked together by correlated random effects.  Respectively, and conditionally on the random effects and covariates, we model the odds of cost being positive (Part I) and the mean cost (Part II) given that costs were actually incurred. The model is novel in that Part II assumes that observed costs follow a generalized gamma distribution with a scale parameter allowed to depend on covariates. The class of generalized gamma distributions is very flexible and includes the lognormal, gamma, inverse gamma and Weibull distributions as special cases. We demonstrate how to carry out estimation using the Gaussian quadrature techniques conveniently implemented in SAS Proc NLMIXED.  The proposed model is used to  analyze pharmacy cost data on 56,245 adult patients clustered within 239 physicians in a midwestern U.S. managed care organization.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Random effects,daniel.dalzin@envisionpharma.com,,TEST,,TEST,TEST,555-555-5555,,daniel.dalzin@envisionpharma.com,TEST,1,TEST,,TEST,TEST,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,TEST,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,jhchen@stat.ubc.ca,,Jiahua Chen,,University of British Columbia,6356 Agricultural Road,6048221848,,jhchen@stat.ubc.ca,Feature selection in GLM with Large Model Spaces,1,Jiahua,,Chen,"Department of Statistics, University of British Columbia",Zehua,,Chen,the National University of Singapore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In genome-wide association studies, hundreds of thousandssingle nucleotide polymorphisms are screened to identifythe ones that are most responsible to the genetic variation underinvestigation. At the same time, the sample size or thenumber or biological replications are at most in thousands.In such applications, the number of independent variablesfar exceeds the sample size. In such 'large-p-small-n'situations, the classical variable selection criteria such as BIC arefound far too liberal. The extended Bayes information criterionproposed by Chen and Chen (2008), in contrast, provideseffective control on false discovery rate while retainingcomparable positive discovery rate. Furthermore, thecriterion has been shown to be consistent at identifyingthe true set of variables in the normal linear regressionmodel. In this talk, we investigate the property of theextended Bayes information criterion under the generalizedlinear model. In particular, we show that under somemild conditions, the extended Bayes information criterionremains consistent.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Variable subset selection/model selection,Genomics,ganj@mailbox.sc.edu,,Jianjun(Kevin) Gan,,University of South Carolina,5002 Edward Lane,713-503-5684,,ganj@mailbox.sc.edu,Impact of Multi-level measurement errors in survey data,1,Jianjun,,Gan,"Department of Epi-Biostatistics, School of Public Health,University of South Carolina",Hongmei,,Zhang,"Department of Epi-Biostatistics, School of Public Health,University of South Carolina",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"It is widely accepted that both environmental and genetic factors are related to the causation of NTDs (neural tube defects).  Many Studies have been contributed to this area and demonstrated the effect of Folic Acid from daily supplement and the effect folate from food on NTD risk reduction. Recent studies also provided evidence of potential contribution from other micronutrients, for instances, dietary botaine and myo-inositol. However, the results vary dramatically from one study to another. Although factors related to experimental designs may account for some of the variations, we expect that much of the variation is at least in part due to measurement errors, because survey questionnaires are usually the only instrument used in these studies and responses to survey questions are likely to be biased. In survey questionnaires, typically there are two possible levels of measurement errors. One level is formed by participants recall bias when filling the questionnaires, and the other level is when a system summarizes the responses. Unfortunately, little attention has been paid on multi-level measurement error modeling. In this paper, we develop a multi-level measurement error model and further incorporate it into a Logistic regression model. Simulations are used to demonstrate the methods, evaluate the impact of measurement errors on the inferences of factor effects , and study the effect of different choices of random effect distributions.  Finally, we apply the method to data sets of CDC questionnaires and FFQ to evaluate the impact of measurement errors at different levels during the data collection process. We expect the findings will self-explain the importance of adjusting for measurement errors and thus benefit future data collection effort. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Hierarchical models,Bruce_Rodda@msn.com,,Bruce Rodda,,Strategic Statistical Consulting LLC,19590 Sandcastle Drive,1-512-264-9994,1-512-264-9995,Bruce_Rodda@msn.com,The role of the statistical consultant in strategic planning and development in the pharmaceutical and biotechnology industries,1,Bruce,E,Rodda,Strategic Statistical Consulting LLC and The University of Texas School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many statistical consultants play a role in the design of clinical trials and even more participate in the analysis of these trials.  However, a clinical trial is rarely performed in isolation, but is an element of a broader, more complete, corporate strategy.  The statistical consultant can play an important role in formulating and developing the strategy that is necessary to provide the framework for an efficient, focused, and successful set of trials.  This presentation will present and discuss opportunities available to a consulting statistician in this critically important responsibility.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Consulting,"Biologics, pharmaceuticals, medical devices",karenc2204@yahoo.com,,Karen Chiswell,Senior Statistician,GlaxoSmithKline,PO Box 13398,(919) 483-1862,,karenc2204@yahoo.com,,1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Trying to see how to submit abstract for an invited session.,FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Consulting,"Biologics, pharmaceuticals, medical devices",xlhuang@mdanderson.org,,Xuelin Huang,Associate Professor,The University of Texas MD Anderson Cancer Center,P.O. Box 301402,713-794-4172,713-563-4243,xlhuang@mdanderson.org,Using Short-Term Response Information to Facilitate Adaptive Randomization for Survival Clinical Trials,1,Xuelin,,Huang,The University of Texas MD Anderson Cancer Center,Jing,,Ning,The University of Texas MD Anderson Cancer Center,Yisheng,,Li,The University of Texas MD Anderson Cancer Center,Donald,A,Berry,The University of Texas MD Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,"Increased survival is a common goal of cancer clinical trials. Due to the long periods of observation and follow-up to assess patient survival outcome, it is difficult to use outcome-adaptive randomization in  these trials.  In practice, often information about a short-term response is quickly available during or shortly after treatment, and this short-term response is a good predictor for long-term survival. For example, complete remission of leukemia can be achieved and measured after a few cycles of treatment. It is a short-term response that is desirable for prolonging survival. We propose a new design for survival trials when such short-term response information is available. We use the short-term information to ``speed up' the  adaptation of the randomization procedure. We establish a connection between a short-term response and long-term survival through a Bayesian model, first by using prior clinical information, and then by dynamically updating the model according to information accumulated in the ongoing trial. Interim monitoring and final decision making are based upon inference on the primary outcome of survival.   The new design  can more effectively assign patients to the better treatment arms. We demonstrate these properties through simulation studies. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Clinical trials,geert.molenberghs@uhasselt.be,,Geert Molenberghs,Professor,Universiteit Hasselt & Katholieke Universiteit Leu,I-BioStat,+3211268205,+3211268299,geert.molenberghs@uhasselt.be,EVERY MISSING NOT AT RANDOM MODEL FOR INCOMPLETE DATA HAS GOT A MISSING AT RANDOM COUNTERPART WITH EQUAL FIT,1,Geert,,Molenberghs,"I-BioStat, Universiteit Hasselt and Katholieke Universiteit Leuven",Michael,G,Kenward,"Medical Statistics Unit, London School of Hygiene and Tropical Medicine, United Kingdom",Geert,,Verbeke,"I-BioStat, Universiteit Hasselt and Katholieke Universiteit Leuven",Caroline,,Beunckens,,Cristina,,Sotto,"I-BioStat, Universiteit Hasselt and Katholieke Universiteit Leuven",,,,,,,,,,,,,,,,,,,,,"Many models for incomplete data allow for MNAR missingness. Sensitivity to unverifiable modeling assumptions, has initiated a lot of work. A key issue is that an MNAR model is not fully verifiable from the data, rendering the empirical distinction between MNAR and MAR next to  impossible, unless one is prepared to accept the posited MNAR model in an unquestioning way. We show that empirical distinction between MAR and MNAR is not possible, since each MNAR model corresponds to exactly one MAR counterpart. Such a pair will produce different predictions of the unobserved outcomes, given the observed ones. This is true for selection, pattern-mixture, and shared-parameter models. We will focus on the latter.Theoretical considerations are supplemented with illustrations based on a clinical trial in onychomycosis and on the Slovenian Public Opinion survey. The implications for sensitivity analysis are discussed.Missing data can be seen as latent variables. Such a view allows extension of our results to other forms of coarsening, such as grouping and censoring. In addition, the technology applies to random effects models, where a parametric form for the random effects can be replaced by certain other parametric (and non-parametric) form, without distorting the models fit, latent classes, latent variables, etc.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Longitudinal data,berge319@umn.edu,,Tracy L Bergemann,Assistant Professor,University of Minnesota,420 Delaware St SE,612-625-9142,,berge319@umn.edu,Imputing Missing Data in Case-Parent Triad Studies,1,Tracy,L,Bergemann,University of Minnesota Division of Biostatistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A commonly used design in genetic association studies is thecase-parent triad design.  Generally, samples are drawn from anaffected offspring, manifesting a disease or phenotype of interest, aswell as from the parents.  The trio genotypes may be analyzed using avariety of available methods, but we focus on log-linear modelsbecause they test for genetic association and additionally estimatethe relative risks of transmission.  The models need to be modified toimpute missing genotypes.  Furthermore, instability in the parameterestimates can arise when certain kinds of genotype combinations do notappear in the dataset.In this research, we kill two birds with one stone.  We propose a newmethod to simultaneously impute missing genotype data and account forgenotype combinations with zero counts.  This approach solves azero-inflated Poisson (ZIP) regression likelihood.  The maximumlikelihood estimates yield relative risks and a likelihood ratio testdetermines the significance of genetic association.We compared the ZIP regression approach to previously proposed methodsin both simulation studies and a dataset investigating the risk oforofacial clefts.  The ZIP likelihood estimates relative risks withless bias than other methods.  Further, the new method preserves theappropriate type I error rate more carefully. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Missing data,mshardel@epi.umaryland.edu,,Michelle Shardell,,University of Maryland School of Medicine,660 W Redwood Street,4107068563,,mshardel@epi.umaryland.edu,Calculating sample size for studies with expected all-or-none nonadherence and selection bias,1,Michelle,D,Shardell,University of Maryland School of Medicine,Samer,S,El-Kamary,University of Maryland School of Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We develop sample size formulas for studies aiming to test mean differences between a treatment and control group when all-or-none nonadherence (noncompliance) and selection bias are expected.  Recently published work addressed the increased variances within groups defined by treatment assignment when nonadherence occurs, compared to the scenario of full adherence, under the assumption of no selection bias.  In this paper, we extend this work to allow selection bias in the form of systematic differences in means and variances between latent adherence subgroups.  We illustrate the approach by performing sample size calculations to plan clinical trials with and without pilot adherence data. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Power analysis/sample size,Clinical trials,zhengc@purdue.edu,,Cheng Zheng,,Purdue University,205 AIRPORT RD APT 4,7654138137,,zhengc@purdue.edu,Functional model for biomarker detection,1,Cheng,,Zheng,Purdue University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,yzhao@bios.unc.edu,,Yufan Zhao,,"Department of Biostatistics, University of North C",709M Audubon Lake Drive,(716) 816-8579,,yzhao@bios.unc.edu,Reinforcement learning design for cancer clinical trials,1,Yufan,,Zhao,"Department of Biostatistics, The University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, U.S.A.",Michael,R,Kosorok,"Department of Biostatistics, The University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, U.S.A.",Donglin,,Zeng,"Department of Biostatistics, The University of North Carolina at Chapel Hill, Chapel Hill, NC 27599, U.S.A.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We develop reinforcement learning trials for discoveringindividualized treatment regimens for life-threatening diseases suchas cancer. A temporal-difference learning method called Q-learning isutilized which involves learning an optimal policy from a singletraining set of finite longitudinal patient trajectories.Approximating the Q-function with time-indexed parameters can beachieved by using support vector regressions or extremely randomizedtrees. Within this framework, we demonstrate that the procedure canextract optimal strategies directly from clinical data without relyingon the identification of any accurate mathematical models, unlikeapproaches based on adaptive design. We show that reinforcementlearning has tremendous potential in clinical research because it canselect actions that improve outcomes by taking into account delayedeffects even when the relationship between actions and outcomes is notfully known. To support our claims, the methodology's practicalutility is illustrated in a simulation analysis. For future research,we will apply this general strategy to studying and identifying newtreatments for advanced metastatic stage IIIB/IV non-small cell lungcancer, which usually includes multiple lines of chemotherapy treatment.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Machine learning,leem2@upmc.edu,,Minjae Lee,,student,1043 N. Negley Ave. Apt 7,412-361-0513,,leem2@upmc.edu,Median regression for longitudinal biomarker measurements subject to detection limit,1,Kong,,Lan,"PhD, Assistant Professor, Department of Biostatistics, Graduate School of Public Health, University of Pittsburgh",Minjae,,Lee,"MS, Graduate student, Department of Biostatistics, Graduate School of Public Health, University of Pittsburgh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"It has become increasingly popular for medical researchers to investigate whether a certain biomarker is useful for the diagnosis and prognosis of a disease. The biomarker measurements are often left censored due to detection limits. Most exiting methods handled the censored observations with maximum likelihood estimation approach. The robust left-censored regression model based on the least absolute deviations (LAD) method has been presented mainly in the field of econometrics. We describe how the LAD approach can be applied to the longitudinal left censored data. We derive the asymptotic properties of the LAD estimators in the median regression model. We conduct a simulation study to evaluate our proposed method and use a dataset from a sepsis study of inflammatory biomarkers for demonstration.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Biomarkers/surrogate markers,Longitudinal data,mkchung@wisc.edu,,Moo K. Chung,Associate Professor,Department of Statistics and Medical Informatics,Waisman Center 437,608 2172452,608 2172452,mkchung@wisc.edu,Tiling Manifolds with Orthonormal Basis,1,Moo,K,Chung,University of Wisconsin-Madison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"One main obstacle in building a sophisticated parametric model along an arbitrary manifold is the lack of an easily available orthonormal basis. Although there are at least two numerical techniques available for constructing an orthonormal basis such as the Laplacian eigenfunction approach and the Gram-Smidth orthogonalization, they are computationally not so trivial and costly. We present a relatively simpler method for constructing an orthonormal basis for an arbitrary manifold by the concept of pullback operation on a preconstructed basis.As an application, we construct an orthonormal basis on amygdala surface of the brain. Then using the basis, we present a new variance reducing shape representation of amygdala compared to the traditional Fourier representation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Imaging,Nonparametric methods,inkyung.jung@gmail.com,,Inkyung Jung,Assistant Professor,University of Texas Health Science Center at San A,7703 Floyd Curl Dr. Mail Code 7933,210-567-0836,,inkyung.jung@gmail.com,A spatial scan statistic for multinomial data,1,Inkyung,,Jung,University of Texas Health Science Center at San Antonio,Martin,,Kulldorff,Harvard Medical School,Otukei,J,Richard,Makerere University (Kampala-Uganda),,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As a geographical cluster detection analysis tool, the spatial scan statistic has been developed for different types of data such as Bernoulli, Poisson, ordinal, exponential and normal. Another interesting data type is multinomial. For example, one may want to find clusters where the disease type distribution is statistically significantly different from the rest of the study region when there are different types of disease which have no ordinal structure. In this paper, we propose a spatial scan statistic for such data, which is useful for geographical cluster detection analysis for categorical data without any intrinsic order information. The proposed method is illustrated using meningitis data in two counties of UK and the performance of the method is evaluated through a simulation study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Epidemiologic methods,ibrahim@bios.unc.edu,,Joseph Ibrahim,Distinguished Professor,Department of Biostatistics,University of North Carolina,919-843-2715,919-966-3804,ibrahim@bios.unc.edu,Transformation Models with Gamma-Frailty for Multivariate Failure Times,1,Joseph,G,Ibrahim,"Department of Biostatistics, UNC",Donglin,,Zeng,"Department of Biostatistics, UNC",Qingxia,,Chen,"Department of Biostatistics, Vanderbilt University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a class of transformation models for multivariatefailure times. The class of transformation models generalize theusual gamma-frailty model and yields a marginally lineartransformation model for each failure time. Nonparametric maximumlikelihood estimation is used for inference. The maximumlikelihood estimators for the regression coefficients are shown to be consistent and asymptotically normal, and theirasymptotic variances attain the semiparametric efficiency bound.Simulation studies show that the proposed estimation procedure providesasymptotically efficient estimates and yields good inferentialproperties for small sample sizes. The method is illustratedusing real data from a cardiovascular study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Multivariate survival,Random effects,qin_yu@urmc.rochester.edu,,qin yu,,University of Rochester,"601 Elmwood Ave., Box 630",9193452680,,qin_yu@urmc.rochester.edu,Modeling Sensitivity and Speci…city with a Time-varying Reference Standard within a Longitudinal Setting,1,qin,,yu,"Department of Biostatistics and Computational Biology,University of Rochester",Wan,,Tang,"Department of Biostatistics and Computational Biology,University of Rochester",Sue,,Marcus,"Department of Psychiatry, Mount Sinai School of Medicine,",Yan,,Ma,"Department of Biostatistics and Computational Biology,University of Rochester",Xin,M,Tu,"Department of Biostatistics and Computational Biology,University of Rochester",,,,,,,,,,,,,,,,,,,,,"Diagnostic tests are used in a wide range of behavioral, medical,psychosocial, and health-care related research. Test sensitivity andspecificity are the most popular measures of accuracy for diagnostictests. Available methods for longitudinal study designs assume fixedgold or reference standards and as such do not apply to studies withdynamically changing reference standards, which are especially popularin psychosocial research. In this paper, we develop a novel approachto address missing data and other related issues for modelingsensitivity and specificity within such a time-varying referencestandard setting. The approach is illustrated with real data in sexualhealth research.keywords:Augmented inverse probability weighted estimate (AIPW),Bivariate monotone missing data pattern (BMMDP), Diagnostic test,Double robust estimate, Inverse probability weighted estimate (IPW),Missing data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Diagnostic and screening tests,Longitudinal data,hsamawi@georgiasouthern.edu,,Hani Samawi,Associate Professor,Georgia Southern University,Jiann-Ping Hsu College of Public Health,912 478 1345,,hsamawi@georgiasouthern.edu,On nonparametric tests for partially correlated data with application to public health issues,1,Hani,M,Samawi,"JPHCOPH, Georgia Southern University",Lili,,Yu,"JPHCOPH, Georgia Southern University",Robert,,Vogel,"JPHCOPH, Georgia Southern University",Laura,H,Gunn,"JPHCOPH, Georgia Southern University",,,,,,,,,,,,,,,,,,,,,,,,,"Correlated or matched data is frequently collected under many study designs in applied sciences such as the social, behavioral, economic, biological, medical, epidemiologic, health, public health, and drug developmental sciences.  Challenges with respect to availability and cost commonly occur with matching observational or experimental study subjects, thus researchers frequently encounter situations where the observed sample consists of a combination of correlated and uncorrelated data.  This paper discusses and proposes testing procedures to handle data when partially correlated data is available. Theoretical as well as numerical investigation will be provided. The proposed testing procedures will be applied to real data.  These procedures will be of special importance in meta-analysis where partially correlated data is a concern when combining results of various studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Nonparametric methods,Applied data analysis,vanderweele@uchicago.edu,,Tyler J. VanderWeele,Assistant Professor,University of Chicago,Department of Health Studies,773-834-2509,773-702-1979,vanderweele@uchicago.edu,"Controlled direct and mediated effects: definition, identification and bounds",1,Tyler,J,VanderWeele,University of Chicago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A new identification result for controlled direct effects is given for settings in which data is available for a set of variables that intercept all paths between a treatment and an outcome. Furthermore, in this setting it is possible to provide a definition not just for controlled direct effects but also for controlled indirect effects (or controlled mediated effects). Further results are given which provide bounds for controlled direct effects when the no-unmeasured-confounding assumptions required for the identification of these effects do not hold. Previous results concerning bounds for controlled direct effects rely on monotonicity relationships between the treatment and the outcome; the results presented in this paper instead assume that monotonicity relationships hold between the unmeasured confounding variable or variables and the treatment, mediator and outcome. The results on bounds for controlled direct effects are motivated by and applied to a problem concerning the effects of prenatal care on birth outcomes.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Epidemiologic methods,klee4@lsuhsc.edu,,Keunbaik Lee,Assistant Professor,Louisiana State University-Health Science Center,1615 Poydras St. Suite 1400,5045686087,,klee4@lsuhsc.edu,Analysis of Multivariate Longitudinal Binary Data using Marginalized Random Effects Models,1,Keunbaik,,Lee,Louisiana State University Health Science Center,Yongsung,,Joo,"Dongguk University, South Koera",Jae Keun,,Yoo,University of Louisville,JungBok,,Lee,Korea University,,,,,,,,,,,,,,,,,,,,,,,,,"Generalized linear models with random effects are often used toexplain the serial dependence of longitudinal categorical data.Marginalized random effects models (MREMs) permit likelihood-basedestimations of marginal mean parameters and also explain the serialdependence of longitudinal data. In this paper, we extend the MREMto accommodate multivariate longitudinal binary data using a newcovariance matrix with a Kronecker decomposition which easilyexplains both the serial dependence and time specific responsecorrelation. A maximum marginal likelihood estimation is proposedutilizing a Quasi-Newton algorithm with Quasi-Monte Carlointegration of the random effects. Our approach is applied toanalyze metabolic syndrome data from the Korean Genomic EpidemiologyStudy (KGES) for Korean adults.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Generalized linear models,yc632@columbia.edu,,Ken Cheung,,Columbia University,"722 West 168th Street, Room 641",212 305 3332,,yc632@columbia.edu,Sequential Implementation of Stepwise Procedures for Identifying the Maximum Tolerated Dose,1,Ken,,Cheung,Columbia University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this talk, I address the dose-finding problem in phase I clinical trials by a multiple test approach: step-down tests are used in an escalation stage, and step-up tests in a de-escalation stage in order to allow sequential dose assignments for ethical purposes. By formulating the estimation problem as a testing problem, the proposed procedures formally control the error probability of selecting an unsafe dose. In addition, we can control the probability of correctly selecting the maximum tolerated dose (MTD) under a parameter subspace where no toxicity probability lies in an interval bracketed by the target toxicity rate and an unacceptably high toxicity rate, the so-called ``indifference zone'. This frequentist property, which is currently lacking in the conduct of dose-finding trials in humans, is appealing from a regulatory viewpoint.  From a practical viewpoint, stepwise tests are simple and easy to understand, and the sequential implementation operates in a similar manner to the traditional algorithm familiarized by the clinicians.  Extensive simulations illustrate that our methods yield good and competitive operating characteristics under a wide range of scenarios with realistic sample size, and performs well even in situations other existing methods may fail, namely, when the dose-toxicity curve is flat up to the targeted MTD.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Clinical trials,fengrong-wei@uiowa.edu,,Fengrong Wei,,University of Iowa,150 Hawkeye Court,319-541-8418,,fengrong-wei@uiowa.edu,Variable Selection in High-Dimensional Varying Coefficient Models via the Adaptive Group Lasso,1,Fengrong Wei,,Wei,The University of Iowa,Jian,,Huang,The University of Iowa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Nonparametric varying coefficient models are important in studying the time-dependent effects of variables. In this paper, we propose an adaptive group Lasso approach  to variable selection and estimation in sparse, high-dimensional varying coefficient regression based on a spline approximation to the models. Under appropriate conditions, we show that the group Lasso selects a model of the right order of dimensionality, selects all variables whose coefficientsare greater than certain threshold level, and is estimation consistent. An interesting aspect of our results is that the logarithm of the number of variables can be of the same order as the sample size for certain random dependant designs. However, the group Lasso is in general not selection consistent and tends to also select variables that are not important in the model. We use the adaptive group Lasso to improve the selection results. We show that under suitable conditions, the adaptive group Lasso has an oracle selection property, in the sense that it can correctly select important variables with probability converging to one. In contrast, group Lasso do not possess such oracle property. Both methods are illustrated by simulation studies and a real data example.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Variable subset selection/model selection,lan@stat.umn.edu,,Lan Wang,Assistant Professor,University of Minnesota,"385 Ford Hall, School of Statistics",(612)6257843,,lan@stat.umn.edu,Weighted Wilcoxon-type Smoothly Clipped Absolute Deviation Method,1,Lan,,Wang,University of Minnesota,Runze,,Li,Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Shrinkage-type variable selection procedures have recently seen increasing applications in biomedical research. However, their performance can be adversely influenced by outliers in either the response or the covariate space. This paper proposes a weighted Wilcoxon-type smoothly clipped absolute deviation (WW-SCAD) method, which deals with robust variable selection and robust estimation simultaneously. The new procedure can be conveniently implemented with the statistical software R. We establish that the WW-SCADcorrectly identifies the set of zero coefficients with probability approaching one and estimates the nonzero coefficients with the rate $n^{-1/2}$. Moreover, with appropriately chosen weights the WW-SCAD is robust with respect to outliers in both the x and y directions. The important special case with constant weights yields an oracle-type estimator with high efficiency at the presence of heavier-tailed random errors. The robustness of the WW-SCAD is partly justified by its asymptotic performance under local shrinking contamination. We propose a BIC-type tuning parameter selector for theWW-SCAD. The performance of the WW-SCAD is demonstrated via simulations and by an application to a study that investigates the effects of personal characteristics and dietary factors on plasma beta-carotene level.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Variable subset selection/model selection,Nonparametric methods,bzhou@bios.unc.edu,,Bingqing Zhou,,University of North Carolina at Chapel Hill,Biostatistics,919-260-0282,,bzhou@bios.unc.edu,Competing Risks Regression for Stratified Data,1,Bingqing,,Zhou,"Department of Biostatistics, University of North Carolina at Chapel HillChapel Hill, N.C. 27599-7420, U.S.A",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For competing risks data, Fine and Gray (1999) proportional hazardsmodel for subdistribution has gained popularity in its convenience indirectly assessing the effect of covariates on the cumulativeincidence function. However, in many important applications,proportional hazards may not be satisfied, including multicenterclinical trials, where the baseline subdistribution hazards may not becommon due to varying patient populations. In this article, weconsider a stratified competing risks regression, to allow thebaseline hazard to vary across levels of the stratification covariate.According to the relative size of the number of strata and stratasizes, two stratification regimes are considered. Using partiallikelihood and weighting techniques, we obtain consistent estimatorsof regression parameters. The corresponding asymptotic distributionsare provided for the two regimes separately, along with variousestimation techniques. Data from a breast cancer clinical trial andfrom a bone marrow transplantation registry illustrate the potentialutility of the stratified Fine-Gray model.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Cancer applications,banks@stat.duke.edu,,David Banks,Professor,Duke University,"Dept. of Stat. Science, Box 90251",919-684-3743,,banks@stat.duke.edu,Statistical Issues in Metabolomics,1,David,L.,Banks,Duke University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Statistical analysis of metabolomics data poses significant challengesin multivariate metrology.  This talk lays out the measurement issues,and describes how one can create uncertainty budgets and makecross-platform inferences using the Mandel bundle-of-lines model.  Italso addresses  the problem of data mining for signal detection in twoapplications, recommending Random Forests as a tool that works welland that seems heuristically apt.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Metabolomics,Data mining/massive data sets,bondell@stat.ncsu.edu,,Howard D. Bondell,,NC State University,Department of Statistics,919-515-1914,,bondell@stat.ncsu.edu,A Penalized Likelihood Approach to Haplotype Specific Analysis,2,Jung-Ying,,Tzeng,NC State University,Howard,D,Bondell,NC State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Haplotypes can hold key information to understand the role of candidategenes in disease etiology. While many existing methods are available forstudying haplotype effects at either the global or individual levels,few provide systematic evaluation on the pattern and structure of the haplotype effects. In most work, haplotype inference focuses on relative effects compared to a baseline haplotype. Ideally, all haplotype effects should be compared to determine a group structure among the haplotypes. This can be done as a secondary post-hoc analysis on pairwise differences as in ANOVA. However, this combined analysis often cannot identify the appropriate structure and tends to lack power. To resolve these issues, we propose a penalized likelihood approach using an L1 penalty on the pairwise differences of haplotype effects. The proposed method treats haplotypes as a factor of multiple levels without a pre-determined baseline level. It simultaneously carries out the effect estimation and comparison of all haplotypes, and outputs the haplotype group structure based on their effect sizes. We use simulation studies to demonstrate the informativeness and power of the proposed method, and to illustrate that the method can better identify the haplotype effect structure than the traditional haplotype association methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Statistical genetics,Invited Sessionhzhu@bios.unc.edu,,Hongtu Zhu,Associate Professor,"UNC-Chapel Hill, Department of Biostatistics","2105 C, MaGavran Greenberg Hall",919-9667272,919-9663804,hzhu@bios.unc.edu,Statistical Analysis of  Brain Morphometric Measures  on Riemannian Manifold,1,Hongtu,,Zhu,"Department of Biostatistics, UNC-Chapel Hill",Joseph G,,Ibrahim,"Department of Biostatistics, UNC-Chapel Hill",Yimei,,Li,"Department of Biostatistics, UNC-Chapel Hill",Weili,,Lin,"Department of Radiology, UNC-Chapel Hill",Yasheng,,Cheng,"Department of Radiology, UNC-Chapel Hill",,,,,,,,,,,,,,,,,,,,,"The aim of this talk is to develop an intrinsic regression model for the analysis ofbrain morphometric measures  as responses in a Riemannian manifold and their association with a set of covariates, such as age and gender, in a Euclidean space. The primary motivation and application of the proposed methodology is in medical imaging. Because some brain morphometric measures do not form a vector space, applying classical multivariate regression to modeling those measures may undermine their association with covariates of interest, such as age and gender, in real applications. Our intrinsic regression model, as a semiparametric model, uses a link function to map from the Euclidean space of covariates to Riemannian manifold. We develop an estimation procedure to calculate parameter estimates and establish their limiting distribution. We develop score statistics to test linear hypotheses of unknown parameters and develop a test procedure based on aresampling method to simultaneously assess the statistical significance of linear hypotheses across a large region of interest.  ",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Imaging,Estimating equations,jun.wu@bms.com,,Jun Wu,Principal Biostatistician,Bristol Myers Squibb,5 Research Parkway,2036776403,,jun.wu@bms.com,Log-rank test weight selection for hazard ratio with a change-point,1,Jun,,Wu,Bristol Myers Squibb,Howard,,Stratton,"School of Public Health, SUNY Albany",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The anti-cancer effect of immunotherapy is delayed because it needs to activate the immune system to mount cytotoxic attack.   Weighted log-rank family tests are typically used to compare overall survival with non-constant hazard ratios.  The power and size of tests with weight selection aided by change-point Cox model will be assessed via simulation based on FDA published data from Phase III clinical trials of a cancer vaccine.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,"Biologics, pharmaceuticals, medical devices",jinha@umich.edu,,Jinkyung Ha,,University of Michigan,1420 Washington Heights,248-953-1071,,jinha@umich.edu,Isotonic Estimation of Survival under a Misattribution of Cause of Death,1,Jinkyung,,Ha,"Biostatistics, School of Public Health, Univeristy of Michigan",Alex,,Tsodikov,"Biostatistics, School of Public Health, Univeristy of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The cause for the observed trends in prostate cancer mortality rates is unclear. Several authors have indicated that incorrectly classified cause of death has played a role in recent mortality trends. Here we consider a competing risks model under a misattribution of the cause of death.We first consider a naive approach using nonparametric maximum likelihood estimation (NPMLE), and then present the isotonic NPMLE. We study their small-sample and asymptotic properties. Contrary to the common belief that the isotonic issue is a small-sample problem, surprising observations were made in the continuous time setting. It is shown that the isotonic NPMLE is asymptotically biased as it achieves monotonicity. Other isotonic approaches, the supremum (SUP) method and the Pooled-Adjacent-Violators (PAV) algorithm and the EM algorithm, are also considered. We found that the EM algorithm is equivalent to the isotonic NPMLE. Both SUP method and PAV algorithm deliver consistent and asymptotically unbiased estimator. All methods were found to behave well asymptotically in the discrete time setting. Data from the Surveillance, Epidemiology and End Results (SEER) database are used to illustrate the proposed estimators.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Constrained estimation/order restricted inference,Survival analysis,xbli@ufl.edu,,Xiaobo Li,Research Assistant,University of Florida,390 Maguire Village apt 4,3528460894,,xbli@ufl.edu,Bayesian association testing of SNP markers and wood chemistry traits in clonal trials of loblolly pine,1,Xiaobo,,Li,University of Florida,Dudley,A.,Huber,University of Florida,George,,Casella,University of Florida,David,B.,Neale,"University of California, Davis",Gary,F.,Peter,University of Florida,,,,,,,,,,,,,,,,,,,,,"With the advance of the sequencing technology, single nucleotidepolymorphism (SNP) markers are now available for genome wideassociation testing in our loblolly pine population. We focus on theassociation testing using Hierarchical Bayesian Models for genome wideassociation for wood chemistry traits. A total number of 7,600 SNPs,which represents approximately 6,500 genes within the loblolly pinegenome sequenced for 999 genotypes, were used to test association.  Asmall set of 46 SNPs from the previous study was used to test themodel and simulation to test the model was done in this project.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Forestry/agriculture applications,fby@wharton.upenn.edu,,Frank B Yoon,,University of Pennsylvania,"Dept of Statistics, 400 JMHH",917-403-5410,,fby@wharton.upenn.edu,Using Multiple Control Groups as Evidence About Unobserved Biases in an Observational Study of Treatments for Melanoma,1,Frank,B,Yoon,"Department of Statistics, University of Pennsylvania",Phyllis,A,Gimotty,"Department of Clinical Epidemiology and Biostatistics, University of Pennsylvania",DuPont,,Guerry,"Department of Medicine, University of Pennsylvania",Paul,R,Rosenbaum,"Department of Statistics, University of Pennsylvania",,,,,,,,,,,,,,,,,,,,,,,,,"In an observational study of treatments effects, treated and controlsubjects may differ systematically prior to treatment, and there isinvariably concern that some important covariates were not measured,so that adjustments such as matching may fail to render thegroups comparable.  We present a simple example from an observationalstudy of a surgical treatment for melanoma that uses two controlgroups to provide some evidence about unmeasured biases.  Weillustrate a procedure for decomposing complex hypotheses into simplerhypotheses that are tested in order of priority.  This use of a secondcontrol group is `without cost' to the investigator, in the specificsense that inferences about the first control group are completedwithout loss of power to corrections for multiple testing; then, onlyif these initial results are promising, are further inferences madeusing the second control group.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Multiple testing,Cancer applications,jboyle@cdc.gov,,James P. Boyle,Mathematical Statistician,CDC,"4770 Buford HWY, NE, MS K-10",770-488-1268,,jboyle@cdc.gov,Model Checking for Bayesian Estimation of State Diabetes Incidence Rates,1,James,P.,Boyle,Centers for Disease Control,Betsy,L.,Cadwell,Centers for Disease Control,Theodore,J.,Thompson,Centers for Disease Control,Lawrence,,Barker,,,,,,,,,,,,,,,,,,,,,,,,,,"Direct design-based survey estimates of annual state incidence rates of diagnosed diabetes and their variances were obtained from the Behavioral Risk Factor Surveillance System (BRFSS). These estimates were for the eight years 1999 through 2006, for three age groups (20-44, 45-64, and 65+ years) and 51 states (including Washington D.C.). Of the 8(51)(3) = 1224 incidence rates, only 1011 were available because of missing survey data. Furthermore, many of the 1011 direct estimates were imprecise with only 87 estimates having a coefficient of variation < 20%. To improve  precision, a model-based approach applying Bayesian models to direct design-based estimates and their estimated variances as data was used. Several multilevel models were fit, ranging from very simple models with no covariates and independent errors to more complicated models with first order autoregressive time series errors and state level covariates from the U.S. Census Bureau. Models were ranked with the deviance information criterion (DIC). We checked the fit of the model with minimum DIC through simulated values from the posterior predictive distribution of replicated data (posterior predictive checking), and found the posterior predictive p-values acceptable.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Hierarchical models,upsattar@yahoo.com,,Abdus Sattar,Graduate Student Researcher,University of Pittsburgh,"305 S. Fairmount Street,",843-670-9611,412-3838956,upsattar@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,sinae@umich.edu,,Sinae Kim,,University of Michigan,Department of Biostatistics,734-936-1002,,sinae@umich.edu,Spike and Slab Dirichlet Prior for Bayesian Multiple Testing in Random Effects Models,1,Sinae,,Kim,"Department of BiostatisticsUniversity of Michigan",David,B.,Dahl,"Department of StatisticsTexas A&M University",Marina,,Vannucci,"Department of StatisticsRice University ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a method for multiple hypothesis testing in random effectsmodels that uses Dirichlet process (DP) priors for a nonparametrictreatment of the random effects distribution. We consider a generalmodel formulation which accommodate a variety of multiple treatmentconditions. A key feature of our method is the use of a product of``spike and slab' distributions as the centering distribution for theDP prior. Adopting spike and slab centering priors readilyaccommodates sharp null hypotheses and allows for the estimation ofthe posterior probabilities of such hypotheses. We demonstrate via asimulation study that our method yields increased sensitivity inmultiple testing hypothesis and produces a lower proportion of falsediscoveries than other competitive methods. In our application, themodeling framework allows simultaneously inference on the parametersgoverning differential expression and inference on the clustering ofgenes. We use experimental data on the transcriptional response tooxidative stress in mouse heart muscle and compare the results fromour procedure with existing Bayesian methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Genomics,qyu@lsuhsc.edu,,Qingzhao Yu,Assistant Professor,Louisiana State University Health Science Center,4831 Purdue Drive,504-568-6086,,qyu@lsuhsc.edu,Hierarchical Additive Modeling of Nonlinear Association with Spatial Correlations - An Application to Relate Alcohol Outlet Density and Neighborhood Assault Rates,1,Qingzhao,,Yu,LSUHSC,Bin,,Li,LSU,Richard,,Scribner,LSUHSC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Previous studies have suggested a link between alcohol outlets and assaults.  In this talk, we explore the effects of alcohol availability on assaults at the census tract level over time.  Several features of the data raise statistical challenges: (1) the association between covariates and the assault rates may be complex and therefore cannot be described using a linear model without covariates transformation; (2) the covariates may be highly correlated with each other; (3) there are a number of observations that have missing inputs; and (4) there is spatial association in assault rates at the census tract level.  We propose a hierarchical additive model, where the nonlinear correlations and the complex interaction effects are modeled using the multiple additive regression trees and the residual spatial association in the assault rates that cannot be explained in the model are smoothed using a Conditional Autoregressive (CAR) method. We develop a two-stage algorithm that connects the non-parametric trees with CAR to look for important covariates associated with the assault rates, while taking into account the spatial association of assault rates in adjacent census tracts.  The proposed method is applied to the Los Angeles assault data.  The method is compared with a hierarchical linear model.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Defense and national security applications,Data mining/massive data sets,dpc10@pitt.edu,,Dev Chakraborty,Associate Professor,University of Pittsburgh,"3520 Forbes Ave, Room 109",412-605-1553,412-605-1554,dpc10@pitt.edu,The case for FROC analysis,1,Dev,P,Chakraborty,"University of Pittsburgh, Department of Radiology",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This is a rebuttal to a recent paper (Acad. Radiol. 15:1312-1315,2008) that raises issues regarding the applicability of FROCmethodology to imaging systems evaluations.  Unlike many tests,diagnostic imaging provides information about the location(s) ofdisease, in addition to its presence or absence.  Unlike FROC, the ROCparadigm only considers the disease present or absent information anddisregards location.  I am responsible for JAFROC, a method foranalyzing FROC data.  JAFROC usage has been gaining acceptance butresistance to it is also increasing.  The authors have done a serviceby expressing their concerns publicly and I am grateful to ENAR forgiving me the opportunity to respond in open forum.  While the authorsnote several issues with FROC, with some of which I concur, I stronglydisagree with their position that ROC is clinically more relevant thanFROC.  Much of my response will focus on the clinical relevance issue. I will describe a specific scenario where ROC is less clinicallyrelevant than FROC.  Specific rebuttals to issues regardingapplicability to screening, data clustering, acceptance target, searchmodel, figure of merit, validation, etc, will be presented. It is myhope that the debate will influence those vested in the ROC method toembrace and contribute to FROC research, rather than feel threatenedby it.",FALSE,FALSE,T4: Receiver Operating Characteristic (ROC) Curves,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,ROC analysis,Clustered data methods,geert.verbeke@med.kuleuven.be,,"Verbeke, Geert",Prof.,I-BioStat,UZ Sint_Rafael,+32 16 336891,,geert.verbeke@med.kuleuven.be,The gradient function for checking goodness-of-fit of the random-effects distribution  in mixed models,1,Geert,,Verbeke,"I-BioStat, Katholieke Universiteit Leuven and Universiteit Hasselt, Belgium",Geert,,Molenberghs,"I-BioStat, Katholieke Universiteit Leuven and Universiteit Hasselt, Belgium",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Inference in mixed models is often based on the marginal distribution obtained from integrating out random effects over a pre-specified, often parametric, distribution. In this paper, we present the so-called gradient function as  a simple graphical diagnostic tool to assess whether the assumed random-effects distribution produces an adequate fit to the data, in terms of marginal likelihood. The method does not require any additional calculations in addition to the computations needed to fit the model, and can be applied to every type of mixed model (linear, generalized linear, non-linear), with univariate as well as multivariate random effects. The diagnostic value of the gradient function is extensively illustrated using some simulated examples, as well as in the analysis of a real longitudinal study with binary outcome values.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Not on wednesday, due to early flight. ",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Random effects,Longitudinal data,tlouis@jhsph.edu,,Thomas A. Louis,Professor,"Biostatistics, Johns Hopkins SPH",615 N. Wolfe Street,202-494-9331,410-955-0958,tlouis@jhsph.edu,DATA SHARING: AN EXAMPLE OF CONFLICTING INCENTIVES,1,Thomas,A,Louis,"Department of BiostatisticsJohns Hopkins Bloomberg SPH",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Requiring NIH grant and contract recipients who generate orconsolidate data to provide publicly available, documented datasets'within a reasonable time' entails potential benefits and drawbacksfor science and policy, and poses challenges for researchers and theresearch enterprise.  I conclude that properly implemented, on balancethe policy will be beneficial, but care is needed to realize thesebenefits.  The policy needs to specify time frames that allowresearchers to understand and document their datasets and to publishprincipal findings.  Funding and infrastructure must be in place toensure long term access; confidentiality must be protected; in somesituations user certification may be necessary.  Less direct, but noless important are ensuring that the requirements do not reduceengagement by the best researchers in high profile studies and thatreward systems in academe and elsewhere respect and reflect the newenvironment.  Potential benefits of the policy include energizingcultural and system changes such as increased standardization of datadefinitions and database configurations, and a movement towardsreproducible research.  Importantly, ready access will benefitresearchers as they formulate and design new studies and will increasethe quality and impact of individual studies and research syntheses.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Health policy applications,NIH Policyboluyede@georgiasouthern.edu,,Broderick O. Oluyede,Professor,Georgia Southern University,Department of Mathematical Sciences,(912)478-5427,,boluyede@georgiasouthern.edu,Some Results on Length-Biased and Current Duration Sampling,1,Broderick,O,Oluyede,"Department of Mathematical SciencesGeorgia Southern UniversityStatesboro, GA 30460",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the analysis of longitudinal data, two semiparametric models that are often used are the Cox proportional hazards model and the accelerated failure time model. In Cox proportional hazards model for failure time, one assumes that the covariate effect is captured via a proportional constant between hazard functions, with unspecified underlying hazard functions. In accelerated hazards model, the hazards functions are related via the scaletime change, which is a function of covariates and the parameters. In a medical setting, current duration sampling require knowledge of the duration of the disease of a group of patients up to the present, but length-biased sampling requires time needed to observe the full duration of the disease of the sampled patients. In this talk, some results on current duration and length-biased sampling for the accelerated failure time model and Cox proportional hazards model are presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Multivariate survival,Semiparametric (Length-Biased Sampling)xliao@hsph.harvard.edu,,Xiaomei Liao,Postdoc fellow,"Department of Biostatistics, Harvard University","10 Peabody terrace, Apt 31",617-4327121,617-566-7805,xliao@hsph.harvard.edu,Survival analysis with error prone time-varying covariates: a risk set calibration approach.,1,Xiaomei,,Liao,"Departments of Biostatistics, Harvard School of Public Health, Boston, MA.",David,,Zucker,"Department of Statistics, Hebrew University, Jerusalem, Israel.",Yi,,Li,"Departments of Biostatistics, Harvard School of Public Health, Boston, MA.",Donna,,Speigelman,"Departments of Epidemiology and Biostatistics, Harvard School of Public Health, Boston, MA.",,,,,,,,,,,,,,,,,,,,,,,,,"Occupational and environmental epidemiologists are often interestedin estimating the prospective effect of time-varying exposurevariables such as the cumulative exposure or average cumulative exposure,in relation tochronic disease endpoints such as cancer incidence and mortality.From exposure validation studies, it is apparent that many of these variablesare measured withmoderate to substantial error. Although the ordinary regressioncalibration approach is valid and efficient for measurement error correctionof relative risk estimates from the Cox model withtime-independent point exposures when the disease is rare, it is notadaptable for use with time-varyingexposures. By recalibrating withineach risk set, the risk set regression method is proposed forthis setting.An algorithm for a bias-corrected point estimate of the relativerisk using an  RRC approach is presented, followed by the derivation of an estimate of itsvariance, resulting in a sandwich estimator.Emphasis is on methods which apply to the main study/external validation studydesign. Simulation studies with different error modelsare carried out to show thevalidity and efficiency of the method, compared to the `naive' cox model, and themethod is applied to a study of diet and cancer from the Nurses' HealthStudy.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Measurement error,Epidemiologic methods,ymunoz@mtu.edu,,Yolanda Munoz-Maldonado,Assistant Professor in Statistics,Michigan Technological University,"Mathematical Sciences, 313 Fisher Hall",(906) 487-3172,(906) 487-3133,ymunoz@mtu.edu,Counterintuitive Results when Calculating Sample Size in ANOVA,1,Yolanda,,Munoz Maldonado,Michigan Technological University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"When designing experiments for testing differences between the levelsof two or more factors, it is usually advised to be parsimonious withthe number of selected treatments. It is assumed that this conventionwill help reduce the sample size required for a given significancelevel and power. We will present a case encountered during aconsulting project that contradicts this assumption. The choice oftreatments and the magnitude of the size effect play a key role inthis outcome. We will also comment on the consequences of thiscounterintuitive result in our daily statistical practice.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Experimental design,Sample Size Estimationtdjtdj@umich.edu,,Timothy D. Johnson,Research Associate Professor,"University of Michigan, Dept. of Biostatistics",1420 Washington Heights,(734) 936-1007,(734) 763-2215,tdjtdj@umich.edu,Spatial Point Process Modeling of Group fMRI Data,1,Timothy,D,Johnson,"University of Michigan,Department of Biostatistics",Thomas,E,Nichols,"Glaxco-Smith-Kline;University of Oxford, FMRIB; University of Michigan, Department of Biostatistics",Lei,,Xu,"Vanderbilt University,Department of Biostatistics",Tor,D,Wager,"Columbia University,Department of Psychology",,,,,,,,,,,,,,,,,,,,,,,,,"We propose a Bayesian hierarchical spatial model for multi-subject fMRI data. While there has been much work on univariate modeling of each voxel for single- and multi-subject data, and some work on spatial modeling for single-subject data, there has been virtually no work on spatial models that explicitly account for inter-subject variability in activation location. Most previous models uses Gaussian mixtures for the activation shape, at the first level, we use Gaussian mixtures for the probability that a voxel belongs to an activated region. Spatial correlation is accounted for in the mixing weights. At the second level mixture component means are clustered about individual activation centers and a priori are assumed to arise from a Cox cluster process. At the third level individual activation centers are clustered about population centers, again arising from a Cox cluster process. At the fourth level, population centers are a priori, modeled as a homogeneous Poisson process. Our approach incorporates the unknown number of mixture components and individual centers into the model as parameters whose posterior intensities are estimated by reversible jump Markov Chain Monte Carlo. We demonstrate our method on a recently published fMRI study. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Imaging,Bayesian methods,shanhong_guan@merck.com,,Shanhong Guan,Biometrician,Merck & Co.,351 N Sumneytown Pike,267-305-1272,267-305-6395,shanhong_guan@merck.com,On Global P-value Calculation in Multi-stage Designs,1,Shanhong,,Guan,Merck & Co.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In multi-stage clinical trials, test statistic can be constructed based on combination of the stage-wise p-values, assuming the condition of p-clud property is satisfied. In this paper, methods based on conditional error principle, such as direct combination of p-values and inverse-normal p-values, and methods based on significance levels of individualstages, are introduced. Numerical studies are conducted to evaluate their properties and some examples are provided to illustrate the application of these methods to multi-stage adaptive designs.Key words: Adaptive design; Conditional error function; Inverse-normal; p-value",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Multiple testing,Adaptive design/adaptive randomization,pashkevich_maksim@lilly.com,,Maksim Pashkevich,,Eli Lilly and Company,Lilly Corporate Center,(317) 433-6584,,pashkevich_maksim@lilly.com,Patient Focused Method for Assessing In Vitro Drug Combinations Using Growth Rates,1,Maksim,,Pashkevich,Eli Lilly and Company,Philip,,Iversen,Eli Lilly and Company,Harold,,Brooks,Eli Lilly and Company,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a new method that allows screening oncology drug combinations using data from in-vitro studies to select agents that have the promise of showing a synergistic effect in-vivo. In contrast to known approaches that define combination effect either on concentration scale or on percent inhibition scale, we use the growth rate of treated cells as a primary indicator of treatment activity. The developed method is based on a novel mathematical model that describes the growth of cancer cells that are subject to treatment with a combination of compounds. Mathematically, the model assumes a multi-compartment cell population with transition rates between compartments modeled according to biochemical reaction properties, and cells in each compartment growing according to exponential law. This translates to a linear system of ordinary differential equations, whose solution is accurately approximated by a closed-form expression using rapid equilibrium assumptions. Special cases of the aforementioned model represent situations when the combination effect is absent or when the considered drugs act as the same compound. Akaike information criterion and the likelihood ratio test are used to distinguish between different mechanisms of action for the considered compounds, and to test if a significant combination effect is being observed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Cancer applications,Biopharmaceutical research,usha@alum.bu.edu,,Usha Govindarajulu,,Harvard Medical School,"1620 Tremont St., OBC-3-12.7",617-525-7014,617-525-7752,usha@alum.bu.edu,The comparison of alternative smoothing methods for fitting non-linear exposure-response relationships with Cox models in a simulation study,1,Usha,S,Govindarajulu,Harvard Medical School,Betty,J,Malloy,American University,Bhaswati,,Ganguli,University of Calcutta,Donna,,Spiegelman,Harvard School of Public Health,Ellen,A,Eisen,"University of California, Berkeley; Harvard School of Public Health",,,,,,,,,,,,,,,,,,,,,"We examined the behavior of alternative smoothing methods for modeling environmental epidemiology data. Model fit can only be examined when the true exposure-response curve is known and so we used simulation studies to examine the performance of penalized splines (P-splines), restricted cubic splines (RCS), natural splines (NS), and fractional polynomials (FP). Survival data were generated under six plausible exposure-response scenarios with a right skewed exposure distribution, typical of environmental exposures. Cox models with each spline or FP were fit to simulated datasets. The best models, e.g. degrees of freedom, were selected using default criteria for each method. The root mean-square error (rMSE) and area difference were computed to assess model fit and bias (difference between the observed and true curves). The test for linearity was a measure of sensitivity and the test of the null was an assessment of statistical power. No one method performed best according to all four measures of performance, however, all methods performed reasonably well. The model fit was best for P-splines for almost all true positive scenarios, although fractional polynomials and RCS were least biased, on average.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Nonparametric methods,lyarborough@drohanmgmt.com,,Laura,,DMG,12100 Sunset Hills,(703) 234-4131,,lyarborough@drohanmgmt.com,X,1,SS,,ZZZ,DMG,TT,,YY,DMG,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"'. . . America cannot be strong abroad unless we are strong at home.People the world over have always been more impressed by the power ofour example than by the example of our power.'",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,"Biologics, pharmaceuticals, medical devices",Multivariate methods,aroy@utsa.edu,,Anuradha Roy,Associate Professor,The University of Texas at San Antonio,Department of Management Science and Statistics,210- 458-6343,210- 458-6350,aroy@utsa.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,cdpiano27@hotmail.com,,Carl DiCasoli,,North Carolina State University,"4108 Cross Creek Court,  Apartment H",(919) 741-9401,,cdpiano27@hotmail.com,On an Empirical Method for a Generalised Version of the Yang and Prentice Model,1,Carl,M,DiCasoli,North Carolina State University,Sujit,K,Ghosh,North Carolina State University,Subhashis,,Ghosal,North Carolina State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In survival data analysis, the proportional hazards (PH), accelerated failure time (AFT), and proportional odds models (POM) are commonly used semiparametric models for the comparison of survivability in subjects. These models assume that the survival curves do not cross. However, in some survival applications, the survival curves pertaining to the two groups of subjects under the study may cross each other. Hence, these three models stated above may no longer be suitable for making inference. Yang and Prentice (Biometrika, 2005 92(1):1-17) proposed a model which separately models the short-term and long-term hazard ratios generalising both PH and POM. This feature allows for the survival functions to cross. We study the estimation procedure in the Yang-Prentice model using the empirical likelihood approach. This method is extended to a regression version involving predictors, where the posterior sample is computed. Good properties of this method are also examined. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Empirical likelihood,gene.pennello@fda.hhs.gov,,Gene Pennello,"Mathematical Statistician,Team Leader",Food and Drug Administration,"1350 Piccard Drive, HFZ-550",240-276-3149,240-276-3131,gene.pennello@fda.hhs.gov,"David Duncan, A Retrospective (Session Title)",1,Gene,A,Pennello,Food and Drug Administration,Dennis,O,Dixon,"National Institutes of Health, NIAID",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Session abstract for contributed session, 'David Duncan: a Retrospective': David Duncan (1916-2006), distinguished Professor of Biostatistics at Johns Hopkins Bloomberg School of Public Health from 1960-1984, had a profound influence in the US and in Australia on the field of biostatistics.  His contributions include the discovery of logistic regression, the development of recursive estimation methods (e.g., dynamic estimation of the regression equation, i.e., the Kalman filter), and the development of multiple comparison procedures, his career long interest. His 1955 Biometrics paper, Multiple Range and Multiple F tests, is one of most highly cited in all of Statistics. It also laid the groundwork for his pioneering, Bayesian decision theoretic approach to multiple comparisons. The approach was honed in doctoral thesis work by many of his Ph.D. students to produce multiple comparison procedures for a variety of problems. David is remembered for his genuine concern for the training and career development of junior colleagues. His energy, enthusiasm, and engaging personality are missed. In this session, a retrospective will be given on David's contributions to the field of statistics and how his contributions relate to contemporary statistical research. Speakers and audience members are also invited to reflect on personal remembrances and anecdotes about David. ",FALSE,FALSE,,FALSE,FALSE,TRUE,"Two speakers in the session, Jay Herson and Karen Bandeen-Roche, can only participate if the session is held on Monday or on Tuesday before noon.",oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Multiple testing,Bayesian methods,padmans@wyeth.com,,S. Krishna Padmanabhan,Sr. Principal biostatistician,Wyeth Research,500 arcola Rd.,484 865 2084,,padmans@wyeth.com,Adaptive penalized D-optimal designs for dose finding for continuous bivariate outcomes,1,Krishna,,Padmanabhan,Wyeth Research,Francis,,Hsuan,Temple University,Vladimir,,Dragalin,Wyeth research,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"When a new drug is under development, a conventional dose-findingstudy involves learning about the dose-response curve in order tobring forward right doses of the drug to late-stage development. Wepropose an alternative adaptive design for dose-finding in clinicaltrials inthe presence of both (continuous) efficacy and toxicity endpoints. Weuse the principles of optimal experimental designs for bivariatecontinuous endpoints. However, instead of using the traditionalD-optimal design, which favors collective ethics but neglects theindividual ethics, we consider the penalized D-optimal design thatachieves an appropriate balance between the efficient treatment ofpatients in the trial and the precise estimation of the modelparameters to be used in the identification of the target dose.  Wealso show how to incorporate these penalty functions into theD-optimality criteria to build penalized optimal designs. This iscompared with the traditional fixed allocation design in terms ofallocation of subjects and precision of the identified dose-responsecurve and selection of the target dose.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Clinical trials,carmen-j-smith@uiowa.edu,,Carmen J. Smith,,University of Iowa,729 Michael St APT 75,3145802971,,carmen-j-smith@uiowa.edu,Determining presence of GB virus type C in HIV positive subjects,1,Carmen,J,Smith,University of Iowa,Kathryn,,Chaloner,University of Iowa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"GB virus type C (GBV-C) is a virus that appears to interact with HIV.   According to several studies, HIV positive individuals infected with GBV-C live longer than HIV positive individuals without GBV-C.   However, there is no gold standard for detecting the presence of GBV-C in blood samples.  One commercial test (Roche) and three locally developed ELISA tests (M5, M6, and GNA) were run on 100 stored blood samples in the UI Stapleton laboratory.  The objective is to investigate relationships between the tests and develop an algorithm for classifying samples as either positive or negative for GBV-C.   The results are explored graphically, and maximum likelihood is used to fit mixtures of normal distributions to the results of each test.   Model selection criteria and graphical inspection are used to examine the results.  None of the tests clearly show two distinct populations.  Further research is needed before these tests can be used routinely.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_poster,,,,FALSE,FALSE,FALSE,FALSE,Diagnostic and screening tests,ROC analysis,rex@mdanderson.org,,Peter F. Thall,Professor,University of Texas,"Dept. of Biostatistics, P.O. Box 301402",713 794 4162,713 563 4243,rex@mdanderson.org,A Prostate Cancer Trial with Re-Randomization: How We Spent a Decade Studying Twelve Dynamic Treatment Regimes,1,Peter,F,Thall,"University of Texas, M.D. Anderson  Cancer Center",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In 1999, I designed a clinical trial that aimed to evaluate four combination chemotherapies for advanced prostate cancer. The design included a multi-stage treatment strategy constructed by an oncologist, Randy Millikan, that re-randomized a patient to a new combination if his initial therapy failed. Consequently, each patient received one of twelve possible dynamic treatment regimes, although we were unaware of this terminology when we began the trial. In this talk, I will review the design published in Statistics in Medicine in 2000, the elementary statistical analyses of the trial results published in JNCI in 2007, our response to an interesting letter to the editor of JNCI criticizing our analyses, how the subsequent desire to account for informative drop-outs led us to refine the definition of patient outcome, and the results of a more recent analysis of the resulting, more refined data, including inverse probability of treatment weighted estimation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Adaptive design/adaptive randomization,rc52@buffalo.edu,,Rameela,,University at Buffalo,264 Farber Hall,7164726724,,rc52@buffalo.edu,Estimating percentiles in dose-response curves for delayed responses using an adaptive compound urn design,1,Rameela,,Chandrasekhar,University at Buffalo/Roswell Park Cancer Institute,Gregory,E,Wilding,University at Buffalo/Roswell Park Cancer Institute,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Dose-response studies are conducted primarily in Phase II/III efficacytrials to estimate pre-specified dose percentiles or the underlyingdose-response function of an investigational drug. A standard doseselecting trial randomizes subjects to several fixed dose groups andanalyzes the data after all the responses have been obtained.Response-adaptive designs have been used as an alternative to thetraditional randomization scheme to overcome the ethical and costdisadvantages. We focus our attention on the generalized Polya urn(GPU) model and its extensions. Often in clinical trials, patientresponses are not instantly obtained, delaying the randomization ofthe subsequent subject in an adaptive design to the next dose level.To investigate the effect of delayed response in an adaptive urndesign, we extend the compound urn model reviewed by Mugno, Zhus andRosenberger (Statist. Med. 2004; 23:2137-2150) to a group sequentialscheme. We estimate the percentiles of interest and evaluate itsproperties. ",FALSE,FALSE,,FALSE,FALSE,TRUE,"Workshop: Fostering Diversity in BiostatisticsWorkshop: Workshops for Junior Researchers",presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Clinical trials,clouden1@yahoo.com,,Christopher Louden,,University of Texas at San Antonio,10950 Biering Lane,(210) 878-7592,,clouden1@yahoo.com,Classification of Data under Autoregressive Circulant Covariance Structure,1,Christopher,L,Louden,The University of Texas at San Antonio,Anuradha,,Roy,The University of Texas at San Antonio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The problem of classification is an old one that has application inenvironmental, geophysical, signal processing and in many otherfields.  There are numerous approaches to this problem using thestatistical properties of the populations from which observations aredrawn. In applications such as geophysical and signals processingthere is a natural structure on the variance-covariance matrix of theobservation vectors.  The efficacy of classification is generallyincreased by taking that structure into account. One such structurethat is used to model that variance-covariance matrix is theautoregressive circulant (ARC) structure. Classification rules havebeen developed for data that have an ARC covariance structure. Theeffectiveness of these rules has been shown by simulating data setsthat have such ARC structure and comparing the error rates by usingthe rule that assumes an ARC structure, a compound symmetric (CS)structure and no structure.  The results of these simulations showthat the rule based on the correct structure has the lowest error rateand the rule based on the simple CS structure, in some cases, has ahigher error rate than the rule based on no structure assumption.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Multivariate methods,Discriminant Analysislingling07.li@gmail.com,,Lingling Li,Assistant Professor,Harvard Medical School,"133 Brookline Ave., 6th floor",6175099994,,lingling07.li@gmail.com,A Conditional Maximized Sequential Probability Ratio Test for Pharmacovigilance,1,Lingling,,Li,"Assistant Professor and Biostatistician,Department of Ambulatory Care and Prevention,Harvard Medical School,Boston, Massachusetts",Martin,,Kulldorff,"Associate Professor,Department of Ambulatory Care and Prevention,Harvard Medical School,Boston, Massachusetts.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The importance of post-marketing surveillance for drug and vaccinesafety is well recognized, as rare but serious adverse events may notbe detected in pre-approval clinical trials.  In such surveillance, itis natural to use a sequential test, as we prefer to detect the severeadverse events as soon as possible.  Various sequential probabilityratio tests (SPRT) have been widely applied in real time vaccine anddrug safety surveillance, including Walds classical SPRT with asingle alternative and the maximized SPRT (MaxSPRT) with a compositealternative.  These methods require that the expected number of eventsunder the null is known as a function of time t.  In practice, theexpected counts are usually estimated from historical data.  When wedont have a large sample size from the historical data, the SPRTswill be biased due to the variance in the estimate of the expectednumber of events.   We present a conditional maximized sequentialprobability ratio test (CMaxSPRT), which adjusts for the uncertaintyin the expected counts.  Our test incorporates the randomness andvariability from both the historical data and the surveillancepopulation.   Evaluation of the power performance of CMaxSPRT underdifferent scenarios will be presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Health services research,"Sequential Analysis,  Pharmacovigilance"Randy.Tobias@SAS.com,,Randall D. Tobias,,SAS Institute Inc.,SAS Campus Dr.,919-531-7933,,Randy.Tobias@SAS.com,Aspects of Optimal Dose Response Design,1,Randall,D,Tobias,SAS Institute Inc.,Alexander,N,Donev,University of Manchester,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Dose response experiments are involved early and late in the development of new drugs.  At the early stages of drug development, a single design is used to screen many different compounds for biologically interesting activity; at the late stages, confirmatory bioassay experiments are performed on compounds whose properties are relatively well-known a priori.  In this talk we will discuss applications of optimal design theory and techniques to both of these problems.  In the confirmatory case, we will show how useful properties of the minimum support designs revealed by optimal design theory can lead to cost-efficient experiments.  At the other end, lack of prior knowledge makes screening experiments more problematic from a theoretical point of view, but we can show that the serial dilution designs that are universally employed in practice have good Bayesian optimality properties.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Other,Experimental design,'Experimental Design in Drug Discovery & Clinical Trials'hchakraborty@rti.org,,Hrishikesh Chakraborty,Group Leader and Senior Research Statistician,RTI International,300 Parish House Road,919-485-2623,,hchakraborty@rti.org,Intracluster Correlation Adjustments to Maintain Power in Cluster Trials for Binary Variables,1,Hrishikesh,,Chakraborty,"Statistics and Epidemiology, RTI International, Research Triangle Park, North Carolina, USA.",Janet,,Moore,"Statistics and Epidemiology, RTI International, Research Triangle Park, North Carolina, USA.",Tyler,D,Hartwell,"Statistics and Epidemiology, RTI International, Research Triangle Park, North Carolina, USA.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Adequately powered sample size calculations for cluster randomized trials primarily depend on the event rate variability, effect size, average cluster size, and intracluster correlation (ICC). Furthermore, an ICC estimate depends on event rate variability among clusters, cluster size, and number of clusters. We evaluated the impact of event rates, event rate variations, cluster size, cluster size variations for different numbers of clusters. We also evaluated how the event rate changes at the end of the trial effect ICC estimates. We created one simulation exercise to investigate how different event rates, event rate variations, cluster size, and cluster size variations impact ICC estimates and 95% confidence intervals. A separate simulation exercise in four different trial scenarios examined the impact of an intervention or drug effect in the intervention group on ICC estimates and 95% confidence intervals and on sample size. The first simulation results suggest that the ICC value depends upon the event rate and event rate variations in addition to the cluster size, cluster size variations, and number of clusters. The second simulation exercise suggested that adjusting the sample size will help to preserve the appropriate power at the end of the trial. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Clustered data methods,Power analysis/sample size,xia.wang@uconn.edu,,Xia Wang,,University of Connecticut,215 Glenbrook Rd. U-4120,1-860-486-2682,,xia.wang@uconn.edu,Bayesian Development of A Generalized Link Function for Binary Response Data,1,Xia,,Wang,"University of Connecticut      //Special Note: It is my advisor Dr. Dipak K. Dey that was invited for the session 'Recent Development of Bayesian Survival and Risk Analysis.' However, I will present our paper in place of him.  Thanks.",Dipak,K.,Dey,University of Connecticut,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper introduces a flexible skewed link function for modeling binary data with covariates based on the generalized extreme value (GEV) distribution.  Extreme value techniques have been widely used in many disciplines for risk analysis.  However, its application in the binary data context is sparse and its strength as a link function has never been explored.  The commonly used complementary log-log link is a special case in the GEV distribution family but it is prone to link misspecification because of its positive and fixed shape parameter.  The GEV link is flexible in fitting the skewness in the data with an unknown shape parameter value.  Using Bayesian methodology, it automatically detects the skewness in the data along with the model fitting.  The propriety of posterior distributions under various proper and improper priors is explored in details.  The flexibility of the proposed model is illustrated by a unique billing data set of the electronic payments system adoption from a Fortune 100 company.  This link is especially attractive when there exists extreme difference between 0 and 1 observations in the binary response such that the response curve could be extremely skewed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Generalized linear models,zhangj8@muohio.edu,,Jing,,Assistant Professor,Department of Mathematics and Statistics,513-529-5824,,zhangj8@muohio.edu,Zero-inflated Bayesian Spatial Models with Repeated Measurements,1,Jing,,Zhang,"Miami University ",Chong,Z,He,"University of Missouri-Columbia",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Zero-inflated data arises inmany contexts. In this paper, we develop a Bayesian hierarchicalmodel which deals with the spatial effects, correlation betweenrepeated measurements as well as the excess zeros simultaneously.Inference, including the simulation from the posteriordistributions, predictions on new locations, and hypothesis testingon the model parameters, is carried out by computationally efficientMCMC techniques. The posterior distributions are simulated using aGibbs sampler with embedded ratio-of-uniform method and the slicesampling algorithm. The approach is illustrated via the applicationto the herbaceous data collected in the Missouri Ozark ForestEcosystem Project.",FALSE,FALSE,T1: Competing Risks,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Forestry/agriculture applications,min.chen@yale.edu,,Min Chen,,Lab of Statistical Genomics and Proteomics at Yale,"262 Bradley Street, #33",469-7341989,,min.chen@yale.edu,Background Correction Based on the Box-Cox Transformation of Noises for Illumina Bead Array Data,1,Min,,Chen,Center of Statistical Genomics and Proteomics at Yale,Yang,,Xie,UT Sourthwestern Medical Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Abstract Illumina bead array is a relatively new technology and becomes increasingly popular due to many attractive features. One distinction with other platforms is that it has negative control beads containing arbitrary oligonucleotide sequences that are not specific to any target genes in the genome. This design provides a way of directly estimating the distribution of the background noise. In the literature of background correction, the noise is often assumed to be normal. However, we show with real data that the noise can be very skewed, and the correction methods based on the normality assumption can lead to biased gene expression intensities. In this study we propose a noise adjustment method based on a model with a Box-Cox transformation on the noise term. To facilitate the search of MLE estimators, a spline technique is applied to approximate the likelihood function, and we show it can greatly improve the performance of the searching algorithm.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Microarray analysis,Measurement error,Nonemartin@stat.columbia.edu,,Martin Lindquist,Associate Professor,Columbia University,"1255 Amsterdam Ave, MC 4690",(212) 851-2148,,martin@stat.columbia.edu,Analyzing fMRI data with unknown brain activation profiles,1,Martin,A,Lindquist,"Department of StatisticsColumbia University",Lucy,F,Robinson,"Department of StatisticsColumbia University",Tor,D,Wager,"Department of PsychologyColumbia University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Most statistical analyses of fMRI data assume that the exact nature,timing and duration of the psychological processes being studied areknown. However, in many areas of psychological inquiry (e.g. studieson memory, motivation, emotion and drug uptake), it is hard to specifythis information a priori. In this talk we discuss a spatio-temporalmodel that can be used to analyze this type of data. The approachallows for the estimation of voxel-specific distributions of onsettimes and durations from the fMRI response assuming no functional form(e.g., no assumed neural or hemodynamic response), and allowing forthe possibility that some subjects may show no response.  Thedistributions can be used to estimate the probability that a voxel isactivated as a function of time, and to cluster voxels based oncharacteristics of their onset, duration, and anatomical location.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Imaging,Data mining/massive data sets,bli@lsu.edu,,Bin Li,Dr.,Louisiana State University,"161 Ag. Admin. Bldg., LSU",(225) 578-1343,,bli@lsu.edu,Classification of Functional Data: A Segmentation Approach,1,Bin,,Li,Louisiana State University,Qingzhao,,Yu,Louisiana State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We suggest a method for combining the classical method of linear discriminant analysis and support vector machine to functional data where the predictor variables are curves or functions. This procedure, which we call Segment Discriminant Analysis (SDA), is particularly useful for irregular functional data, characterized by spatial heterogeneity and local patterns like spikes. In addition, SDA allows us to (1) reduce the computation and storage burden by using a fraction of curves; (2) select important markers and extract features automatically; (3) incorporate prior knowledge from the investigators. We apply SDA to two public domain data sets and discuss the understanding developed from the study. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Functional data analysis,Applied data analysis,yuz115@psu.edu,,Yiyun Zhang,,Penn State University,326 Thomas Building,814-863-1772,,yuz115@psu.edu,Regularization Parameter Selections via Generalized Information Criterion,1,Yiyun,,Zhang,Penn State University,Runze,,Li,Penn State University,Chih-Ling,,Tsai,"University of California, Davis",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We apply the nonconcave penalized likelihood approach to obtainvariable selections as well as shrinkage estimators. This approachrelies heavily on the choice of regularization parameter, whichcontrols the model complexity. In this paper, we propose employing thegeneralized information criterion, encompassing the commonly usedAkaike information criterion (AIC) and Bayesian information criterion(BIC), for selecting the regularization parameter. Our proposal makesa connection between the classical variableselection criteria and the regularization parameter selections for thenonconcave penalized likelihood approaches. We show that the BIC-typeselector enables identification of the true model consistently, andthe resulting estimator possesses the oracle property in theterminology of Fan and Li (2001). In contrast, the AIC-type selectortends to overfit. However, similar as the loss efficiency of Li (1987)and Shao (1997), we further showed that under appropriate conditions,AIC selector enjoys an asymptotic loss efficiency which BIC-typeselectors do not possess. Our simulation results confirm thesetheoretical findings. An application in breast cancer mammographyassessment is presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Variable subset selection/model selection,Generalized linear models,borko@northwestern.edu,,Borko D Jovanovic,Associate Professor,"Feinberg School of Medicine, Northwestern Universi",680 N Lake Shore Drive,312 503 2008,,borko@northwestern.edu,FROM POPULATION TO CELL TO ANIMAL TO HUMAN,1,Borko,D,Jovanovic,Northwestern U Medical School,Raymond,C,Bergan,Northwestern U Medical School,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We discuss the scientific process and the statistical consulting process related to discovery of a pathway upstream from cell detachment and invasion, consequently leading to prostate cancer metastasis. In particular, the process involved the analysis of epidemiologic data, cell line microarray data, cell motility observations data, RTPCR and Northern Blot data, mouse metastasis count data and Phase II prevention clinical trial for patients following radical prostatectomy. A single research team and a single statistician have ben collaborating in this process for the last eight years. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Consulting,Applied data analysis,seckel@jhsph.edu,,Sandrah P. Eckel,,Johns Hopkins Bloomberg School of Public Health,"615 North Wolfe Street, Room E3527",410-614-5135,410-955-0958,seckel@jhsph.edu,Surrogate screening models for determining low physical activity in the Cardiovascular Health Study,1,Sandrah,P,Eckel,Johns Hopkins Bloomberg School of Public Health,Karen,,Bandeen-Roche,Johns Hopkins Bloomberg School of Public Health,Paulo,H,Chaves,Johns Hopkins Bloomberg School of Public Health,Linda,P,Fried,Columbia University Mailman School of Public Health,Thomas,A,Louis,Johns Hopkins Bloomberg School of Public Health,,,,,,,,,,,,,,,,,,,,,"Frailty is a geriatric syndrome of major public health importance. Low physical activity is a key item of the widely used frailty phenotype originally created in the Cardiovascular Health Study (CHS). In CHS, physical activity was assessed using the long Minnesota Leisure Time Physical Activity Questionnaire at 3 of the first 10 annual visits. Our goal is to develop and apply methods to identify and evaluate surrogate screening models for the low physical activity item of the frailty phenotype. We investigate whether subsets of questionnaire items and/or other related data available in CHS can create an effective surrogate screening model with the dual goals of simplifying screening and of determining low physical activity when the questionnaire information is missing. We use Generalized Boosted Models (GBM) and logistic regression to select nested subsets of the most influential candidate predictors. Using these subsets, we build potential surrogate screening models and evaluate predictive accuracy using complementary criteria. We ``train' models on baseline CHS data and evaluate them at baseline and follow-up. Results may inform future frailty screening tools.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Diagnostic and screening tests,Machine learning,shuli@schoolph.umass.edu,,Shuli Yu,,University of Massachusetts-Amherst,408 Arnold House,6016209639,,shuli@schoolph.umass.edu,New Development of Optimal Coefficients for a Best Linear Unbiased Estimator of the Total for Simple Random Sampling with Replacement Using Godambe's General Linear Estimator,1,Shuli,,Yu,"Division of Biostatistics, School of Public Health, University of Massachusetts-Amherst ",Edward,J.,Stanek III,"Division of Biostatistics, School of Public Health, University of Massachusetts-Amherst",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Questions about real populations/subjects are appropriately framed in models that closely match the setting, rather than in artificial superpopulation or infinite population frameworks.  Finite population mixed models (FPMM) are appropriately framed, but are limited to simple random sampling without replacement settings (one or two stage).    Godambe (1955) defined and proved that there is no best linear unbiased estimator (BLUE) for a sample from a finite population in a general setting that allowed for with replacement sampling. We discuss Godambe's results in the context of the FPMM, obtaining optimal coefficients for the BLUE of the population total based on a model for with replacement sample sets of size n=2 from a finite population of size N=3. Optimal coefficients are obtained when parameters are distinct and not equal to zero after specifying unbiased constraints and one additional constraint for over-parameterization. The solutions are consistent with the results of our separate study regarding a simple random sampling without replacement of size n=2 from N=3. Substituting the optimal coefficients into the estimator gives a solution with zero mean squared error (MSE). ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Other,Estimating equations,Samplingtat5@cdc.gov,,Theodore J. Thompson,,Centers for Disease Control and Prevention,1332 Stoneybrook Drive,770-488-1270,,tat5@cdc.gov,Bayesian Model-based Estimates of Diabetes Incidence by State,1,Theodore,J,Thompson,"Centers for Disease Control and PreventionAtlanta, GA",Betsy,L,Cadwell,"Centers for Disease Control and PreventionAtlanta, GA",James,P,Boyle,"Centers for Disease Control and PreventionAtlanta, GA",Lawrence,,Barker,"Centers for Disease Control and PreventionAtlanta, GA",,,,,,,,,,,,,,,,,,,,,,,,,"The Behavioral Risk Factor Surveillance System (BRFSS) is a statebased random-digit-dialed survey of the U.S. civilian, noninstitutionalized population aged > 18 years.  The fraction of thepopulation diagnosed with diabetes within the past year (incidence)can be determined from the diabetes module of the BRFSS, notadministered by all states in all years.  Direct design-basedestimates of diabetes incidence for a single state in a single yearare not sufficiently precise to be useful.  We develop a multilevelmodel that provides the first practical yearly estimates of diabetesincidence for all 50 states and the District of Columbia.  Using arealevel models that treat design-based estimates and their estimatedvariances as data, we provide estimates for the years 1999 to 2006.State level covariates (Census division, diabetes prevalence, percentcompleting high school) are included.  State-level temporal randomeffects are modeled as first order autoregressive processes.  Unlikesome competing methods, Bayesian models constrain all estimates to theinterval (0, 1).  Posterior distributions of diabetes incidence bystate and year and posterior distributions of state ranks by year areprovided.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Survey research data,Bayesian methods,hsienming.hung@fda.hhs.gov,,H.M. James Hung,Dr.,Food and Drug Administration,"Division of Biometrics I, CDER, FDA",301-796-1092,301-796-9781,hsienming.hung@fda.hhs.gov,Utility and Pitfalls of Meta Analysis for Designing Non-Inferiority Trial,1,H.M. James,,Hung,FDA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As ethical reasons prohibit enrolling study patients into a placebo arm, non-inferiority trial designs are employed more frequently in clinical research. The classical non-inferiority trial methodology usually requires a non-inferiority margin be pre-specified at the trial design stage to clearly define the clinical/statistical hypothesis for testing. In the absence of a placebo arm in the trial, the margin can only be selected relying on a guidance from some kind of meta-analysis of the existing relevant placebo controlled trials for estimating the effect of the selected active control. In this presentation, I shall discuss the roles of meta analysis in terms of the extent of useful evidence or information that may be carved out by such a analysis.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Our session discussant Dr. DeMets, our session chair Dr. Wang and I (Dr. Hung) will not be available on March 18, 2009. Please schedule this session to take place on March 16 or 17.",invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Random effects,llan@mcg.edu,,Ling Lan,Assistant Professor,Medical College of Georgia,"1120 15th Street, AE-3031",(502) 379-7292,,llan@mcg.edu,"Comparison of state occupation, entry, exit and waiting times in K independent multistate models under current status data",1,Ling,,Lan,"Department of Biostatistics, Medical College of Georgia",Somnath,,Datta,"Department of Bioinformatics and Biostatistics, University of Louisville",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose  distance based nonparametric bootstrap tests comparing the occupation probabilities and entry, exit and waiting times in a given state in two or more multistate systems each of which has the same topology. The actual transition times are subjected to current status censoring resulting from a single random inspection time for each individual. A detailed simulation study shows that the proposed test has close to nominal size and reasonable power. An illustrative application to a pubertal development data set obtained from the NHANES III is also presented. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Multivariate survival,Survival analysis,di_an@merck.com,,Di An,,"Merck & Co., Inc.",351 N Sumneytown Pike,2673051628,,di_an@merck.com,Multiple Imputation Methods for Disclosure Limitation in Longitudinal Data,1,Di,,An,"Merck Research Laboratories, Merck & Co., Inc.",Roderick,J.A.,Little,"Department of Biostatistics, University of Michigan",James,W.,McNally,"Institute for Social Research, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Disclosure limitation is an important consideration in the release of public use data sets. It is particularly challenging for longitudinal data sets, since information about an individual accumulates over time. We consider problems created by high ages in cohort studies. Because of the risk of disclosure, ages of very old respondents can often not be released, as stipulated by the Health Insurance Portability and Accountability Act (HIPAA). Top-coding of individuals beyond a certain age is a standard way of dealing with this issue, but it has severe limitations in longitudinal studies. We propose and evaluate an alternative to top-coding for this situation based on multiple imputation (MI). This MI method is applied to a survival analysis of simulated data and data from the Charleston Heart Study, and is shown to work well in preserving the relationship between hazard and covariates.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Survey research data,liang.zhu@stjude.org,,Liang Zhu,,St. Jude Children's Research Hospital,"MS 768, Room R6037",573-529-6075,,liang.zhu@stjude.org,Semiparametric analysis of multivariate recurrent and terminal events,1,Liang,,Zhu,"Biostatistics, St. Jude Children's Research Hospital",Jianguo,,Sun,"Statistics, University of Missouri-Columbia",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recurrent event data occur in many clinial and observational studies and in this type of data, it is often the case that there also exists a terminal event such as death that is related to the recurrent event of interest. In addition, sometimes there may exist more than one type of recurrent events, that is, one faces multivariate recurrent event data with some correlated terminal event. It is apparent that in these situations, one has to take into account the dependence both among different types of recurrent events and between the recurrent and terminal events. In this paper, we propose two approaches for regression analysis of such data, a joint modelling approach and a marginal model approach. Both finite and asymptotic properties of the proposed estimates are established. The methods are applied to a set of bivariate recurrent event data arising from a study of the patients with the end-stage renal disease.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Multivariate methods,donatello.telesca@gmail.com,,Donatello Telesca,Postdoctoral Fellow,U.T. / M.D. Anderson Biostatistics,"1515 Holcombe Blvd., Unit 447",713 792 1619,,donatello.telesca@gmail.com,Bayesian Modeling of Pharmacogenetics data,1,Donatello,,Telesca,U.T. / M.D. Anderson Biostatistics,Gary,L,Rosner,U.T. / M.D. Anderson Biostatistics,Peter,,Muller,U.T. / M.D. Anderson Biostatistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We describe a general framework for the exploration of therelationships between pharmacokinetic pathways and  polymorphisms ingenes associated with the metabolism of a compound of interest. Weintegrate a population pharmacokinetics model with a simple samplingmodel of genetic mutation via a latent conditional dependence prior.Significant interactions are selected allowing the parmacokineticparameters to depend on gene sets of variable dimension. We discussposterior inference and prediction based on RJ-MCMC simulation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Pharmacokinetic/pharmacodynamics (PK/PD) modeling,a0kris04@louisville.edu,,Ashok Krishnamurthy,Doctoral Student,"Department of Bioinformatics and Biostatistics, Un",627 S Preston St Apt 4G,5028524293,5028523294,a0kris04@louisville.edu,An Iterative Phase I/II Clinical Trial Design Incorporating Genomic Biomarkers,1,Ashok,,Krishnamurthy,"Doctoral Student,Department of Bioinformatics and Biostatistics,School of Public Health and Information Sciences,University of Louisville, Louisville, KY",Ashok,,Krishnamurthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The goal of a Phase I clinical trial is to determine the maximum tolerated dose (MTD) that corresponds to some given acceptable level of toxicity known as a dose-limiting toxicity. Effectiveness of the MTD is tested in a Phase II trial, where the purpose is to find the minimum effective dose (MED). The designs fall into two classes: Nonparametric rule-based designs (ex: 3 + 3) and the Bayesian model-guided designs (ex: CRM). A well known limitation to the generalizability of Phase I clinical trial is the heterogeneity of the patient population. Ignoring the differences in subgroups and running a single Phase I clinical trial would result in an inaccurate estimate of the MTD. We propose an iterative Phase I/II design that incorporates genomic biomarker information to classify patients as Biomarker Positive (B+) or Negative (B-). We present and examine a method for obtaining separate MTD estimates for subgroups (B+/B-). Performance is evaluated by considering three possible MTD re-estimation techniques: the logistic regression; the isotonic regression and the constrained logistic regression. The design then branches into two secondary Phase I trials to obtain a redefined MTD for subgroups. We can then conclude a drug may be appropriate for a defined subgroup.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Constrained estimation/order restricted inference,pmxic@nottingham.ac.uk,,Irina Czogiel,Diplom-Statistikerin,University of Nottingham,School of Mathematical Sciences,0044 115 95 14961,,pmxic@nottingham.ac.uk,Bayesian Alignment of Continuous Molecular Shapes,1,Irina,,Czogiel,"University of Nottingham, UK",Ian,L,Dryden,University of Nottingahm,Christopher,J,Brignell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A frequent objective in drug design is to find molecules with a highbinding affinity towards a certain target protein. If no structuralinformation about the target protein is available, putative ligandsare often superimposed with the structure of a reference ligand whichis known to bind to the target under consideration. If a ligand can bealigned closely, it is likely to exhibit a similar biochemicalactivity and hence drug potency.Here, we propose a statistical model for evaluating and comparingmolecular shapes using methods from the field of statistical shapeanalysis. In order to account for the continuous nature of molecules,we combine these methods with techniques used in spatial statisticsand apply  kriging to predict the values of the considered molecularproperties (e.g. partial atomic charge) in three--dimensional space. Superimposing entire fields rather than discrete points solves theproblem that there is usually no clear one--to--one correspondencebetween the atoms of the considered molecules. Using similar concepts,we also propose an algorithm for the simultaneous alignment ofmolecular fields.Our methods work well on a data set comprising 31 steroid moleculeswhich has been used as a test bed for various alignment techniques.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Biopharmaceutical research,Spatial/temporal modeling,lms86@cornell.edu,,Lynn M. Johnson,graduate student,Cornell University,Department of Statistical Science,(607) 227-0981,,lms86@cornell.edu,Induced Smoothing for the Semiparametric Accelerated Failure Time Model: Asymptotics and Extensions to Clustered Data,1,Lynn,M,Johnson,Cornell University,Robert,L,Strawderman,Cornell University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper extends the induced smoothing procedure of Brown and Wang (2006) for the semiparametric accelerated failure time model to the case of clustered failure time data.  The resulting procedure permits fast and accurate computation of regression parameter estimates and standard errors using simple and widely available numerical methods (e.g., the Newton-Raphson algorithm).  The regression parameter estimates are shown to be strongly consistent and asymptotically normal; in addition, we prove that the asymptotic distribution of the smoothed estimator coincides with that obtained without the use of smoothing.  These results establish a key claim of Brown and Wang (2006) for the case of independent failure time data as well as extend such results to the case of clustered data.  Simulation results demonstrate that these smoothed estimates perform as well as those obtained using the best available methods at a fraction of the computational cost.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Multivariate survival,wm_pku@hotmail.com,,Ming Wang,,Emory University,3131 N Druid Hills Rd,5028760676,,wm_pku@hotmail.com,Inference for Marginal Linear Models with Clustered Longitudinal Data with Potentially Informative Cluster Sizes,1,Ming,,Wang,Emory University,Maiying,,Kong,University of Louisville,Somnath,,Datta,University of Louisville,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Clustered longitudinal data are often collected as repeated measures on subjects arising in clusters. Examples include periodontal disease study, where the measurements related to the disease status of each tooth are collected over time for each patient. Under such situations, generalized estimating equations (GEE) may lead to invalid inferences. We investigate the performance of three competing proposals of fitting marginal linear models to clustered longitudinal data, namely, generalized estimating equations (GEE), within-cluster resampling (WCR), and cluster-weighted generalized estimating equations (CWGEE). We show by simulations and theoretical calculations that, when the cluster size is informative, GEE provides biased estimators, while both WCR and CWGEE achieve unbiasedness under a variety of working correlation structures for temporal measurements within each subject. Statistical properties of confidence intervals have been investigated using the probability-probability plots. Overall, CWGEE appears to be the recommended choice for marginal parametric inference with clustered longitudinal data that achieves similar parameter estimates and test statistics as WCR while avoiding Monte Carlo computation. We illustrate our analysis using a temporal dataset on periodontal disease which clearly demonstrates the need for CWGEE over GEE.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Clustered data methods,bcfrench@upenn.edu,,Benajmin French,Assistant Professor,"Department of Biostatistics and Epidemiology, Univ",625 Blockley Hall,2155738545,2155734865,bcfrench@upenn.edu,Estimating a volume-outcome association from aggregate longitudinal data,1,Benjamin,,French,"Department of Biostatistics and Epidemiology, University of Pennsylvania",Farhood,,Farjah,"Department of Surgery, University of Washington",David,R,Flum,"Department of Surgery, University of Washington",Patrick,J,Heagerty,"Department of Biostatistics, University of Washington",,,,,,,,,,,,,,,,,,,,,,,,,"Recently there has been much interest in using volume-outcome data toestablish causal associations between measures of surgical experienceand patient outcomes following a surgical procedure. However, theredoes not appear to be a standard approach to a volume-outcome analysiswith respect to specifying a volume measure and selecting anestimation method. We establish the recurrent marked point process asa general framework from which to approach a longitudinalvolume-outcome analysis and examine the statistical issues associatedwith using longitudinal data analysis methods to model aggregatevolume-outcome data. We conclude with the recommendation that analysiscarefully specify a volume measure that most accurately reflects theirscientific question of interest and select an estimation method thatis appropriate for their scientific context.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Longitudinal data,bcfrench@mail.med.upenn.edu,,Benjamin French,Assistant Professor,"Department of Biostatistics and Epidemiology, Univ",625 Blockley Hall,2155738545,2155734865,bcfrench@mail.med.upenn.edu,Estimating a volume-outcome association from aggregate longitudinal data,1,Benjamin,,French,"Department of Biostatistics and Epidemiology, University of Pennsylvania",Farhood,,Farjah,"Department of Surgery, University of Washington",David,R,Flum,"Department of Surgery, University of Washington",Patrick,J,Heagerty,"Department of Biostatistics, University of Washington",,,,,,,,,,,,,,,,,,,,,,,,,"Recently there has been much interest in using volume-outcome data toestablish causal associations between measures of surgical experienceor qualityand patient outcomes following a surgical procedure. However, theredoes not appear to be a standard approach to a volume-outcome analysiswith respect to specifying a volume measure and selecting anestimation method. We establish the recurrent marked point process asa general framework from which to approach a longitudinalvolume-outcome analysis and examine the statistical issues associatedwith using longitudinal data analysis methods to model aggregatevolume-outcome data. We conclude with the recommendation that analystscarefully specify a volume measure that most accurately reflects theirscientific question of interest and select an estimation method thatis appropriate for their scientific context.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Longitudinal data,xbasagana@creal.cat,,Xavier Basagana,,Harvard School of Public Health,Doctor Aiguader 88 (CREAL),(+34) 933160651,,xbasagana@creal.cat,Power and Sample Size Calculations for Longitudinal Studies Estimating a Main Effect of a Time-Varying Exposure,1,Xavier,,Basagana,Harvard School of Public Health,Donna,,Spiegelman,Harvard School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Existing study design formulas for longitudinal studies assume that the exposure is time-invariant or that it varies in a manner that is controlled by design. In observational studies, the investigator does not control how exposure varies within subjects over time. Typically, a large number of exposure patterns are observed, with differences in the number of exposed periods per participant and with changes in the cross-sectional prevalence of exposure over time. This paper provides formulas for study design calculations that incorporate these features for studies with a continuous outcome and a binary, time-varying exposure, when the effect of exposure is assumed constant over time. We show that incorrectly using the formulas for time-invariant exposure can produce substantial overestimation of the required sample size. It is shown that the mean prevalence and the intraclass correlation of exposure are the only two additional parameters needed for exact solutions for the required sample size, if compound symmetry of residuals can be assumed, or to a good approximation if residuals follow a damped exponential correlation structure. We provide a useful interpretation of the intraclass correlation of exposure as a measure of imbalance in the number of exposed periods per participant. The methods are applied to several examples. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Experimental design,Epidemiologic methods,mengyego@mail.med.upenn.edu,,Mengye Guo,,"Biostatistics, University of Pennsylvania",503 Blockley Hall 423 Guardian Drive,215-573-8950,,mengyego@mail.med.upenn.edu,Multiplicity-Calibrated Bayesian Hypothesis Tests,1,Mengye,,Guo,"Biostatistics, University of Pennsylvania",Daniel,F,Heitjan,"Biostatistics, University of Pennsylvania",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"When testing multiple hypotheses simultaneously, there is a need toadjust the levels of the individual tests to effect control of thefamily-wise-error-rate (FWER).  Standard frequentist adjustmentseffectively control the error rate but are typically conservativeand oblivious to prior information. We propose a Bayesian testingapproach --- Multiplicity-Calibrated Bayesian Hypothesis Testing(MCBHT) --- that sets individual critical values to reflect theprior information while controlling the FWER via the Bonferroniinequality.  If the prior information is specified correctly, in thesense that those null hypotheses considered most likely to be falsein fact are false, the power of our method is substantially greaterthan that of standard frequentist approaches. We illustrate ourmethod using data from a preclinical cancer study and apharmacogenetic trial. We demonstrate its error rate control andpower advantage by simulation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Multiple testing,mourad.atlas@louisville.edu,,Mourad,Ph.D Candidate,University of Louisville,778 David Fairleigh Ct # 7,502-515-3111,918-515-3111,mourad.atlas@louisville.edu,Monoisotopic Peak Detection for Mass spectrometry data,1,Mourad,,Atlas,"Department Of Bioinformatics and BiostatisticsSchool of Public Health and Information ScienceUniversity of Louisville",Susmita,,Datta,"Department Of Bioinformatics and BiostatisticsSchool of Public Health and Information ScienceUniversity of Louisville",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Mass spectrometry has emerged as a core technology for high throughput proteomics profiling. It has enormous potential in biomedical research; however, the complexity of the data poses new statistical challenges for the analysis. Statistical methods and software developments for analyzing proteomics data will continue to be a major  area of research in the coming years.  	In this paper, we develop novel statistical methods for analyzing high dimensional mass-spectrometry proteomics data. We propose to use the chemical knowledge of the isotopic distribution of peptide molecules along with quantitative modeling to detect chemically valuable peaks from each spectrum. A mixture of location-shifted Poisson distribution is fitted to the deamidated isotopic distribution of a peptide molecule. Maximum likelihood estimation by Expectation-Maximization (EM) technique is used to estimate the parameters of the distribution.  We then determined the monoisotopic peak for each of the isotopic distribution. Our method is examined through simulations and real data. We compare our method with an existing method of peak detection. keywords: Mass spectrometry, Proteomics, peaks, isotopic distribution, location-shifted Poisson, monoisotopic peaks.",FALSE,FALSE,,FALSE,FALSE,TRUE,WORKSHOP 'FOSTERING DIVERSITY IN BIOSTATISTICS',oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Proteomics,Computational methods,lei-hua@uiowa.edu,,Lei Hua,,"University of Iowa, Department of Biostatistics",E176-87 GH,(319)335-9787,,lei-hua@uiowa.edu,Spline-Based Sieve Semiparametric Generalized Estimating Equation Method,1,Lei,,Hua,"University of Iowa, Department of Biostatistics",Ying,,Zhang,"University of Iowa, Department of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose to analyze panel count data using a spline-based sieve semiparametric generalized estimating equation method with a semiparametric proportional mean model. The baseline mean function is approximated by monotone cubic B-spline functions. The estimates of regression parameters and spline coefficients are the roots of generalized estimating equations (sieve GEE) and computed by the generalized Rosen algorithm utilized in Zhang and Jamshidian (2004)The proposed method avoids assuming the parametric structure of the mean function and the underlying counting process. Selection of an appropriate covariance matrix that accounts for the over-dispersion and autocorrelation generally improves estimation efficiency.The asymptotic variances of the sieve semiparametric GEE estimates can be estimated using a sandwich formula and the semiparametric inference about the unknown parameters is robust to the misspecification of the covariance matrix. Simulation studies are conducted to investigate the finite sample performance of the sieve semiparametric GEE estimates with different sample sizes. Finally, the proposed method with different covariance matrices is applied to a real data from a bladder tumor clinical trial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Estimating equations,Nonparametric methods,jieru.xie@louisville.edu,,Jieru Xie,,"department of Bioinformatics and Biostatistics, Un",627 S. Preston ST #3G,484-347-1893,,jieru.xie@louisville.edu,"CLAN:  A Novel, Practical Method of Curvature Assessment in Nonlinear Regression Models",1,Jieru,,Xie,"Department of Bioinformatics and Biostatistics, University of Louisville",Linda,Jane,Goldsmith,"Department of Bioinformatics and Biostatistics, University of Louisville",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A novel, practical method of assessing nonlinearity behavior is developed to assess the extent of the nonlinearity in a nonlinear regression model with design data points. We consider the geometric aspects of nonlinear regression modeling and use the familiar concept of confidence level as the criterion for nonlinearity assessment. The computation is based on the difference between the linear approximation inference ellipsoid region and the likelihood region, the often banana-shaped confidence region computed without the linear assumption. The method is applied to 23 published nonlinear datasets. It is found that the new method, CLAN (Confidence Level Assessment of Nonlinearity), is in good agreement with the root mean squared estimates of parameter effects and intrinsic nonlinearity introduced by Bates & Watts in their 1980s paper and book.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Nonlinear models,Exact methods,hscho@bios.unc.edu,,Hyunsoon Cho,,Univ. of North Carolina at Chapel Hill,"3920 South Roxboro Street, Apt213",919-428-4316,919-966-7285,hscho@bios.unc.edu,Bayesian Case Influence Measures and Applications,1,Hyunsoon,,Cho,"Department of Biostatistics, University of North Carolina at Chapel Hill",Hongtu,,Zhu,"Department of Biostatistics, University of North Carolina at Chapel Hill",Joseph,G,Ibrahim,"Department of Biostatistics, University of North Carolina at Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Three types of Bayesian case influence measures based on case deletion, namely phi-divergence, Cook's posterior mode distance and Cook's posterior mean distance are introduced to evaluate the effects of deleting a single observation in Bayesian regression models. The aim of this paper is to examine the statistical properties of these three diagnostic measures and their applications to model assessment. We derive their asymptotic approximations and establish their asymptotic equivalence. Moreover, we show that the sums of the proposed Bayesian case-deletion diagnostics measure model complexity, which is related to the effective number of parameters in the Deviance Information Criterion (DIC). We illustrate the proposed methodologies on generalized linear models and survival models. In addition, we present two real data examples to demonstrate the proposed methodology.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Variable subset selection/model selection,Diagnostic method for Bayesian regression modelsysui@stat.brown.edu,,Yunxia Sui,,Brown University,"Brwon University Box G-S121,7th fl.",4018633510,4018639182,ysui@stat.brown.edu,Background Adjustment for DNA Microarrays using a Database of Microarray Experiments,1,Yunxia,,Sui,Brown University,Zhijin,,Wu,Brown University,Xiaoyue,,Zhao,Bionovo Inc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Microarrays has become an indispensable technique in biomedical research.The raw measurements from microarrays undergo a number of preprocessing steps beforethe data are converted to genomic level for further analysis. Background adjustment isan important step in preprocessing. Estimating background noise has been challengingbecause of limited replication of microarray experiments. Most current methods have usedthe empirical Bayes approach to borrow information across probes in the same array. Theseapproaches shrink the background estimate for either the entire sample or probes sharingsimilar sequence structures. In this article we present a probe speci¯c solution in estimatingbackground noise using a database of large number of microarray experiments. Informationis borrowed across samples and background noise is estimated for each probe individually.The ability to prove truly probe speci¯c background distribution allows us to extend thedynamic range of gene expression levels. We illustrate the improvement in detecting geneexpression variation on two datasets: a Latin Square spike-in experiment publicly availableand an Estrogen Receptor experiment with biological replicates.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Microarray analysis,High dimensional data,microarray preprocessingjjkim@uga.edu,,Jaejik Kim,Graduate student,University of Georgia,155 International DR APT 403,706-614-6206,,jjkim@uga.edu,Clustering Techniques for Histogram-valued Data,1,Jaejik,,Kim,University of Georgia,Lynne,,Billard,University of Georgia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Contemporary datasets are becoming increasingly larger and more complex, while techniques to analyse them are becoming more and more inadequate. Thus, new methods are needed to handle these new types of data. This paper introduces methods to cluster histogram-valued observations. First, three new dissimilarity measures for histogram data are proposed. Then, a polythetic clustering algorithm is developed (based on all p variables). Validity criteria to aid in the selection of the optimal number of clusters are described. The new methodology is illustrated on a large dataset collected from the US Forestry Service.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Other,Data mining/massive data sets,Symbolic data analysiszhantt@temple.edu,,Tingting Zhan,Ph.D. student,Temple Univeristy,1801 N. Broad St.,267-324-2212,,zhantt@temple.edu,An Empirical Study on Weighted Likelihood  Estimators for Mixture of Normal Model,1,Tingting,,Zhan,Temple Univeristy,Inna,,Chervoneva,Thomas Jefferson University,Boris,,Iglewicz,Temple University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this work, we consider a general class of robust weighted likelihood estimators (WLE) for finite mixture of normal model. The WLE class unifies several families of minimum disparity estimators through weight and bias adjustment term. The influence function and asymptotic covariance matrix for a general density function model are derived for the WLE class. A simple and efficient iterative EM-like algorithm is described for computing WLE for mixture of normal model. Three particular WLEs, corresponding to the density power divergence, symmetric chi-squre and negative exponential disparity are compared in a simulation study. The negative exponential disparity has not yet been considered for robust estimation in mixture of normal model. In the simulation study, we consider three-component mixtures with substantially overlapping components and large contaminations. Previously reported simulation studies are generally limited to two well-separated components. Our results show clear advantage of WLEs with weights based on kernel-smoothed Pearson residuals over the density power divergence or maximum likelihood estimators. A real data exampleis provided to illustrate the potential differences in parameter estimates obtained using different robust estimating approaches and maximum likelihood. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Applied data analysis,Computational methods,dxu1@umbc.edu,,Dihua Xu,,UMBC,Department of Math & Statistics,4439553509,,dxu1@umbc.edu,Methyl Bromide Alternatives in Huelva (Spain): A Case of Meta-analysis application,1,Dihua,,Xu,"UMBC, Department of Math & Statistics",Bimal,K,Sinha,"UMBC, Department of Math & Statistics",Guido,,Knapp,"TU Dortmund University, Department of Statistics ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Methyl Bromide has been widely used globally as a pre-planting soilfumigant in the horticultural industry all over the world.Due to itsozone depleting nature, effective alternatives have been sought.Several studies in Spain have been carried out with conflictingconclusion. In this talk an appropriate statistical meta-analysis hasbeen performed to settle the issues. Some special features of the datasets have been pointed out.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Other,Environmental and ecological applications,Meta-Analysisandrei@biostat.wisc.edu,,Adin-Cristian Andrei,Assistatnt Professor of Biostatistics,University of Wisconsin-Madison,K6/428 CSC 600 Highland Avenue,608-263-6797,,andrei@biostat.wisc.edu,Nonparametric Quantile Estimation for Successive Events Subject to Censoring,1,Adin-Cristian,,Andrei,"University of Wisconsin-Madison, Department of Biostatistics and Medical Informatics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Quantiles represent important and useful summary measures used insurvival analysis. For example,in a cancer clinical trial investigators may want to know thequantiles of the time-to-death, measured from diseaserelapse, given that one has been disease-free for one year.Conditional quantile estimates based on the conditionalKaplan-Meier curve are not consistent, due to induced dependentcensoring. Methodology for properly estimating suchquantities in successive or recurrent time-to-event settings islacking. By consistently estimating the joint distributionof the successive times involved, we develop consistent nonparametricconditional quantile estimators and provideconfidence intervals by inverting a test statistic. Simulationsperformed in a variety of scenarios confirm the goodfunctional characteristics of the method. An example from theInternational Breast Cancer Study Group Trial V is usedto illustrate the practical usefulness of this methodology.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Multivariate survival,Survival analysis,pullena@mcmaster.ca,,Eleanor Pullenayegum,Dr,McMaster University,St Joseph's Healthcare Hamilton,905 522 1155 35929,,pullena@mcmaster.ca,Semi-parametric models for longitudinal cost data subject to incomplete observation,1,Eleanor,M,Pullenayegum,McMaster University,Andrew,R,Willan,University of Toronto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The mean cost of a treatment strategy is an important consideration for policy-makers. Thus modelling the mean cost semi-parametrically is an attractive option to avoid the need to transform. Costs are often collected longitudinally, and estimation for semi-parametric mean regression models is typically via inverse-probability weighting to account for censoring in the cost data. Because inverse-probability weighting is inefficient, there has been much work on improving the efficiency of these estimators. Typically semi-parametric models for costs have stratified by time interval. Although efficiency has received much attention, modelling has often been overlooked. This talk will argue that careful modelling can lead to greater improvements in precision than complex estimation techniques.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Health policy applications,Missing data,yud101@psu.edu,,Yuexiao Dong,,Pennsylvania State University,429 Oakwood Ave,8143214004,,yud101@psu.edu,Dimension reduction for non-elliptically distributed predictors: second-order methods,1,Yuexiao,,Dong,Pennsylvania State University,Bing,,Li,Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many classical dimension reduction methods --- especially those based on inverse conditional moments --- require the predictors to have elliptical distributions, or at least to satisfy a linearity condition. Such conditions, however,  are too strong for some applications. Li and Dong (2008) introduced the notion of the central solution space and used it to modify the first-order methods, such as Sliced Inverse Regression, so that they no longer rely on these conditions. In this paper we generalize this idea to the second-order methods, such as Sliced Average Variance Estimator and Directional Regression. In doing so we demonstrate that the central solution space is a versatile framework: we can use it to modify essentially all inverse conditional moment based methods to relax the distributional assumption on the predictors. Simulation studies and an application show a substantial improvement of the modified methods over their classical counterparts.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Nonparametric methods,lgl1@rice.edu,,Luis Gonzalo Leon-Novelo,PhD in Statistics Candidate,Rice University,2352 canden unit C,8327710266,,lgl1@rice.edu,Borrowing strength with non-exchangeable priors over subpopulations,3,Peter,,Mueller,M.D. Anderson Cancer Center.,Benjamin,N,Bekeley,M.D. Anderson Cancer Center. ,Luis,G,Leon Novelo,Rice University,Kyle,,Wathen,M.D. Anderson Cancer Center. ,Fernando,A,Quintana,Pontificia Universidad Católica de Chile ,,,,,,,,,,,,,,,,,,,,,"We introduce a non-parametric Bayesian model for success rates in aphase II clinical trial with patients presenting different subtypes of thedisease under study. The subtypes are not a priori exchangeable. Thelack of a priori exchangeability hinders straightforward use oftraditional hierarchical models to implement borrowing of strengthacross disease subtypes.We propose instead a random partition model for the set of diseasesubtypes. All subtypes within the same cluster share a common successprobability. Our model is a variation of the productpartition model with a non-exchangeable priorstructure.In particular the data arises from a clinicaltrial of patients with sarcoma, a rare cancer affectingconnective and soft tissues (e.g., cartilage andfat). Each patient presents one subtype of the disease andsubtypes are grouped by good, intermediate and poor prognosis.The prior model respects the varying prognosis across diseasesubtypes. Two subtypes with equal prognosis are more likely apriori to co-cluster than two subtypes with different prognosis.The practical motivation for this approach is thatthe number of accrued patients within each subtype is too smallto asses the success rates with the desired precision if analyzing the data for each subtype separately.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Bayesian methods,rvogel@georgiasouthern.edu,,Robert L. Vogel,Professor,Jiann-Ping Hsu College of Public Health,Georgia Southern University,912-478-7423,,rvogel@georgiasouthern.edu,On tests of homogeneity for partially matched-pair data,2,Hani,,Samawi,"Jiann-Ping Hsu College of Public HealthGeorgia Southern UniversityStatesboro, Georgia",Robert,L,Vogel,"Jiann-Ping Hsu College of Public HealthGeorgia Southern UniversityStatesboro, Georgia",Wilson,A,Koech,"Jiann-Ping Hsu College of Public HealthGeorgia Southern UniversityStatesboro, Georgia",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this project, we are investigating several tests of homogeneity between two groups when the data is partially matched-pair by comparing their power. Also we propose a weighted test of homogeneity based on Pearson and McNemer chi-squared statistics.  Numerical and simulation studies will be conducted to compare the power of the proposed test against tests that are currently used in the literature.  Real data from the National Survey of Childs Health (NSCH) 2003 provided by Center for Disease Control and Prevention, Hyattsville Maryland is used to illustrate the method we developed.Key words: McNemar Test, Pearson Chi-square test, Weighted Chi-square test, partially matched-pair, case-control and matching.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Categorical data,Nonparametric methods,cdbarr@gmail.com,,Christopher David Barr,Postdoctoral Fellow,Hopkins Biostatistics,615 N. Wolfe Street,443-287-4770,,cdbarr@gmail.com,On the Voronoi estimator for intensity of an inhomogeneous planar Poisson process,1,Christopher,D,Barr,"Department of BiostatisticsJohns Hopkins Univeristy",Frederic,P,Schoenberg,"Department of StatisticsUniversity of California, Los Angeles",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We investigate the statistical properties of the Voronoi estimator forthe intensity of an inhomogeneous Poisson process. The Voronoiestimator may be defined for any location as the inverse of the areaof the corresponding Voronoi cell. The estimator is well-known to beunbiased in the case of estimating the intensity of a homogeneousPoisson process, and is shown here to be approximately ratio unbiasedin the inhomogeneous case. Simulation studies show the samplingdistribution is well approximated by the inverse gamma model,extending similar results from the homogeneous case. Performance ofthe Voronoi estimator is compared to a kernel estimator using twosimulated data sets as well as a dataset consisting of earthquakeswithin the continental United States.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Environmental and ecological applications,mil21@pitt.edu,,Minjae Lee,,Student,1043 N. Negley Ave. Apt 7,412-361-0513,,mil21@pitt.edu,A multiple imputation approach for left-censored biomarkers with limits of detection,1,Minjae,,Lee,"MS, Graduate student, Department of Biostatistics, Graduate School of Public Health, University of Pittsburgh",Lan,,Kong,"PhD, Assistant Professor, Department of Biostatistics, Graduate School of Public Health, University of Pittsburgh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We often encounter left-censored biomarker measurements subject to the limits of detection (LOD). Ignoring or replacing the censored observations with naive imputation method lead to biased estimates of the parameters in the regression analysis. Maximum likelihood methods have been developed when the distribution of the biomarkers are assumed normal. However, the computation can be very intensive or even prohibitive as the number of censored biomarkers increases. Motivated by a sepsis study, where a panel of biomarkers were measured to investigate the association between the sepsis and the biomarkers such as cytokines and coagulation markers, we propose a multiple imputation(MI) approach based on Tobit regression and Gibbs sampling. We conduct simulation study to evaluate the performance of our MI approach and use a sepsis dataset for demonstration.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biomarkers/surrogate markers,Missing data,nlazar@stat.uga.edu,,Nicole Lazar,,University of Georgia,Department of Statistics,(706) 542-0632,,nlazar@stat.uga.edu,On Combining and Contrasting Brains,1,Nicole,A.,Lazar,"Department of StatisticsUniversity of GeorgiaAthens, GA 30602",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A challenging problem in the statistical analysis of human brainfunction via functional magnetic resonance imaging (fMRI) is that ofcomparing activation across groups of subjects.  In the first part ofthis talk, I will discuss methods for 'combining' brains, that is,creating a map that summarizes the overall activity pattern of a groupof subjects.  This can be analogized to the old problem of combininginformation from independent studies, and I draw on techniqueshistorically used for that problem, to solve the current one.Once a map has been created for a single group of subjects, we canthink about 'contrasting', or comparing, the maps for multiple groups.While group comparisons are often accomplished via such standardtechniques as the random effect linear model, I will argue that thisapproach is potentially over-conservative, impairing the ability todetect differences of interest, which may be differences of extent, ofmagnitude, or both.  Instead, I propose extending various of themethods used in the first part of the talk for making group maps, viaa combination of statistical distribution theory and computationalprocedures (bootstrap and permutation).  In the second part of thistalk, I will discuss some of the issues that arise in extending groupmaps in this way, and some possible solutions.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Imaging,Data mining/massive data sets,sfwang@umich.edu,,Shufang Wang,,University of Michigan,2004 Medford Rd,734-763-4803,,sfwang@umich.edu,A self-consistent approach to multinomial logit model with repeated measures,1,Shufang,,Wang,University of Michigan,Alex,,Tsodikov,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"    Modeling repeatedly observed categorical response is a challenge in terms of computation, due to the complex form of likelihood function and the presence of random effects. The computation is costly especially when the categorical response has a large number of categories, since it involves a high-dimensional integration.  In this paper, we develop a stable MLE approach to the problem, based on generalized self-consistency and quasi-EM algorithm. The method transforms the complex multinomial likelihood to Poisson likelihood and hence allows for the estimates to be obtained iteratively and through a set of smaller problems. Simulation study indicates that the parameter estimates are consistent and stable.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Categorical data,Longitudinal data,meijuanl@biostat.umn.edu,,Meijuan Li,,"Division of Biostatistics, University of Minnesota",912 Juniper Ave.,507 645 5388,,meijuanl@biostat.umn.edu,Accelerated Failure Time Models for a Censored Quantitative Trait and Candidate Genes Association Mapping in Structured Populations,1,Meijuan,,Li,"Division of Biostatistics,University of Minnesota",Cavan,,Reilly,"Division of Biostatistics,University of Minnesota",Tim,,Hanson,"Division of Biostatistics,University of Minnesota",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Several statistical methods for detecting associations between quantitative traits and candidate genes in structured populations have been developed for fully observed phenotypes. However, many experiments are concerned with failure-time phenotypes, which are usually subject to censoring. In this paper, we developed statistical methods for detecting association between a censored quantitative trait and candidate genes in structured populations withcomplex multiple levels of relatednees among sampled individuals. The proposed methods corrects for continuous population stratification using population structure variables as covariates and the frailty terms attributable to kinship. The effects attributable to kinship are modeled via a Gaussian conditional autoregressive model with the kinship matrix being the adjacency matrix connecting subjects. We modeled the relationship between the time-at-onset data and genotypicscores at a candidate marker through a parametric Weibull accelerated failure time (AFT) model as well as a semiparametric AFT model where the baseline survival function is flexibly modeled as a mixture of Polya trees centered around a family of Weibull distributions. The proposed methods were applied to a real data set of 95 Arabidopsis lines and simulated data to demonstrate power, type I error rate, and precision.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Nonparametric methods,huizh@umich.edu,,Hui Zhang,,"Department of Biostatistics, University of Michiga",1984 Traver Rd Apt 201,734-277-6934,,huizh@umich.edu,Proportional hazards regression for the analysis of clustered survival data from case-cohort studies,1,Hui,,Zhang,"Department of Biostatistics, University of Michigan",Douglas,E.,Schaubel,"Department of Biostatistics, University of Michigan",John,D.,Kalbfleisch,"Department of Biostatistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Case-cohort sampling is a commonly used andefficient method for studying large cohorts. Most existing methodsof analysis for case-cohort data have concerned the analysis ofunivariate failure time data. However, clustered failure time dataare commonly encountered in public health studies. For example,patients treated at the same center are unlikely to be independent.In this article, we consider methods based on estimating equationsfor case-cohort designs for clustered failure time data. We assume amarginal hazard model, with a common baseline hazard and commonregression coefficient across clusters. The proposed estimators ofthe regression parameter and cumulative baseline hazard are shown tobe consistent and asymptotically normal, and consistent estimatorsof the asymptotic covariance matrices are derived. The regressionparameter estimator is easily computed using any standard Coxregression software that allows for offset terms. The proposedestimatorsare investigated in simulation studies, and demonstrated empiricallyto have increased efficiency relative to some existing methods. Theproposed methods are applied to a study of mortality among Canadiandialysis patients.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Clustered data methods,Survival analysis,zliu5@mail.med.upenn.edu,,Ziyue Liu,,University of Pennsylvania School of Medicine,503 Blockley Hall,(215)573-8950,,zliu5@mail.med.upenn.edu,Data driven adaptive spline smoothing with applications to epileptic EEG data,1,Ziyue,,Liu,University of Pennsylvania School of Medicine,Wensheng,,Guo,University of Pennsylvania School of Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The band power of 26-50Hz of EEG has been shown to be a potentialpredictor of epileptic seizure and can help neurologists locatingseizure's spatiotemporal initiation. It is flat before seizure, quickchanges around seizure and returns to stationary after seizure.Traditional methods smooth it using a global smoothing method, leadingto oversmooth theregions around seizure and undersmooth of other regions. In this paperwe propose an adaptive smoothingspline model where the smoothing parameter changes across the timedomain, allowing the model to adapt to the change of roughness inthe data. We model the penalty function by a stepfunction with data driven segmentation. We impose a binary treestructure on the step function and propose a Best Basis likesearch algorithm. We propose an AIC like criterion based on thegeneralized likelihood to select the optimal segmentation.  Wederive the state space representation for efficient computation. Theproposed method smaller true mean square errors comparing tonon-adaptive smoothing spline, wavelet shrinkage and Bayesianadaptive P-spline for a wide range of signals. Application toepileptic EEG example reveals that after remaining flat for 7minutes, the band power starts to rise 33 seconds before the onsetof seizure. This finding will enable development of short termpredictive model.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Functional data analysis,Nonparametric methods,BZHU@UMICH.EDU,,BIN ZHU,,"Department of Biostatistics, University of Michiga",1420 Washington Heights,734-330-0674,,BZHU@UMICH.EDU,Stochastic Functional Data Analysis:  A Diffusion Model-based Approach,1,Bin,,Zhu,"Department of Biostatistics, University of Michigan, Ann Arbor, USA",Peter,X.-K., Song,"Department of Biostatistics, University of Michigan, Ann Arbor, USA",Jeremy,M.G.,Taylor,"Department of Biostatistics, University of Michigan, Ann Arbor, USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider the problem of estimating an unknown smooth function givenfunctional data. The unknown function is treated as the realization ofa stochastic process. We propose a new type of continuous-discretestate space model, called a stochastic velocity model. The resultingmodel allows straightforward and meaningful interpretation. The methodof smoothing splines is a special case of this approach. Thelikelihood of the model is derived with Euler approximation and dataaugmentation. Bayesian inference is carried out via a Markov ChainMonte Carlo algorithm. The proposed model and method are illustratedusing a blood oxygenation-level dependent signal data, and prostate specific antigen data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Functional data analysis,Biomarkers/surrogate markers,chialing.kuo@gmail.com,,Eleanor Feingold,Associate Professor,"Department of Human Genetics and Biostatistics, Un",Crabtree Hall A310B 130 DeSoto Street,412-383-8599,412-624-3020,chialing.kuo@gmail.com,What's the best statistic for a simple test of genetic association in a case-control study?,1,Chia-Ling,,Kuo,"Department of Biostatistics, University of Pittsburgh",Eleanor,,Feingold,"Department of Human Genetics and Biostatistics, University of Pittsburgh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genome-wide genetic association studies typically start withunivariate statistical tests. In a case-control study, one can use a 1df allele-based test, a 1 df or 2 df genotype-based test, or acompound procedure that combines two or more of these statistics.While there are a number of statistical papers that make powercomparisons among subsets of these methods, none has comprehensivelytackled the question of which of the methods in common use is best forunivariate scanning in a genome-wide association study. In this paperwe consider a wide variety of realistic test procedures, and firstcompare the power of different procedures to detect a single locusunder different genetic models. We then address the question ofwhether or when it is a good idea to include covariates in theanalysis. Finally, we consider the performance of the statistics in agenome scan. In the genome-scan context our loss function for judgingthe statistics is not power to detect an individual locus, but ratherthe expected number of truly associated loci on a 'top-10' or 'top-20'gene list. Since different association tests have higher power fordifferent genetic models, our results must inevitably be viewed byindividual scientists in terms of their prior assumptions about whatmodels are most likely, but we are able to draw some generalconclusions about which methods are robust and broadly more powerfulthan others.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Epidemiologic methods,xw2144@columbia.edu,,Xiaoru Wu,,Columbia University,"Room 1021, SSW, 1255 Amsterdam Ave.",6462752476,,xw2144@columbia.edu,Comparison of Different Sample Size Designs ---Group Sequential versus Re-estimation,1,Xiaoru,,Wu,Columbia University,Lu,,Cui,Eisai Medical Research Inc.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Adaptive sample size designs include group sequential and sample size re-estimation methods. Although the group sequential likelihood ratio test has been proved to be optimal among all group sequential tests with a similar structure, it has some inherent inflexibility in the final sample size since its maximum sample size is fixed and it can only stop at several pre-specified information time.  Taking the flexibility in the final sample size into consideration, the simulation results based on a new performance assessment criterion suggest that the re-estimation method generally performs better than the group sequential method in terms of delivering more on the targeted final sample size and power. This is also true even when we select the optimal group sequential likelihood ratio test among all group sequential tests.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Power analysis/sample size,sshshoh1105@gmail.com,,Sunghee OH,Ph.D student,University of Pittburgh Dept of Biostatistics,130 Desoto Street 311 Parran Hall Graduate School of Public Health,4124807012,,sshshoh1105@gmail.com,The effects on identifying the significant genes to various MVs (Missing Values) imputation methods,1,Sunghee,,OH,"University of Pittsburgh, Graduate School of Public Health, Dept of Biostatistics",George,C.,Tseng,"University of Pittsburgh, Graduate School of Public HealthDept of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,lushuya@gmail.com,,SHUYA LU,,"UNIVERSITY OF PITTSBURGH, BIOSTAT",3407 WARD STREET,412-606-9141,,lushuya@gmail.com,Biomarker Detection Methods When Combining Multiple Multi-class Microarray Studies,1,Shuya,,Lu,"Department of Biostatistics, University of Pittsburgh, Pittsburgh, USA.",Jia,,Li,"Department of Biostatistics, University of Pittsburgh, Pittsburgh, USA.",George C,,Tseng,"Department of Biostatistics, University of Pittsburgh, Pittsburgh, USA.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As the microarray technology becomes mature in biomedical research, increasing number of datasets has been accumulated. Systematic information integration of multiple studies can provide improved biomarker detection. So far, published meta-analysis methods mostly consider two-class comparison. Methods for combining multi-class studies and pattern concordance are rarely explored. We first consider a natural extension of combining p-values from the traditional ANOVA model. Since p-values from ANOVA do not reflect the expression pattern information within classes, we propose a multi-class correlation measure (MCC) for biomarkers of concordant inter-class patterns across a pair of studies. For both approaches, we focus to identify biomarkers differentially expressed in all studies (ANOVA-maxP, min-MCC). Both ANOVA-maxP and min-MCC are evaluated by simulation studies and by applications to a multi-tissue mouse metabolism data set and a multi-platform mouse trauma data set. The results show complementary strength of the two methods for different biological purposes. When detecting only biomarkers with concordant inter-class patterns across studies, min-MCC has better power. If biomarkers with discordant inter-class patterns across studies are expected and are of interests together with concordant inter-class pattern genes, ANOVA-maxP better serve for the purpose.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Microarray analysis,,yhu@bios.unc.edu,,Yijuan Hu,,"Department of Biostatistics, UNC-CH","3101 McGavran-Greenberg Hall, Department of Biostatistics, CB#7420",919-357-6882,,yhu@bios.unc.edu,A General Framework for Estimating Genetic Effects and Gene-Environment Interactions with Missing Data,1,Yijuan,,Hu,"Department of Biostatistics, UNC-CH",Danyu,,Lin,"Department of Biostatistics, UNC-CH",Donglin,,Zeng,"Department of Biostatistics, UNC-CH",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Missing data arise in genetic associationstudies when genotypes are unknown or when haplotypes are ofdirect interest. We provide a general likelihood-based frameworkfor estimating genetic effects and gene-environment interactionswith such missing data. We allow genetic and environmentalvariables to be correlated while leaving the distribution ofenvironmental variables completely unspecified. We consider threemajor study designs--- cross-sectional, case-control, and cohort designs ---and construct appropriate likelihood functions for all commonphenotypes (e.g., case-control status, quantitative traits, andpotentially censored ages at onset of disease). The likelihoodfunctions involve both finite- and infinite-dimensionalparameters. The maximum likelihood estimators are shown to beconsistent, asymptotically normal, and asymptotically efficient.Fast and stable numerical algorithms are developed to implementthe corresponding inference procedures. Extensive simulationstudies demonstrate that the proposed inferential and numericalmethods perform well in practical settings. Illustration with agenomewide association study of schizophrenia is provided.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Missing data,sshshoh1105@gmail.com,,Sunghee OH,Ph.D student,University of Pittburgh Dept of Biostatistics,"130 Desoto Street, 311 Parran Hall",4124807012,,sshshoh1105@gmail.com,The effects of missing imputation on various down-stream analyses in microarray experiments,1,Sunghee,,OH,University of Pittsburgh Deparment of Biostatistics,George,C.,Tseng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Amongst the high-throughput technologies, DNA microarray experiments provide enormous quantity of genes and arrays with biological information to disease. Despite advances and the popular usage of microarray, the microarray experiments frequently produce multiple missing values due to many flaw factors. Thus, gene expression data contains some missing entries and a large number of genes may be affected. Many downstream algorithms for gene expression analysis require a complete matrix as an input.  For now, there exists no uniformly superior imputation method and the performance depends on the structure and nature of data set.  In addition, imputation methods have been mostly compared in terms of variants of RMSEs (Root Mean Squared Error) which compare true expression values to imputed values. The drawback of RMSE-based evaluation is that the measure does not reflect the true biological effect in down-stream analyses. In this study, we investigate how missing value imputation process affects the biological results of differentially expressed genes discovery, clustering and classification. Quantitative measures reflecting the true biological effects in each down-stream analysis will be used to evaluate imputation methods and compared to RMSE-based evaluation.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Microarray analysis,Missing data,microarray analysishhchang@jhsph.edu,,Howard Chang,,Johns Hopkins School of Public Health,516 North Wolfe Street,4106145124,,hhchang@jhsph.edu,Bayesian Model Averaging for Clustered Data: Imputing Missing Daily Air Pollution Concentrations,1,Howard,H,Chang,Johns Hopkins University,Roger,D,Peng,Johns Hopkins University,Francesca,,Dominici,Johns Hopkins University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Missing observations are often present and imputation has become apopular approach to handle missing data. When multiple competingregression models can be used for missing data imputation, Bayesianmodel averaging (BMA) provides a powerful tool for missing dataimputation and prediction. We develop a BMA-based missing dataimputation strategy for clustered data. Our approach has the featureof allowing the weights assigned to competing models to vary betweenclusters while borrowing information across clusters in estimatingmodel parameters. We first demonstrate the benefits of carrying outour proposed cluster-specific BMA through simulation studies. We thenapply the newly proposed method to a national dataset of daily ambientcoarse particulate matter (PM10-2.5) concentration between 2003 and2005. Using cross-validation, we demonstrate that cluster-specific BMAfor imputing missing air pollution time series data outperformsstandard approaches. Finally by using the national dataset withimputed PM10-2.5 data, we estimate the posterior probability ofPM10-2.5 nonattainment status for 95 US counties based on theEnvironmental Protection Agency's proposed 24-hour standard.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Environmental and ecological applications,zzxu@umich.edu,,ZHENZHEN XU,Student,"Department of Biostatistics, University of Michiga","Department of Biostatistics, University of Michigan",(617)8219536,,zzxu@umich.edu,Propensity Score Matching in Randomized Clinical Trial,1,Zhenzhen,,Xu,"Department of Biostatistics, University of Michigan",John,D,Kalbfleisch,"Department of Biostatistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Cluster randomization trials with relatively few clusters have beenwidely used in recent years for evaluation of health care strategies.On average, randomized treatment assignment achieves balance in bothknown and unknown confounding factors between treatment groups,however, inpractice investigators can only introduce a small amount ofstratification and cannot balance on all the important variablessimultaneously. The limitation arises especially when there are manyconfounding variables and in small studies. Such is the case in theINSTINCT trial designed toinvestigate the effectiveness of an education program in enhancing thetPA use in stroke patients. In this paper, we introduce a newrandomization design, the balance match weighted (BMW) design, whichapplies the optimal matching with constraints technique to aprospective randomized design and aims of to minimize the mean squarederror of the treatment effect estimator. When true confounding effectsare known, we construct the BMW design to produce treatment effectestimators with minimal MSE; these results suggest that, even when theconfounding effects are unknown, the BMW design with appropriatelychosen parameters can generate treatment effect estimators withsubstantially improved MSE properties. The simulation study showsthat, under various confoundingscenarios, the BMW design can reduce the MSE for the treatmentestimator by 10% to as much as 80% compared to a completely randomizedor matched-pair design. We illustrate these methods inproposing a design for the INSTINCT trial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Experimental design,Clinical trials,wcao5@ncsu.edu,,Weihua Cao,,North Carolina State University,2125A Gorman St,919-793-8198,,wcao5@ncsu.edu,Improving efficiency and robustness of the doubly robust estimator for a population mean with incomplete data,1,Weihua,,Cao,"Department of Statistics, North Carolina State University",Anastasios,A.,Tsiatis,"Department of Statistics, North Carolina State University",Marie,,Davidian,"Department of Statistics, North Carolina State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Considerable recent interest has focused on doubly robust estimatorsfor a population mean response in the presence of incomplete data,which involve models for both the propensity score and the regressionof outcome on covariates.  The 'usual' doubly robust estimator mayyield severely biased inferences if neither of these models iscorrectly specified and can exhibit nonnegligible bias if theestimated propensity score is close to zero for some observations.  Wepropose alternative doubly robust estimators that achieve comparableor improved performance relative to existing methods, even with someestimated propensity scores close to zero.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Missing data,jhsong@gmail.com,,Juhee Song,Sr. Biostatistician,Scott & White Hospital,2401 South 31st Street,254-724-5391,254-724-2495,jhsong@gmail.com,Comparison of Correlated Correlation Coefficients using Bootstrapping,1,Juhee,,Song,Scott & Whote Hospital,Jeffrey,D,Hart,Texas A&M University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many clinical researches have been studied comparison of a gold standard measure and other surrogate measures using diagnostic test to find the best surrogate measure. In many cases a cutoff point for diagnostic test is not well established and depends on study population.Instead of applying diagnostic test, correlations between a gold standard measure and other surrogate measures were compared to find the best surrogate measure. A method of comparing correlated pearson's correlation coefficients using the Fisher's transformation proposed by Meng et al was adapted and modified.   A test that the highest correlation coefficient is significantly better than one of other correlation coefficients was studied. A critical values from Gaussian distribution may not be appropriate, since the distribution of a correlation coefficient, which is not the highest, subtracts from the highest correlation coefficient is not nornally distributd. A linear model that considered a standardized gold standard as response variable, and standardized other surrogate measures as explanatory variables was motivated and all parameters were estimated with least square estimation. Bootstrap samples from surrogate measures and residuals were taken to construct the sampling distribution of a test statistic. Simulation study was done.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Diagnostic and screening tests,Nonparametric methods,lanjia@stat.fsu.edu,,Lanjia Lin,,Florida State University,326 Pennell Cir. Apt 6,850-320-5562,,lanjia@stat.fsu.edu,Association Models for Clustered Data with Bivariate Mixed Responses,1,Lanjia,,Lin,"Department of Statistics, Florida State University",Debajyoti,,Sinha,"Department of Statistics, Florida State University",Stuart,,Lipsitz,Harvard Medical School,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider fully specified models and associated likelihood andBayesian analysis of clustered data with mixed bivariate responses. We present a novel bivariate random effects model whichinduces associations among the binary outcomes within a cluster,among the continuous outcomes within a cluster, between a binary anda continuous outcome from different subjects within a cluster, aswell as the direct association between the binary and continuousoutcomes within the same subject. For the ease of interpretation ofthe regression effects, the marginal model of the binary responseprobability integrated over the random effects preserves thelogistic form and the marginal regression function of the continuousresponse preserves the linear form. We implement maximum likelihoodestimation of model parameters using standard software such as PROCNLMIXED of SAS, as well as fully parametric Bayesian analysis. Weextend our fully parametric model to accommodate a semiparametricBayesian model using a Dirichlet mixture of normal for thecontinuous response. Posterior computations for both parametric andsemiparametric models are implemented via Markov Chain Monte Carlosampling methods. We illustrate our methodology by analyzing adevelopmental toxicity study of ethylene glycol in mice using thethree methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Generalized linear models,Clustered data methods,robin_mogg@merck.com,,Robin Mogg,,University of Pennsylvania,207 Cambridge Place,215-822-0754,,robin_mogg@merck.com,A Causal Selection Model to Compare Treatment Groups in a Subset Selected Post-Randomization with Application to an HIV Antiretroviral Immunotherapy Trial,1,Robin,,Mogg,"Department of Biostatistics and EpidemiologyUniversity of Pennsylvania School of Medicine",Marshall,M,Joffe,"Department of Biostatistics and EpidemiologyUniversity of Pennsylvania School of Medicine",Devan,V,Mehrotra,Merck Research Laboratories,Thomas,R,Ten Have,"Department of Biostatistics and EpidemiologyUniversity of Pennsylvania School of Medicine",,,,,,,,,,,,,,,,,,,,,,,,,"A successful therapeutic HIV vaccine could offer significant advantages in fighting the HIV epidemic, including reducing the costs and potential toxicities associated with antiretroviral therapy (ART) by allowing patients to have structured treatment interruptions and potentially contributing to a delay in the onset of AIDS-defining illnesses or death.  In this paper, we describe a proof-of-concept (POC) antiretroviral immunotherapy (ARI) trial that was initiated to assess whether immunization with an experimental HIV vaccine has a measurable impact on the control of viremia following subsequent interruption of ART.  For a number of reasons, some patients will not interrupt ART after vaccination; as such, a comparison of outcomes among vaccine and placebo patients who interrupt ART must adjust for a potential selective effect of the vaccine on the post-randomization event of ART interruption.  We propose a selection model that utilizes the principal stratification framework developed by Frangakis and Rubin (2002) without imposing assumptions on the direction of the selective effect of the vaccine.  Methods to test the causal effect of the vaccine on viral load among the principal stratum of patients who would interrupt treatment regardless of randomization assignment are developed.  Finite sample properties of the testing methodology are assessed via computer simulation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Clinical trials,omalley@hcp.med.harvard.edu,,A James O'Malley,Associate Professor of Statistics,Harvard Medical School,Department of Health Care Policy,6174323493,6174322563,omalley@hcp.med.harvard.edu,The Role of Health and Health Behaviors in the Formation and Dissolution of Friendship Ties,1,A James,,O'Malley,Harvard Medical School,Nicholas,A,Christakis,Harvard Medical School,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"An important question in social network analysis is whether observed association in individual characteristics between connected individuals in a network is a consequence of social influences (i.e., forces that act once the tie is formed) or whether individuals who are similar (dissimilar) are more likely to form (break) ties. The latter mechanism is known as homophily and is commonly described as birds of a feather flock together.  In this talk we describe an approach for estimating the magnitude of the homophily effect, and examine the extent to which ties in a social network form or dissolve as a function of the similarity (or lack thereof) of individuals health-related traits. We also investigate whether the health traits have greater effects than the non-health unchangeable traits. If so, this would suggest that those traits which are affected by sociological phenomena have a greater influence on the dynamic behavior of the network. The health behaviors and traits considered are: Body mass index, smoking, depression, and hypertension. For comparison, we also consider two non-health traits, height and handedness (left or right-handed) that are immutable.",FALSE,FALSE,T5: Analysis of Censored Cost or Health Outcomes Data,FALSE,TRUE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Random effects,awagler2@utep.edu,,Amy Wagler,Dr. Amy Wagler,U of Texas at El Paso,500 W. University Ave.,915-747-6847,,awagler2@utep.edu,Penalized Maximum Likelihood Estimation in Logistic Dose Response Models,1,Amy,E,Wagler,U of Texas at El Paso,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Logistic regression is a widely utilized method of modeling quantal dose response data.  These models typically employ maximum likelihood (ML)methods for estimating the model parameters.  Maximum likelihood estimators (MLEs) are well-known to be biased at smaller sample sizes.  This bias can lead to very misleading results when the number of doses or the number of replications per dose is small. For example, past simulations indicate that the empirical coverage level for quantities such as the effective dose (ED) can be as low as 18.8% for models utilizing ML estimation with low numbers of dose levels. Additionally, when there are small numbers of doses or few replications per dose, the ML estimators often fail to converge. An alternative estimator, the Penalized Maximum Likelihood Estimator (pMLE), is considered in order to address the bias present in the MLE and the problem of non-convergence.  Simulations confirm that less bias is present when utilizing the pMLE in place of the MLE in quantal dose-response models with and without a covariate. Additionally, simulations compare the empirical coverage levels of intervals for response probabilities and delta and Fieller intervals for the effective dose (ED).",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,TRUE,Biopharmaceutical research,"Biologics, pharmaceuticals, medical devices",benn@uga.edu,,Benjamin Neustifter,,University of Georgia,Department of Statistics,(706) 254-6795,,benn@uga.edu,Estimator of the Intensity of a Modulated Poisson Process with a Gamma Prior,1,Benjamin,B,Neustifter,University of Georgia,Stephen,L,Rathbun,University of Georgia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The works of Rathbun, Shiffman, and Gwaltney (2007) and Waagepetersen(2008) on using modulated Poisson processes to model events based ontime-varying covariates are extended to allow for variation amongsubjects in baseline rates.  Estimating functions for covariateparameters are proposed and their large-sample properties areexamined, with proof that the resulting estimators are consistent andasymptotically normal.  The approach is illustrated using data from anecological momentary assessment of smoking.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Other,Random effects,Point processesycheng3@jhsph.edu,,Yu-Jen Cheng,Mr.,Johns Hopkins University,615 N. Wolfe St. E3035,410-502-3364,,ycheng3@jhsph.edu,Cox Models with Smooth Functional Effect of Covariates Measured with Error,1,Yu-Jen,,Cheng,"Department of Biostatistics, Johns Hopkins University ",Ciprian,,Crainiceanu,"Department of Biostatistics, Johns Hopkins University ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose, develop and implement a fully Bayesian inferentialapproach for the Cox model when the log hazard function containsunknown smooth functions of the variables measured with error. Ourapproach is to model nonparametrically both the log-baseline hazardand the smooth components of the log-hazard functions using low-rankpenalized splines. The likelihood of the Cox model is coupled withthe likelihood of the measurement error process. Carefulimplementation of the Bayesian inferential machinery is shown toproduce remarkably better results than the naive approach. Ourmethodology was motivated by and applied to the study of progressiontime to chronic kidney disease (CKD) as a function of baseline kidney function and applied to the Atherosclerosis Risk in Communities (ARIC) study, a large epidemiological cohort study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Measurement error,Nonparametric methods,zhiying.xu@bms.com,,Zhiying Cindy Xu,,BMS,311 Pennington-Rocky Hill Road,609-818-6105,,zhiying.xu@bms.com,A Comparison of Meta-analysis Methods for Rare Events in Clinical Trials,1,Zhiying,,Xu,Bristol-Myers Squibb Company,Mark,,Donovan,Bristol-Myers Squibb Company,David,H,Henry,Bristol-Myers Squibb Company,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Rare adverse events such as deaths or cardiac disorders observed in clinical trials are crucial to assess the safety of new drugs. However, in most cases, single trials are not designed to evaluate such kind of outcomes and inherently the tests are largely underpowered. Meta-analysis is commonly used to synthesize results from individual studies to support the claimed effect with improved power. When performing the meta-analysis for clinical studies, the possibility that the control group may have less duration of follow-up than the experimental group should not be ignored.  In addition, time-dependent event rates can affect the ability to adjust for follow-up imbalances. Through simulation, we evaluate the performance of multiple methods with and without consideration of time-dependent factors with respect to the accuracy and precision of the estimates. Simulations are based on a diabetes project with different event rates and relative risks. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biopharmaceutical research,Clinical trials,mjuraska@u.washington.edu,,Michal Juraska,MS,University of Washington,Department of Biostatistics,(206) 543-1044,,mjuraska@u.washington.edu,"Longitudinal changes in carotid IMT and risk of MI, stroke and CHD: The Cardiovascular Health Study",2,David,,Yanez,"Department of Biostatistics,University of Washington",Michal,,Juraska,"Department of Biostatistics,University of Washington",Bruce,M,Psaty,"Departments of Epidemiology & Internal Medicine,University of Washington",Mary,,Cushman,"Department of Pathology and Laboratory Medicine,University of Vermont",Cam,,Solomon,"CHSCC, Department of Biostatistics,University of Washington",Joseph,F,Polak,"Department of Radiology,Tufts University",Daniel,,O'Leary,"Department of Radiology,Tufts University",,,,,,,,,,,,,"Ordinary regression calibration (ORC) is a popular method employed asa bias-correcting technique in regression models with covariatemeasurement error. In the analysis of failure time data, ORC does notaccount for possible dependence of survival or censoring probabilitieson mismeasured covariates, suggesting a potential bias reduction byrecalibrating within each risk set (RSRC). Xie et al. (2001) proposedsuch a method for measurement error correction for time independentpredictors. The purpose of this work is to evaluate differences in ORCand RSRC in a Cox regression model with mismeasured time-dependentcovariates. To this end, an association study of longitudinal changein carotid intima-media thickness (IMT) and the risk of MI, stroke andCHD is presented. As one might expect, ignoring measurement errorinduced bias in the analysis leads to erroneous conclusions. Both theRSRC and ORC method produce greater cross-sectional and smallerlongitudinal effects of IMT on the risk of each CVD event as comparedto the naïve procedure. A reduction in bias is accompanied by anincrease in the standard error of the longitudinal IMT effect whereasthe precision of the cross-sectional IMT effect remains stable acrossthe considered methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Measurement error,Survival analysis,Haiyan_Su@urmc.rochester.edu,,Haiyan Su,,"Dept. of Biostatistics and Computational Biology,","601 Elmwood, Box 630",(585)-233-0010,,Haiyan_Su@urmc.rochester.edu,Comparison of Treatment Effects-An Empirical Likelihood Based Method,1,Haiyan,,Su,University of Rochester,Hua,,Liang,University of Rochester,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"To compare two treatment effects, which can be described as the difference of the parameters in two linear models, we propose an empirical likelihood based method to make inference for the difference. Our method is free of the assumptions of normally distributed and homogeneous errors, and equal sample sizes. The empirical likelihood ratio for the difference of the parameters of interest is shown to be asymptotically chi-squared. Simulation experiments illustrate that our method outperforms the published ones. Our method is used to analyze a data set from a drug study",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Empirical likelihood,Nonparametric methods,hsingyi@nhri.org.tw,,Hsing-Yi,Associate Investigator,National Health Research Institutes,"35, Keyan Road, A3223",886-37-246166 ext. 36333,886-37-586261,hsingyi@nhri.org.tw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,alazar@hsph.harvard.edu,,Ann Lazar,Postdoctoral Fellow,Harvard School of Public Health and Dana Farber Ca,44 Binney Street,6175828849,,alazar@hsph.harvard.edu,Determining When Time Response Curves Differ in the Presence of Censorship with Application to a Rheumatoid Arthritis Biomarker Study,1,Ann,A,Lazar,Harvard School of Public Health & Dana-Farber Cancer Institute,Gary,O,Zerbe,"University of Colorado, Denver",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a method to determine the significance regions of time response curves in random coefficient models with censored data.  Here, significance region refers to the set of times for which the curves differ, and censored data arise when all that is known is that the data are less than or more than a known value.  An explicit solution provides the set of times for which the two curves (population average or subject specific) generated from the models significantly differ with adjustment of the resulting significance level via the Scheffe method for multiple comparisons.  The application and motivation for this methodology is from a longitudinal biomarker study of subjects with rheumatoid arthritis.  For example, it is useful to determine when left censored autoantibody levels (such as rheumatoid factor) differ between the cases and controls during the pre-diagnosis study period.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Multiple testing,hui_zhang@urmc.rochester.edu,,Hui Zhang,,University of Rochester,601 Elmwood Avenue Box 630,(585) 275-9645,(585) 273-1031,hui_zhang@urmc.rochester.edu,How to Appropriately Estimate IC50 When Inhibitory Drug Produces a Dose-dependent Stimulation of HIV Replication,1,Hui,,Zhang,"Department of Biostatistics and Computational Biology,University of Rochester Medical Center",Jeanne,,Holden-Wiltse,"Department of Biostatistics and Computational Biology,University of Rochester Medical Center",Jiong,,Wang,"Departments of Medicine and Microbiology & Immunology,University of Rochester Medical Center",Lisa,M,Demeter,"Departments of Medicine and Microbiology & Immunology,University of Rochester Medical Center",Hua,,Liang,"Department of Biostatistics and Computational Biology,University of Rochester Medical Center",,,,,,,,,,,,,,,,,,,,,"     The half maximal inhibitory concentration (IC50) is an importantpharmacodynamical index of drug effectiveness. To estimate this value,the dose-response relationship needs to be established which isgenerally done by fitting monotonic sigmoidal models. However, recentstudies of HIV mutation resistance to antiviral drugs show that thedose-response curve may not be monotonic. The traditional models canfail, and ignore observations which may reflect important biologicalsignificance. We propose a nonparametric model to describe thedose-response relationship, and fit the curve using local polynomialregression. The nonparametric approach is shown to be promisingparticularly for estimating IC50 of HIV inhibitory drugs which displaya dose-dependent stimulation of response when resistance mutations arepresent. This model strategy may be applicable for more generalpharmacological, toxicological, or other biomedical data which exhibita non-monotonic dose-response relationship and where traditionalparametric models fail.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_poster,,,,FALSE,FALSE,FALSE,TRUE,Toxicology/dose-response,Nonparametric methods,seongmin.han@yale.edu,,Summer Seongmin Han,Graduate Student,Yale University,24 Hillhouse Avenue,203-432-0666,,seongmin.han@yale.edu,Reconsidering the asymptotic null distribution of likelihood ratio tests for genetic linkage in multivariate variance components models,1,Summer,S,Han,"Department of Statistics, Yale University",Joseph,T,Chang,"Department of Statistics, Yale University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Accurate knowledge of the null distribution of hypothesis tests isimportant for valid application of the tests.  In previous papers andsoftware, the asymptotic null distribution of likelihood ratio testsfor detecting genetic linkage in multivariate variance componentsmodels has been stated to be a mixture of chi-square distributionswith binomial mixing probabilities.  Here we show, by simulation andby theoretical arguments based on the geometry of the parameter space,that all aspects of the previously stated asymptotic null distributionare incorrect-both the binomial mixing probabilities and thechi-square components.  Correcting the null distribution gives moreconservative critical values than previously stated, yielding P valuesthat can easily be ten times larger.  The true mixing probabilitiesgive the highest probability to the case where all variance parametersare estimated positive, and the mixing components show severedepartures from chi-square distributions.  Thus, the asymptotic nulldistribution has complex features that raise challenges for theassessment of significance of multivariate linkage findings.  Wepropose a method to generate an asymptotic null distribution that ismuch faster than other empirical methods such as gene-dropping,enabling us to obtain P values with higher precision more efficiently.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Random effects,sheldon.wang@sanofi-aventis.com,,Xiaodong (sheldon) wang,,sanofi-aventis,4 Savannah Ct,908-304-7154,,sheldon.wang@sanofi-aventis.com,A Full Exchangeable Negative Binomial Likelihood Procedure For Modeling Correlated Overdispersed Count Data,1,xiaodong,,wang,Sanofi-aventis,Hanxiang,,Peng,Indiana University - Purdue University At Indianapolis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We introduce an exchangeable negative binomial by relaxing independence to exchangeability in the negative binomial. Based on this model, we propose a full likelihood procedure for investigating overdispersed and correlated binary data common in biomedical sciences and teratology. The proposed model can be characterized by a completely monotone sequence of infinitely many parameters and used to model all distributional information. We give two methods about converting from the distribution of infinitely many parameters to a parsimonious distribution of finitely many parameters, i.e., truncation and completely monotone links. We calculate the moments and perform simulation to illustrate the distribution. We also provide an estimating procedure based on maximum likelihood and the empirical estimates.",FALSE,FALSE,,FALSE,FALSE,TRUE,"SC5: statistical modeling and analysis of brain imaging dataR10: Information priors and sensitivity analysis for missing data",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Biopharmaceutical research,Categorical data,lyu2@email.uky.edu,,Lei Yu,,University of Kentucky,"333 Waller Avenue, Suite 206",8592576777,,lyu2@email.uky.edu,A nonhomogeneous Markov transition model for computing the probability of dementia before death,1,Lei,,Yu,"Department of Statistics, University of Kentucky",William,S,Griffith,"Department of Statistics, University of Kentucky",David,,Snowdon,"Sanders-Brown Center on Aging,Department of Neurology University of Kentucky",Richard,J,Kryscio,"Department of Statistics, Department of Biostatistics,University of Kentucky",,,,,,,,,,,,,,,,,,,,,,,,,"This paper investigates the long term behavior of the k-steptransition probability matrix for a nonhomogeneous discrete timeMarkov chain in the context of modeling transitions from intactcognition to dementia with mild cognitive impairment (MCI) and globalimpairment (GI) as intervening cognitive states. The authors deriveformulas for the following absorption statistics: (1) the relativerisk of absorption between competing absorbing states, and (2) themean and variance of the number of visits among the transient statesbefore absorption. Since absorption is not guaranteed, sufficientconditions are discussed to ensure that the substochastic matrixassociated with transitions among transient states converges to zeroin limits. Results are illustrated with an application in the NunStudy, a cohort of 678 participants, 75 to 107 years of age, followedlongitudinally with up to ten cognitive assessments over a fifteenyear period.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Applied data analysis,jamyers@jhsph.edu,,Jessica A. Myers,PhD Candidate,Johns Hopkins University,15 E. Eager St.,(410)502-3365,(410)955-0958,jamyers@jhsph.edu,Bayesian Hierarchical Models for Extracting Useful Information from Medication Error Reports,1,Jessica,A,Myers,"Bloomberg School of Public HealthJohns Hopkins University",Francesca,,Dominici,"Bloomberg School of Public HealthJohns Hopkins University",Laura,,Morlock,"Bloomberg School of Public HealthJohns Hopkins University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Medical errors originating in healthcare facilities are a significantsource of unnecessary morbidity, mortality, and healthcare costs.Voluntary error report systems that collect information on the causes andcontributing factors of medical errors and the resulting harm may beuseful for developing effective harm prevention strategies. Somepatient safety experts question the utility of data from errors thatdid not result in harm, also called near-misses. We use data from alarge voluntary reporting system of 836,174 medication errors from1999 to 2005 to provide evidence for the causal continuum hypothesis,which states that the causes and contributing factors of errors thatresult in patient harm are similar to thecauses and contributing factors of errors that do not result inpatient harm. Bayesian hierarchical models are developed forestimating the log odds of each cause (or contributing factor) givenharm and the log odds of each cause given that harm did not occur. Theposterior distribution of the correlation between these two vectors oflog odds is used as a measure of the evidence for the hypothesis. Inaddition, we identify the causes and contributing factors that mostlikely deviate from the hypothesis, and thereforehave the highest or lowest odds of harm. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Health services research,Bayesian methods,shojaie@umich.edu,,Ali Shojaie,,University of Michigan,"269 West Hall, 1085 South Univ Ave",7347863463,,shojaie@umich.edu,Network Based Gene Set Analysis Under Temporal Correlation,1,Ali,,Shojaie,University of Michigan,George,,Michailidis,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Cellular functions of living organisms are carried out through complexsystems of interacting components. Including such interactions in theanalysis and considering subsystems instead of individual componentscan unveil new facts about complex mechanisms of life. Networks areoften used to demonstrate the interactions among components ofbiological systems and can be efficiently incorporated in the model toimprove efficiency in estimation and inference. In this paper, wepropose a model for incorporating external information about theunderlying network in differential analysis of gene sets. The modelprovides a flexible framework for analysis of complex experimentaldesigns and can efficiently incorporate temporal correlations amongobservations. The model is applied to real data on yeast environmentalstress response (ESR) as well as simulated data sets.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Genomics,salk0008@umn.edu,,Nicholas J. Salkowski,,"University of Minnesota, School of Public Health","A460 Mayo Building, MMC 303",612-624-0131,,salk0008@umn.edu,Evaluating Predictions of Event Probabilities from a Joint Model for Longitudinal and Event Time Data,1,Nicholas,J,Salkowski,"University of MinnesotaDivision of BiostatisticsSchool of Public Health",Melanie,M,Wall,"University of MinnesotaDivision of BiostatisticsSchool of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many clinical trials periodically measure participants until an event,generating longitudinal and event time data. Joint modelssimultaneously model the longitudinal data and the event times. Oneclass of joint models uses latent variables to model associationsbetween longitudinal trajectories and event risks. Both event timesand longitudinal data influence the fitted values of the latentvariables, which determine a subjects true trajectory and survival.We may wish to predict the probability of an event conditional onobserved longitudinal measurements. Clinical heart failure trial dataare used to evaluate approaches to predicting survival usinglongitudinal measurements of disease impact on daily life. Theapproach of Schoop et al. (2008) is used to compare the predictionerrors from a joint model, a two-step model, a nonparametricKaplan-Meier model, and a parametric Weibull model without covariatesfor several scenarios. The two-step model had the lowest predictionerror, and the joint model had the highest prediction error.Simulations show that mean prediction error for the two-step model andthe joint model are comparable under a scenario like the motivatingdata set.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Joint models for longitudinal and survival data,Latent variables,janelle.k.charles@ttu.edu,,Janelle K Charles,,Texas Tech University,Department of Mathematics and Statistics,8067422566,,janelle.k.charles@ttu.edu,Estimation of Probability Distributions using Control Theoretic Splines,1,Janelle,K,Charles,Texas Tech Univeristy,Clyde,F,Martin,Texas Tech Univeristy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We examine the relationship between optimal control and statistics. We explore the use of control theoretic smoothing splines in the estimation of continuous probability distribution functions defned on a finite interval [0,T], where the data is summarized by empirical probability distributions. In particular, we consider the estimation of distributionsof the form exp(f(t)), where there is no restriction on the sign of f(t). The construction of the optimal smoothed curve, y(t), is based on the minimization of an integral cost function done through the application of the Hilbert Projection Theorem, which guarantees that a uniqueminimum exists.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_poster,,,,FALSE,FALSE,FALSE,FALSE,Other,Computational methods,Distribution estimationmpa@ksu.edu,,Michael Anderson,,Kansas State University,2103 Prairie Field,785-532-0527,,mpa@ksu.edu,DNA BARCODING: BAYESIAN DISCRETE ORDERED CLASSIFICATION,1,Michael,P,Anderson,Kansas State University,Suzanne,R,Dubnicka,Kansas State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"DNA barcodes are short strands of nucleotide bases taken from the cytochrome c oxidase subunit 1 (COI) of the mitochondrial DNA (mtDNA).  Unlike nuclear DNA (nDNA), mtDNA remains largely unchanged as it is passed from mother to offspring, and it has been proposed that these barcodes may be used as a method of differentiating between biological species (Hebert 2003).  While this proposal is sharply debated among some taxonomists (Will 2004), it has gained much momentum and attention from biologists.  One issue at the heart of the controversy is the use of genetic distance measures as a tool for species differentiation.  Current methods of species classification utilize these distance measures that are heavily dependent on both evolutionary model assumptions as well as a clearly defined 'gap' between intra- and interspecies variation (Meyer 2005).  We point out the limitations of such distance measures and propose a character-based method of species classification which utilizes an application of Baye's rule to overcome these deficiencies.  The proposed method is shown to provide accurate species-level classification as well as answers to important questions not addressable with current methods.   ",FALSE,FALSE,,FALSE,FALSE,TRUE,Would prefer not to present on Sunday March 15 for religous reasons.,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Bayesian methods,bibhas@umich.edu,,Bibhas Chakraborty,,"Department of Statistics, University of Michigan","439 West Hall, 1085 South University Avenue",734-645-2534,734-763-4676,bibhas@umich.edu,Inference for Nonregular Parameters in Optimal Dynamic Treatment Regimes,1,Bibhas,,Chakraborty,"Department of Statistics, University of Michigan",Susan,,Murphy,"Department of Statistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A dynamic treatment regime is a set of decision rules, one perstage, that takes a patient's treatment and covariate history asinput, and outputs a recommended treatment. In the estimation ofthe optimal dynamic treatment regime from longitudinal data, thetreatment effect parameters at any stage prior to the last can benonregular under certain distributions of the data. This resultsin biased estimates and invalid confidence intervals for thetreatment effect parameters. In this paper, we discuss the problemof nonregularity, and present an estimation method that addressesthe problem. We also provide a simulation study to compare ourproposed estimator with the original estimator under a variety ofnonregular scenarios.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Clinical trials,guojian@umich.edu,,Jian Guo,,"Department of Statistics, University of Michigan,","436 West Hall, 1085 South University",734-883-6958,,guojian@umich.edu,Pairwise Variable Selection for High-dimensional Model-based Clustering and Its Application to Microarray Data,1,Jian,,Guo,"Department of Statistics, University of Michigan, Ann Arbor",Elizaveta,,Levina,"Department of Statistics, University of Michigan, Ann Arbor",George,,Michailidis,"Department of Statistics, University of Michigan, Ann Arbor",Ji,,Zhu,"Department of Statistics, University of Michigan, Ann Arbor",,,,,,,,,,,,,,,,,,,,,,,,,"Gene (variable) selection for high-dimensional model-based clustering is an important yet challenging problem in microarray analysis. Existing variable selection methods for model-based clustering select informative variables in an ``one-in-all-out' manner; that is, a variable is selected if at least one pair of clusters are separable by this variable and is removed if all clusters are nonseparable by this variable. In many situations, however, biologists are also interested in knowing which clusters are separable and which clusters are nonseparable for each informative variable. To address this problem, we propose a pairwise variable selection method for high-dimensional model-based clustering. We provide some evidence that our new method performs better than the $l_1$-norm approach and offers better interpretation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Machine learning,Computational methods,minqian@umich.edu,,Min Qian,,University of Michigan,"439 West Hall, 1085 S. University Ave.",7342764305,,minqian@umich.edu,Performance Guarantee for Individualized Treatment Rules,1,Min,,Qian,"Department of Statistics, University of Michigan",Susan,A,Murphy,"Department of Statistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"An individualized treatment rule is a decision rule that assignstreatment according to patient characteristics. Our goal is toestimate the individualized treatment rule that yields the maximalmean response using data from a randomized trial. We consideralgorithms based on prediction error minimization and derive anupper bound on the excess mean response of any decision rule interms of its associated excess prediction error. This upper bound issharp if a margin type condition holds. To ensure a goodapproximation and avoid overfitting, LASSO is used to minimize theprediction error. We study the finite sample properties of the LASSOestimator under mild conditions. Combining the above two results, weobtain an oracle inequality of the excess mean response of theestimated individualized treatment rule. Results from simulationsand a real example are provided.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Other,Machine learning,Decision makingyimeili@mail.med.upenn.edu,,Yimei Li,,"Department of Biostatistics and Epidemiology, Univ","503 Blockley Hall, 423 Guardian Drive",2678253817,,yimeili@mail.med.upenn.edu,Modeling survival data with alternating states and a cure fraction using frailty models,1,Yimei,,Li,"Department of Biostatistics and Epidemiology, University of Pennsylvania",Paul,E,Wileyto,"Department of Psychiatry, University of Pennsylvania",Daniel,F,Heitjan,"Department of Biostatistics and Epidemiology, University of Pennsylvania",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We introduce a cure model for survival data where a common frailtyinfluences both the cure probability and the hazard function given notcured. Data generated from this model have a close-form likelihood,making it straightforward to obtain maximum likelihood estimates(MLEs). We then extend our model to data with multiple events andalternating states, using a Clayton copula to link two gammafrailties, one for each type of event. We illustrate the model with ananalysis of data from two smoking cessation trials comparing bupropionand placebo, in which each subject potentially experienced a series oflapse and recovery events. The model suggests that bupropion increasesthe probability of being abstinent for good, and decreases the hazardof lapse. However, bupropion does not significantly influence theprobability of abandoning the quit attempts, nor does it acceleratetime to recovery significantly. The data also suggest a positive butnot significant association between lapse and recovery. A simulationstudy suggests that the estimates have little bias and their 95%confidence intervals (CI) have nearly nominal coverage.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Longitudinal data,yswu@bios.unc.edu,,Yuanshan Wu,,"Deparment of Biostatistics, University of North Ca","Deparment of Biostatistics, CB#7429, University of North Carolina at Chapel Hill",919-966-7788,,yswu@bios.unc.edu,Additive-Multiplicative Rates Model for Recurrent Events,1,Yuanshan,,Wu,"Wuhan University, China and University of North Carolina at Chapel Hill",Yanyan,,Liu,"Wuhan University, China",Jianwen,,Cai,University of North Carolina at Chapel Hill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recurrent events are frequently encountered inbiomedical studies. Evaluating the  covariates effects on themarginal recurrent event rate is  of practical interest. There aremainly two types of rate models for the recurrent event data: themultiplicative rates model and the  additive rates model. Weconsider a more flexible additive-multiplicative rates model foranalysis of recurrent event data, wherein some covariate effects areadditive while others are multiplicative. We formulate estimatingequations for estimating the regression parameters. The estimatorsfor these regression  parameters are shown to be consistent andasymptotically normally distributed under appropriate regularityconditions. Moreover, the  estimator of the baseline mean functionis proposed and its  large sample properties are investigated. Inaddition, a simple graphical and numerical procedure based oncommonly used cumulative residual process is presented to assess thegoodness-of-fit of the model. We also investigate the finite samplebehavior of the proposed estimators through simulation studies. Awell-known medical study of bladder cancer is analyzed  forillustration of the proposed method.",FALSE,FALSE,T5: Analysis of Censored Cost or Health Outcomes Data,FALSE,FALSE,TRUE,Recurrent event data analysis,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Multivariate survival,Survival analysis,lyrica@umich.edu,,Lyrica Xiaohong Liu,Ph.D. Candidate,University of Michigan,"M4048C SPH II, 1420 Washington Heights",734-936-4018,,lyrica@umich.edu,Multiple Imputation Based on Restricted Mean Models for Censored Survival Data,1,Lyrica Xiaohong,,Liu,University of Michigan,Susan,,Murray,University of Michigan,Alex,,Tsodikov,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Most multiple imputation methods for censored survival data either ignore patient characteristics when imputing a likely event time, or place quite restrictive modeling assumptions on the survival distributions used for imputation. In this research, we propose a multiple imputation approach that directly imputes restricted lifetimes over the study period based on a model of the mean restricted life as a linear function of covariates. This method retains patient characteristics through the model on the mean structure when making imputation choices, but does not make assumptions on the shapes of hazards or survival functions. Simulation results show that the resulting model of mean restricted life gives more precise parameter estimates than a pseudo-value approach for fitting a similar model for the restricted mean, without making additional parametric assumptions.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Survival analysis,yingding@umich.edu,,Ying Ding,Ph.D. Candidate,"Department of Biostatistics, University of Michiga",1420 Washington Heights,734-355-7582,,yingding@umich.edu,Intercept Estimation for the Semiparametric Accelerated Failure Time Model,1,Ying,,Ding,"Department of Biostatistics, University of Michigan",Bin,,Nan,"Department of Biostatistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There is a rich literature on the slope parameter estimation for a linear regression model for censored survival data when the error distribution is unspecified. The estimation of intercept, however, has not been thoroughly studied mostly because that the follow up time is usually finite in practice so the intercept, directly related to the mean survival time, would be inevitably underestimated. Motivated by the results of consistent estimation of the mean survival time from Susarla and Van Ryzin (1980) and Stute and Wang (1993), in this paper we show that the intercept can be consistently estimated when the support of some covariates with nonzero coefficients is unbounded. Without the commonly assumed regularity condition on bounded covariates, we also show that the slope estimators obtained from the rank-based estimating equation with Gehan weights are consistent and asymptotically normal, which provides a crucial condition for the consistency of the intercept estimator. The theoretical finding is further verified for finite samples by a simulation study. Simulation also shows that, when both models are correct, the accelerated failure time model yields reasonable mean square errors for survival time prediction and outperforms the Cox model for censored data, particularly with heavy censoring. An illustrative real data example is provided.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Estimating equations,yangc@musc.edu,,Chengwu Yang,,Medical University of South Carolina,117 Heritage Circle,843-276-9728,843-216-0487,yangc@musc.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,pcgv8@missouri.edu,,Ping Chen,,University of Missouri,146 Middlebush Hall,651-503-8148,,pcgv8@missouri.edu,Statistical Analysis of Clustered Current Status Data,1,Ping,,Chen,"Department of Statistics, University of Missouri-Columbia",Junshan,,Shen,"Department of Mathematics, Beijing University, China",Jianguo,,Sun,"Department of Statistics, University of Missouri-Columbia",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper discusses regression analysis of clustered current status data which occur ifthe failure times of interest in a cluster survival study is either left- or right-censored,or each subject is observed only once. Some examples of the areas that often producesuch data are cross-sectional studies and tumorigenicity experiments (Keiding, 1990;Sun, 2006). A few methods have been developed if the failure time of interest is onlyright-censored but there does not seem to exist a method for current status data. Forthe problem, we present a Cox frailty model and a two-step EM algorithm is developedfor parameter estimation. A simulation study is conducted for the evaluation of theproposed methodology and indicates that the approach performs well for practicalsituations. An illustrative example from a tumorigenicity experiment is provided.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Random effects,sam976@pitt.edu,,Sachiko Miyahara,,University of Pittsburgh,326 Parran Hall,(412)624-0277,,sam976@pitt.edu,Weighted Kaplan-Meier Estimator for Two-stage Treatment Regimes,1,Sachiko,,Miyahara,"Department of Biostatistics,University of Pittsburgh",Abdus,S,Wahed,"Department of Biostatistics,University of Pittsburgh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In two stage randomization designs, patients are randomized to one of the available initial treatments, and at the end of the first stage, they are randomized to one of the second stage treatments depending on the outcome of the initial treatment.  Statistical inference for survival data from these designs uses methods such as marginal mean models and weighted risk set estimates.  In this article, we propose a weighted Kaplan-Meier (WKM) estimator based on the method of inverse-probability weighting and compare its properties to that of the standard Kaplan-Meier (SKM) estimator, marginal mean model based (MM) estimator and weighted risk set (WRS) estimator.  Simulation study reveals that the WKM estimator is asymptotically unbiased, and provides coverage rates similar to the MM and WRS estimators.  The SKM estimator, however, is biased when the second randomization rates are not equal between the responders and non-responders to initial treatment.  The methods described are demonstrated by applying to a leukemia dataset.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Survival analysis,weixx035@umn.edu,,Peng Wei,,University of Minnesota,420 Delaware St SE,612-803-6945,,weixx035@umn.edu,Network-based genomic discovery: application and comparison of Markov random field models,1,Peng,,Wei,"Division of Biostatistics, School of Public Health, University of Minnesota",Wei,,Pan,"Division of Biostatistics, School of Public Health, University of Minnesota",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As biological knowledge accumulates rapidly, gene networks encoding genome-wide gene-gene interactions have been constructed. As extensions of the standard mixture model that tests all the genes iid a priori, Wei and Li (2007) and Wei and Pan (2008) proposed two methods to incorporate gene networks into statistical analysis of genomic data via Discrete- and Gaussian-Markov random field (DMRF and GMRF) based mixture models, respectively. However, it may not be clear how the two methods compare with each other in practical applications. This paper is aimed at this comparison. We also propose two novel constraints on prior specifications for the GMRF model and a fully Bayesian approach to the DMRF model. In addition, we assessed the accuracy of direct posterior probability approach to estimating the False Discovery Rate (FDR) in the context of MRF models. Applications to a ChIP-chip data set and simulated data showed that the modified GMRF models had superior performance as compared with other models, while both MRF-based mixture models, with reasonable robustness to misspecified gene networks, outperformed the standard mixture model that assumes independent genes.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Microarray analysis,NONEliyimei@email.unc.edu,,Yimei Li,,"Biostat, UNC-Chapel Hill",2006 Winterborne Dr,9198246161,,liyimei@email.unc.edu,Multiscale Adaptive Regression Models for Imaging Data,1,Yimei,,Li,"Department of Biostatistics, UNC-Chapel Hill",Hongtu,,Zhu,"Department of Biostatistics,UNC-Chapel HillThe Biomedical Research Imaging Center, UNC-Chapel Hill",Joseph,G,Ibrahim,"Department of Biostatistics, UNC-Chapel Hill",Dinggang,,Shen,"Department of Radiology, UNC-Chapel HillThe Biomedical Research Imaging Center, UNC-Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,"We develop a multiscale adaptive regression model (MARM) for spatialand adaptive analysis of imaging data. The primary motivation andapplication of the proposed methodology is statistical analysis ofimaging data on the two-dimensional (2D) surface or in the 3D volumefor neuroimaging studies. The key idea of the MARM is to successivelyincrease the radius of a spherical neighborhood around each voxel andcombine all the data in a given radius of each voxel with appropriateweights to adaptively calculate parameter estimates and teststatistics. We establish consistency and asymptotic normality of theadaptive estimates and the asymptotic distributions of the adaptivetest statistics. Particularly, we show theoretically that the MARMoutperforms classical voxel-wise approach. Simulation studies are usedto demonstrate the methodology and examine the finite sampleperformance of the MARM. We apply ourmethods to the detection of spatial patterns of brain atrophy in aneuroimaging study of Alzheimers disease. Our simulation studies andreal data analysis demonstrate that the MARM significantly outperformsthe voxel-wise methods.KEY WORDS: Asymptotic properties; Multiscale adaptive regression;Voxel-wise method; Wald statistic; Weights.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Imaging,Generalized linear models,xiy17@pitt.edu,,Xing Yuan,,University of Pittsburgh,4733 Centre Ave Apt 2D,412-596-9117,,xiy17@pitt.edu,A Meta-analytic Framework for Combining Incomparable Cox Proportional Hazard Models Caused by Omitting Important Covariates,1,Xing,,Yuan,University of Pittsburgh,Stewart,,Anderson,University of Pittsburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In Cox proportional hazard models with censored survival data,estimates of treatment effects with some important covariates omittedwill be biased toward zero (Gail et al., 1984). This is especiallyproblematic in meta-analyses to combine estimates of parameters fromstudies where different covariate adjustments are made. Presently, fewconstructive solutions have been provided to address this issue. Wepropose a meta-analytic framework for combining incomparable Coxmodels under both aggregated patient data (APD) and individual patientdata (IPD) structures. For APD, two meta-regression models withindicators of different covariates in Cox models are proposed toadjust the heterogeneity of treatment effects across studies. Bothparametric and nonparametric estimators for the pooled treatmenteffect and the heterogeneity variance are presented and compared. ForIPD, we propose a fully augmented weighted estimator based on frailtymodels accommodating covariate(s) omission from different studies, andresults are compared with estimations from multiple imputationsmethod. Furthermore, while most meta-analyses focus on combiningunivariate effect sizes, we generalize our methods to estimate theentire vector of effect sizes simultaneously. We illustrate theadvantages of our proposed analytic procedures over existingmethodologies by simulation studies and real data analyses usingmultiple breast cancer clinical trials.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Clinical trials,bruce.swihart@gmail.com,,Bruce Swihart,,Johns Hopkins School of Public Health,Biostatistics,720-244-1126,410-955-0958,bruce.swihart@gmail.com,Modeling Sleep with Bayesian Poisson Regression,1,Bruce,,Swihart,Johns Hopkins School of Public Health,Brian,,Caffo,Johns Hopkins School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Bayesian Poisson Regression describes the sleep hypnogram more fullythan that of traditional sleep architecture.  Showing the derivationof the Poisson representation provides motivation for a shift in theconceptualization of modeling sleep.  The problem can be thought of asa multi-state, recurrent event, competing risk, hierarchical,stratified survival model or a Poisson process with the sufficentstatistics of number of transitions arising from time at risk forthose transitions.  This shift makes concerns about tie handling oftimes-to-event(s) inconsequential.  The ability to piecewise model thehazard, segment the night, and account for transition-type allows for avery flexible model that can easily incorporate time varyingcovariates.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Bayesian methods,tibs@stanford.edu,,rob tibshirani,professor,stanford univ,dept of health research & policy,6507237264,,tibs@stanford.edu,TRANSPOSABLE REGULARIZED COVARIANCE MODELS WITH AN APPLICATION TO HIGH- DIMENSIONAL MISSING DATA IMPUTATION,1,Genevera,,Allen,stanford univ,Rob,,Tibshirani,stanford univ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Missing data is a common concern in high-dimensional problems such asmicroarrays and user-ratings data.  Recent authors have suggested thatthese examples of matrix data do not necessarily have independent rowsor columns and it is not clear whether the row or columns arefeatures.  Hence, the data is transposable. To model this, wepresent a modification of the matrix-variate normal, themean-restricted matrix-variate normal, in which the rows and columnseach have a separate mean vector and covariance matrix.  We extendregularized covariance models, which place an additive penalty on theinverse covariance matrix, to this distribution by placing separatepenalties on the covariance matrices of the rows and the columns. These so called transposable regularized covariance models allow formaximum likelihood estimation of the mean and non-singular covariancematrices.  Using these models, we formulate EM-type algorithms formissing data imputation in both the multivariate and transposableframeworks.  An efficient approximation is also given for transposableimputation. Simulations and results on microarray data and the Netflixdata show that these imputation techniques often outperform existingmethods and offer a greater degree of flexibility.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Missing data,yswu@bios.unc.edu,,Yuanshan Wu,Mr.,Wuhan University and University of North Carolina,"Department of Biostatistics, CB#7420, University of North Carolina at Chapel Hill",919-966-7788,,yswu@bios.unc.edu,Additive-Multiplicative Rates Model for Recurrent Events,1,Yuanshan,,Wu,Wuhan University and University of North Carolina at Chapel Hill,Yanyan,,Liu,Wuhan University,Jianwen,,Cai,University of North Carolina at Chapel Hill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recurrent events are frequently encountered inbiomedical studies. Evaluating the  covariates effects on themarginal recurrent event rate is  of practical interest. There aremainly two types of rate models for the recurrent event data: themultiplicative rates model and the  additive rates model. Weconsider a more flexible additive-multiplicative rates model foranalysis of recurrent event data, wherein some covariate effects areadditive while others are multiplicative. We formulate estimatingequations for estimating the regression parameters. The estimatorsfor these regression  parameters are shown to be consistent andasymptotically normally distributed under appropriate regularityconditions. Moreover, the  estimator of the baseline mean functionis proposed and its  large sample properties are investigated. Inaddition, a simple graphical and numerical procedure based oncommonly used cumulative residual process is presented to assess thegoodness-of-fit of the model. We also investigate the finite samplebehavior of the proposed estimators through simulation studies. Awell-known medical study of bladder cancer is analyzed  forillustration of the proposed method.",FALSE,FALSE,T5: Analysis of Censored Cost or Health Outcomes Data,FALSE,TRUE,TRUE,Recurrent event data analysis,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Other,Survival analysis,Recurrent events analysisbhattacharjees@mail.nih.gov,,Samsiddhi Bhattacharjee,,National Cancer Institute,"6120 Executive Bouevard, EPS 8044",412-513-5166,,bhattacharjees@mail.nih.gov,Score Statistics for Family-based Genetic Association Studies of Quantitative Traits,1,Samsiddhi,,Bhattacharjee,National Cancer Institute,Eleanor,,Feingold,University of Pittsburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Family-based tests of genetic association protect from spurious associations by ignoring and/or conditioning on certain data for the pedigree founders. Hence, these tests are often considerably less powerful than population-based tests that use all of the available information but are sensitive to stratification. We present a unified likelihood for quantitative traits in families and derive several score statistics for testing association. Our statistics make varying assumptions on the nature of substructure that may be present in the dataset, and provide a range of options between purely population-based and traditional family-based tests. Under certain assumptions about the stratification, we are able to incorporate founder phenotypes and derive significant additional power from the founder genotype-phenotype correlation and the environmental correlation between founders and non-founders. We extend these score tests to handle known linkage, and also derive formulas for conditional moments required to compute them in general pedigrees. Finally, we use simulations to compare the performance of these statistics to the standard approaches under varying extents of stratification.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Epidemiologic methods,tianyue.zhou@sanofi-aventis.com,,Tianyue Zhou,,Sanofi-aventis,42 Dewitt Lane,908-304-6518,,tianyue.zhou@sanofi-aventis.com,Weighted likelihood method for a linear mixed model,1,Tianyue,,Zhou,Sanofi-aventis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Maximum likelihood is a widely used estimation method in statistics. This method is model dependent and as such is criticized as being non-robust. In this paper we consider using weighted likelihood method to make robust inferences for linear mixed models where weights are determined at both the subject level and the observation level. This approach is appropriate for problems where maximum likelihood is the basic fitting technique, but a subset of data points is discrepant with the model. It allows us to reduce the impact of outliers without complicating the basic linear mixed model with normal distributed random effects and errors. The weighted likelihood estimators are shown to be robust and asymptotically normal. Our simulation study demonstrates that the weighted estimates are much better than the unweighted ones when a subset of data points is far away from the rest. Its application to the analysis of deglutition apnea duration in normal swallows shows that the differences between the weighted and unweighted estimates are due to large amount of outliers in the data set.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Computational methods,Random effects,jichun@mail.med.upenn.edu,,Jichun Xie,MS.,"Department of Biostatistics & Epidemiology, UPENN",Rm 501 Blockley Hall,215-573-8950,,jichun@mail.med.upenn.edu,Quasi-Least Squares with Mixed Correlation Structure,1,Jichun,,Xie,"Department of Biostatistics & EpidemiologySchool of Medicine University of Pennsylvania",Justine,,Shults,"Department of Biostatistics & EpidemiologySchool of Medicine University of Pennsylvania",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this paper, we focus on a two-stage nonparametric approach,quasi-least squares (QLS), which yields a consistent estimator forthe correlation parameter in longitudinal data analysis. Our work ismotivated by a desire to appropriately model the correlation andregression parameters in an analysis of families in a longitudinalOphthalmology study in Old Order Amish (OOA). To argue the validityof QLS for the OOA analysis, we first prove some properties of QLSfor linear correlation structures, that previously have only beenproven on a case by case basis for particular structures: namely,that the stage one QLS estimator always exists and is feasible andthat the stage two estimator is unique. Based on our general proofs,we then discuss application of QLS for analysis of longitudinal datathat are missing completely at random (MCAR) and that are missing atrandom (MAR). We then implement QLS for familial longitudinal datawith families of varying sizes, which is a special case of data withmixed correlation structures.  We conduct simulations to assess theperformance of QLS in estimation of the correlation parameter. Wethen conduct an analysis of the Ophthalmology study in the Old OrderAmish, which reveals insights regarding the intra-familialcorrelations that are of scientific interest.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Estimating equations,sxh350@psu.edu,,Sasiprapa Hiriote,,"Department of Statistics, The Pennsylvania State U",331A Thomas building,814-404-9543,,sxh350@psu.edu,Multivariate Concordance Correlation Coefficient,1,Sasiprapa,,Hiriote,"Department of Statistics,Eberly College of Science,The Pennsylvania State University,University Park,PA 16802",Vernon,M.,Chinchilli,"Department of Public Health Sciences,Penn State College of Medicine,Hershey, PA 17033-0855",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In many clinical studies, the concordance correlation coefficientintroduced by Lin (1989) is a common tool to assess the agreement of acontinuous response measured by two raters or methods. However, theneed of measures of agreement may arise for more complex situations,such as when the responses are measured on more than one occasion byeach rater or method.  In this work, we propose a new version of theconcordance correlation coefficient, called the multivariateconcordance correlation coefficient, which possesses the propertiesneeded to characterize the level of agreement between two p_1 vectorsof random variables. It reduces to Lin's concordance correlationcoefficient when p = 1. The proposed estimators are proven to beasymptotically normal and their performances are evaluated viasimulation studies. Real data from asthma clinical trials are used fordemonstration.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Agreement,Multivariate methods,bz38d@mizzou.edu,,Bin Zhang,,University of Missouri,510 High Street Apt. 316,573-884-8820,,bz38d@mizzou.edu,Efficient Estimation for the Proportional Odds Model with Bivariate Current Status Data,1,Bin,,Zhang,University of Missouri,Xingwei,,Tong,Beijing Normal University,Jianguo,,Sun,University of Missouri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Efficient estimation approach provides a useful tool for semiparametric analysis of failure time data if the main interest is estimation of regression parameters (Huang, 1996; Martinussen and Scheike, 2002).  This paper discusses the application of this approach to regression analysis of bivariate current status data arising from the proportional odds model (Yang and Prentice, 1999; Rabinowitz et al., 2000). Current status data arise if each study subject is observed only once and often occur in fields including demographical studies and tumorigenicity experiments.  For the analysis, the copula model is assumed for the joint survival function of two related failure time variables which are supposed to follow the proportional odds model marginally. Simulation studies indicate that the estimates of regression parameters derived perform well in practical situations and the methodology is illustratedusing a set of data from a tumorigenicity experiment.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Cancer applications,hycao@email.unc.edu,,Hongyuan Cao,,UNC-Chapel Hill,180 B.P.W. Club Road,919-448-5597,,hycao@email.unc.edu,Finding Critical Value for t-Tests in Very High Dimensions,1,Hongyuan,,Cao,"Department of Statistics and Operations Research, University of North Carolina - Chapel Hill",Michael,R.,Kosorok,"Department of Biostatistics and Department of Statistics and Operations Research, University of North Carolina - Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In micro array studies, image analysis, high throughput molecularscreening, astronomy, and in many other high dimensional statisticalproblems, hypothesis testing is applied in a simultaneous way. Popularalternatives to familywise error rate for calibration of multiplicityinclude k-familywise error rate (k-FWER), false discovery rate (FDR)and false discovery proportion(FDP). Most procedures in the literature are based on the assumptionthat the p-values of tests are known or that the distributions undernull and alternative hypothesis are known. However, these assumptionsare usually not realistic. In this paper, we focus on one-sample andtwo-sample t-statistics and develop a procedure to find cut-off valuesto control k-FWER, FDR and tail probability of FDP (FDTP). Ourapproach doesn't require any distributional assumptions on thepopulation and is robust as long as the population has finite thirdmoment plus some very general conditions on the mean and variance. Wealso develop a new estimator for the proportion of alternativehypotheses. Simulation results and a real data example show that ourapproach is generally better than the Benjamini and Hochberg (1995)'sprocedure.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,TRUE,Multiple testing,High dimensional data,jane@stat.columbia.edu,,Jane Paik,,Columbia University,"Room 1005 SSW, MC 4690",212.851.2136,,jane@stat.columbia.edu,Semiparametric inference of linear transformation models with length-biased censored data,1,Jane,,Paik,Columbia University,Zhiliang,,Ying,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this article we propose an estimation method for the regressionparameters and the transformation function in semiparametric lineartransformationmodels when dealing with biased sampling in censored data.  This paperis the first to propose a method for length-biased sampling in lineartransformation models. Existing estimation procedures for censored data using lineartransformation models yield biased estimators for regressionparameters of interest. Our approach is motivated by the unifiedestimation procedure proposed by Chen et al. (2002) which made useof the martingale integral representation. The proposed estimatorsfor the regression parameters are proven to be consistent andasymptotically normal. The variance-covariance matrix has a closedform which can be consistently estimated.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Estimating equations,qpan@gwu.edu,,Qing Pan,Assistant Professor,"Stat Dept, GWU",2140 Pennsylvania Ave. N.W.,202-994-6359,202-994-6917,qpan@gwu.edu,Likelihood Ratio Test for Qualitative Interactions,1,Qing,,Pan,"Stat Dept, GWU",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Often, the factor of interest has qualitatively different effectsacross subject subgroups. Two interesting cases motivatethe need for a formal statistical test targeting at detectingcross-over patterns in treatment effects. In the Diabetes PreventionProgram, the Metformin treatment was shown to reduce the risk of typeII diabetes. What is of concern is whether Metforminworks negatively (increase the risk) for certain types of patients. In the second motivating example, the state of Illinois was sued by the Black members over the fairness in the promotion process. Due to the limited sample size, the odds ratio of promotion between the protected and the majority groups would be calculated across all ranks as long as no qualitative (instead of quantitative) differences exist among the race effects in different ranks. Gail and Simon (1985) proposed a likelihood ratio test (LRT) for qualitative interactions by comparing the unrestricted MLE to the parameter values under the null with the smallest test statistics value. This paper proposes another LRT where the projection of the unrestricted MLE to the null region is employed. The asymptotic mixed Chi-square distribution is derived. Size and power are verified under logistic and proportional hazards models. Finally, this new test is applied to the two motivating cases.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Clinical trials,daeyoung@math.umass.edu,,DAEYOUNG KIM,Assistant Professor,Department of Mathematics and Statistics,Lederle Graduate Research Tower,814 4048593,814 4048593,daeyoung@math.umass.edu,Simulation based Visualization of Inference functions,1,DAEYOUNG,,KIM,"Department of Mathematics and Statistics, University of Massachusetts Amherst",Bruce,G.,Lindsay,"Department of Statistics, The Pennsylvania State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents a new simulation based methodology designed to facilitate visual analysis of the confidence sets generated by an inference function such as the likelihood. This methodology generates a sample from the parameter space using a fiducial-like distribution. This distribution is designed so that its probabilities on the parameter space are equal to the coverage probabilities of the targeted confidence sets. Plotting these samples provides picture of the inference function surface around the point estimator optimizing the inference function. Once the sample is created, one can picture the profile inference function confidence sets for multiple parameters of interest without further complicated optimization. We illustrate the methodology with four different inference functions.Although this methodology is related to Fisher's concept of fiducial inference, the fiducial-like distribution we create here is chosen for its ability to recover the confidence sets generated by the inference function and for its ease in computation. Unlike resampling methods such as parametric bootstrap,  our method uses the original data set, just as is done in Bayesian inference. We use illustrative examples to compare simulation-based confidence sets with those based on numerical optimization, and to compare the confidence regions generated by different inference functions.",FALSE,FALSE,,FALSE,FALSE,TRUE,"1. March 16(Monday) : Roundtables from 12:15 pm-1:30 pm2. March 18(Wednesday)",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Computational methods,Visualizationjcao@smu.edu,,Jing Cao,,Southern Methodist University,144 Heroy Science Hall,214-768-2451,,jcao@smu.edu,A Bayesian Chi-Squared Goodness-of-Fit Test for Censored Data Models,1,Jing,,Cao,Southern Methodist University,Ann,,Moosman,Patrick Air Force Base,Valen,,Johnson,U.T. M.D. Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a Bayesian chi-squared model diagnostic for analysis of data subject to censoring. The test statistic has the form of Pearson's chi-squared test statistic and is easy to calculate from standard output of Markov chain Monte Carlo algorithms.  The key innovation of this diagnostic is that is based only on observed failure times.  Because it does not rely on the imputation of failure times for observations that have been censored, we show that it can have higher power for detecting model departures than a comparable test based on the complete data.  In a simulation study, we show that tests based on this diagnostic exhibit comparable power and better nominal Type I error rates than a commonly used alternative test proposed by Akritas (1988).  An important advantage of the proposed diagnostic is that it applies to a broad class of censored data models, including generalized linear models and other models with non-identically distributed and non-additive error structures.  We illustrate the proposed model diagnostic for testing the adequacy of two parametric survival models for Space Shuttle main engine failures. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Variable subset selection/model selection,song.zhang@utsouthwestern.edu,,Song Zhang,Assistant Professor,University of Texas Southwestern Medical Center,5551 Monticello Ave,214-648-3235,,song.zhang@utsouthwestern.edu,Avoid ecological fallacy: using BART to impute missing ordinal data,1,Song,,Zhang,University of Texas Southwestern Medical Center,Tina,,Shih,University of Texas M.D. Anderson Cancer Center,Peter,,Muller,University of Texas M.D. Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Ecological fallacy is a situation that can occur when making inferenceabout an individual based on aggregate data from a group. In healthdisparity research, a missing individual-levelsocial-economical-status (SES) variable is usually replaced by acensus-based SES statistic, which is considered a proxy for theindividual SES. We use a real data example to demonstrate thepotential biases associated with the census-based approach, as aresult of ecological fallacy. We further propose a Bayesian additiveregression tree (BART) method to impute missing ordinal data(household income category), utilizing statistical learning from adifferent dataset with the income variable observed. The imputationbased on BART is shown to be a better proxy for the missing variablethan census-based SES, and it avoids ecological fallacy when makinginference about the important factors in the disparity of colorectalcancer screening utilization. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Bayesian methods,boos@stat.ncsu.edu,,Dennis Boos,Professor,North Carolina State University,Department of Statistics,919-515-1918,,boos@stat.ncsu.edu,Fast FSR Variable Selection with Interactions,1,Dennis,D,Boos,"Department of Statistics, North Carolina State University",Hugh,B,Crews,SAS Institute Inc.,Leonard,A,Stefanski,"Department of Statistics, North Carolina State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Fast FSR approach of Boos et al. (2009, Biometrics) is extended tohandle forward selection for second-order models under varioushierarchy restrictions between main effects and second-order terms(squares and interactions).  New easy-to-use SAS macros are presentedthat implement these FSR variable selection approaches for linearregression, logistic regression, and Cox regression, and are available athttp://www4.stat.ncsu.edu/~boos/var.select/hugh.crews.software.html.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Variable subset selection/model selection,High dimensional data,ofer.harel@uconn.edu,,Ofer Harel,,University of Connecticut,215 Glenbrook road Unit 4120,860-486-6989,,ofer.harel@uconn.edu,Outfluence -- The impact of missing values,1,Ofer,,Harel,University of Connecticut,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There are numerous measures that assess the effect of an observation, group of observations, a variable, or variables and observations on the regression estimation. Incomplete data is a common difficulty in data analysis. I introduce a new measure which assesses the effect of a missing observation, a group of missing observations, an incomplete variable or any combination of these, on the overall estimation. I call this measurement 'outfluence'. The outfluence measure can be used in a regression analysis context or any other parametric settings. I illustrate the major benefits of outfluence using biomedical examples. ",FALSE,FALSE,,FALSE,FALSE,TRUE,"I have to leave TX on Tuesday (March 17) around noon. Therefore, I have to present beforehand.",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Applied data analysis,glan@its.jnj.com,,Kuang-Kuo Gordon Lan,Senior Director,Johnson & Johnson PRD,920 Route 202,908-704-5001,,glan@its.jnj.com,The use of cumulative meta analysis in drug development,1,Kuang-Kuo,G,Lan,Johnson & Johnson PRD,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Meta analysis is an extremely useful method to summarize accrued information. However, due to heterogeneity of various study background, it is difficult to provide reliable inference from the pooled data.  The problem becomes more serious in cumulative meta analysis since (i) the between-study variation cannot be estimated accurately when there are only a few studies under consideration, and (ii) the characteristics of future studies are hard to predict.  In general, the parameters under consideration may not follow a simple parametric distribution. Motivated by the Law of Iterated Logarithm, we propose adding a multiplicative penalty factor to the cumulative test statistic. This will introduce a conservative approach to show efficacy for a new compound, or a new treatment procedure.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Biopharmaceutical research,Health policy applications,wang@stat.ncsu.edu,,huixia judy wang,assistant professor,north carolina state university,Department of Statistics,919-513-1661,,wang@stat.ncsu.edu,Locally Weighted Censored Quantile Regression,1,Huixia,Judy,Wang,"Department of Statistics, North Carolina State University",Lan,,Wang,"Department of Statistics, University of Minnesota",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Censored quantile regression offers a valuable supplement to Coxproportional hazards model for survival analysis. Existing work in theliterature often requires stringent assumptions, such as unconditionalindependence of the survival time and the censoring variable or globallinearity at all quantile levels. Moreover, some of the work usesrecursive algorithms which makes it challenging to derive asymptoticnormality. To overcome these drawbacks, we propose a novel locallyweighted censored quantile regression approach. The new approachadopts the redistribution-of-mass idea and employs a local reweightingscheme. Its validity only requires conditional independence of thesurvival time and the censoring variable given the covariates, andlinearity at the particular quantile level of interest. Our methodleads to a simple algorithm that can be conveniently implemented withR software. Applying recent theory of M-estimation with infinitedimensional parameters, we rigorously establish the consistency andasymptotic normality of the proposed estimator. The proposal method isstudied via simulations and the analysis of an acute myocardialinfarction dataset.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Nonparametric methods,achatter@westga.edu,,Ayona Chatterjee,Dr.,University of West Georgia,3201 Post Woods Dr Apt A,678-839-4142,,achatter@westga.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,rui_chen@urmc.rochester.edu,,Rui Chen,,University of Rochester Medical Center,"Department of Biostatistics and Computational Biology, University of Rochester Medical Center",585-275-6692,,rui_chen@urmc.rochester.edu,A composite likelihood approach to the analysis of longitudinal clonal data on multitype cellular systems under an age-dependent branching process,1,Rui,,Chen,"Department of Biostatistics and Computational BiologyUniversity of Rochester Medical Center ",Ollivier,,Hyrien,"Department of Biostatistics and Computational BiologyUniversity of Rochester Medical Center ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The theory of age-dependent branching processes provides an appealingstatistical framework for drawing inference on cell kinetics fromclones observed longitudinally at discrete time points. Likelihoodinference being difficult in this context, we propose an alternativecomposite likelihood approach, where the estimation function isdefined from the marginal or conditional distributions for the numberof cells of each observable cell type. These distributions havegenerally no closed-form expressions but can be approximated usingsimulations. We construct a bias-corrected version of the estimatingfunction, which also offers computational advantages. The compositelikelihood estimator is proven to be consistent and asymptoticallynormal, and its finite-sample properties are investigated insimulation studies. It is shown that the proposed approach outperformsthe existing methods. Finally an application to the analysis of theeffect of neurothrophin-3 on the generation of oligodendrocytes fromoligodendrocyte type-2 astrocyte progenitor cells cultured in vitro ispresented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Longitudinal data,Stochastic process modelinglpeng@sph.emory.edu,,Limin Peng,,"Department of Biostatistics and Bioinformatics, Em","1518 Clifton Rd NE, 3rd floor",404-727-7701,,lpeng@sph.emory.edu,Competing Risks Quantile Regression,1,Limin,,Peng,"Department of Biostatistics and Bioinformatics,Rollins School of Public Health, Emory University",Jason,P,Fine,"Department of Biostatistics, University of North Carolina at Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Quantile regression has emerged as asignificant extension of traditional linear models and its potentialin survival applications has recently been recognized. In this paperwe study quantile regression with competing risks data, formulatingthe model based on conditional quantiles defined using thecumulative incidence function, which includes as a special case ananalog to the usual accelerated failure time model. The proposedcompeting risks quantile regression model provides meaningfulphysical interpretations of covariate effects and moreover relaxesthe constancy constraint on regression coefficients, therebyproviding a useful, perhaps more flexible, alternative to thepopular subdistribution proportional hazards model. We derive anunbiased monotone estimating equation for regression parameters inthe quantile model. The uniform consistency and weak convergence ofthe resulting estimators are established across a quantilecontinnum. We develop inferences, including covariance estimation,second-stage exploration, and model diagnostics, which can be stablyimplemented using standard statistical software without involvingsmoothing or resampling. Our proposals are illustrated viasimulation studies and an application to a breast cancer clinicaltrial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Estimating equations,xhe@cph.osu.edu,,Xin He,Assistant Professor,The Ohio State University,B-116 Starling-Loving Hall,(614) 293-3925,,xhe@cph.osu.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,iryna.lobach@nyumc.org,,Iryna Lobach,Assistant Professor of Biostatistics,"New York University, School of Medicine","650 First Ave, 540",2122636256,,iryna.lobach@nyumc.org,Haplotype-Based Regression Analysis and Inference of Case-Control Studies with Unphased Genotypes and Measurement Errors in Environmental Exposures,1,Iryna,V,Lobach,"Divistion of Biostatistics, New York University, School of Medicine, New York, NY 10016",Raymond,J,Carroll,"Department of Statistics, Texas A&M University, College Station, Texas 77843-3143, U.S.A. ",Christine,,Spinka,"Department of Statistics, University of Missouri, Columbia, Missouri 65211-6100, U.S.A",Mitchell,,Gail,"Biostatistics Branch, Division of Cancer Epidemiology and Genetics, National Cancer Institute, 6120 Executive Boulevard, EPS 8038 Rockville, Maryland 20852, U.S.A. ",Nilanjan,,Chatterjee,"Biostatistics Branch, Division of Cancer Epidemiology and Genetics, National Cancer Institute, 6120 Executive Boulevard, EPS 8038 Rockville, Maryland 20852, U.S.A. ",,,,,,,,,,,,,,,,,,,,,"It is widely believed that risks of many complex diseases are determined by genetic susceptibilities, environmental exposures, and their interaction. Chatterjee and Carroll (2005, Biometrika92, 399418) developed an efficient retrospective maximum-likelihood method for analysis of casecontrol studies that exploits an assumption of geneenvironment independence and leaves the distribution of the environmental covariates to be completely nonparametric. Spinka, Carroll, and Chatterjee (2005, Genetic Epidemiology29, 108127) extended this approach to studies where certain types of genetic information, such as haplotype phases, may be missing on some subjects. We further extend this approach to situations when some of the environmental exposures are measured with error. Using a polychotomous logistic regression model, we allow disease status to have K+ 1 levels. We propose use of a pseudolikelihood and a related EM algorithm for parameter estimation. We prove consistency and derive the resulting asymptotic covariance matrix of parameter estimates when the variance of the measurement error is known and when it is estimated using replications. Inferences with measurement error corrections are complicated by the fact that the Wald test often behaves poorly in the presence of large amounts of measurement error. The likelihood-ratio (LR) techniques are known to be a good alternative. However, the LR tests are not technically correct in this setting because the likelihood function is based on an incorrect model, i.e., a prospective model in a retrospective sampling scheme. We corrected standard asymptotic results to account for the fact that the LR test is based on a likelihood-type function. The performance of the proposed method is illustrated using simulation studies emphasizing the case when genetic information is in the form of haplotypes and missing data arises from haplotype-phase ambiguity. An application of our method is illustrated using a population-based casecontrol study of the association between calcium intake and the risk of colorectal adenoma.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Measurement error,sudiptob@biostat.umn.edu,,Sudipto Banerjee,Associate Professor,University of Minnesota,420 Delaware Street SE,612 624-0624,612 626-0660,sudiptob@biostat.umn.edu,Hierarchical spatial modeling of genetic variance for large spatial trial datasets,1,Sudipto,,Banerjee,"Division of Biostatistics, University of Minnesota, Minneapolis.",Andrew,O,Finley,Departments of Forestry and Geography. Michigan State University,Patrik,,Waldmann,"Department of Forest Genetics and Plant Physiology, Swedish University of Agricultural Sciences, Ume, Sweden.",Tore,,Ericsson,"Department of Forest Genetics and Plant Physiology, Swedish University of Agricultural Sciences, Ume, Sweden.",,,,,,,,,,,,,,,,,,,,,,,,,"This talk expands upon recent interest in Bayesian hierarchical modelsin quantitative genetics by developing spatial process models forinference on additive and dominance genetic variance within thecontext of large spatially referenced trial datasets. Directapplication of such models to large spatial datasets are, however,computationally infeasible because of cubic order matrix algorithmsinvolved in estimation. Recently much attention has been devoted tothis problem. In this talk we primarily focus upon the use of apredictive process derived from the original spatial process thatprojects process realizations to a lower-dimensional subspace therebyreducing the computational burden. This approach can be looked upon asa process-based approach to reduced-rank methods for 'kriging' butoffers additional complexities. We discuss attractive theoreticalproperties of this predictive process as well as its greater modelingflexibility compared to existing methods. We also discuss somepitfalls of this and other reduced-rank methods and offer remedies. Acomputationally feasible template that encompasses these diversesettings will be presented and illustrated.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Short Course: Hierarchical Modeling and Analysis of Spatial-Temporal Data: Emphasis in Forestry, Ecology, and Environmental Sciences.Instructors: Andrew O. Finley and Sudipto Banerjee.",invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Hierarchical models,lphilip@hsph.harvard.edu,,Loni Philip,Student,Harvard School of Public Health,655 Huntington Avenue,2679785553,,lphilip@hsph.harvard.edu,A marginalized zero altered model for spatially correlated counts with excessive zeros,1,Loni,P,Philip,Harvard School of Public Health,Brent,,Coull,Harvard School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Excessive zero counts occur if the number of observed zeros exceeds that expected under the assumption of a traditional count distribution,such as a Poisson or Negative Binomial model. In health disparitiesresearch focusing on the spatial patterning of disease counts and itsrelationship with socioeconomic factors, spatial correlation among thecounts is also typically present.  Therefore, proper statisticalanalyses must account for the complex distribution of the diseasecounts as well as the correlation that exists among observations. Wepropose a marginalized zero-altered Poisson (ZAP) model that has theadvantage of specifying an interpretable marginal relationship between the outcome and the covariates of interest in thepresence of zero inflation or deflation. We describe a Bayesianapproach to model fitting, and compare the approach to existingstatistical approaches to analyzing spatially correlated zero-alteredcount data.  We apply the model to motivating data on the associationbetween premature mortality rates and socioeconomic status in Boston,MA during 1999 to 2001.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Bayesian methods,victor.deoliveira@utsa.edu,,Victor De Oliveira,,University of Texas at San Antonio,Department of Management Science and Statistics,(210) 4586592,,victor.deoliveira@utsa.edu,On Shortest Prediction Intervals in Log-Gaussian Random Fields,1,Victor,,De Oliveira,"Department of Management Science and StatisticsUniversity of Texas at San Antonio",Changxiang,,Rui,"Department of Mathematical SciencesUniversity of Arkansas",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This work considers the problem of constructing prediction intervalsin log-Gaussian random fields. New prediction intervals are derivedthat are shorter than the standard prediction intervals of common use,where the reductions in length can be substantial in some situations.We consider both the case when the covariance parameters are known andunknown. For the latter case we propose a bootstrap calibration methodto obtainprediction intervals with better coverage properties than the plug-in(estimative) prediction intervals. The methodology is illustratedusing a spatial dataset consisting ofcadmium concentrations form a contaminated region in Switzerland.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Environmental and ecological applications,shuangge.ma@yale.edu,,Shuangge Ma,,Yale University,"60 College ST, LEPH 209",203-785-3119,,shuangge.ma@yale.edu,An integrative analysis approach for identification of genes associated with multiple cancers,1,Shuangge,,Ma,Yale University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genomic markers identified from expression studies can be used toimprove diagnosis, prognosis and prediction in cancer clinicalstudies. Biomedical studies have suggested that development ofmultiple cancers may share common genomic basis. A completedescription of the associations between genes and cancers amounts toidentification of not only multiple genes associated with a singletype of cancer, but also multiple cancers that a specific gene isassociated with. For such a purpose, we propose an integrativeanalysis approach capable of analyzing multiple cancer microarraystudies conducted on different cancers. The proposed approach is thefirst regularized approach to conduct ``two-dimensional' selection ofgenes in the joint modeling of multiple gene effects. Using theproposed approach, we analyze seven microarray studies investigatingdevelopment of seven different types of cancers. Genes associated withone or more of the seven cancers are identified. Many identified geneshave sound biological basis. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Variable subset selection/model selection,Cancer applications,eric.kalendra@gmail.com,,Eric Kalendra,,North Carolina State University,2924 Tutman Court,919-345-9668,,eric.kalendra@gmail.com,"Spatial Association Between Racial and Social Factors in Determining Low Birth Weight and Preterm Birth, New York, 1995-2005",1,Eric,,Kalendra,"North Carolina State University, Department of Statistics",Montserrat,,Fuentes,"North Carolina State University, Department of Statistics",Brian,,Reich,"North Carolina State University, Department of Statistics",Amy,,Herring,"The University of North Carolina, Department of Biostatistics",Matthew,,Wheeler,"The University of North Carolina, Department of Biostatistics",,,,,,,,,,,,,,,,,,,,,"Regardless of whether low birth weight (2500 grammes or less at birth) is caused by intra-uterine growth retardation or pre-term birth (before 37 weeks of gestation), the condition remains one of the strongest predictors of neonatal and infant mortality. Using data from singleton live births in New York City from 1995-2003, in which over 92% of vital records were matched to maternal and infant hospital discharge data, this study examines the the potential impact that racial and social factors have in determining low birth weight and preterm birth. The analysis controls for a number of potentially confounding factors, including mother's age and census-based neighborhood socioeconomic and demographic characteristics of the environment in which these women live. The analysis is carried out using a spatially varying coefficients model for race. Spatial smoothing is done with a conditionally autoregressive (CAR) model. By using different weighting functions in the CAR model we are able to control the amount of smoothing. In our model we are able to explain local differences that are averaged over when we use a non-spatially varying coefficients model. Our study shows that the census level information is not enough to explain the within racial variation, that the spatially varying coefficients model is able to characterize. ",FALSE,FALSE,,FALSE,FALSE,TRUE,"Due to a wedding in the family, I need to request a presentation time that is not on Sunday or Monday (March 15/16).",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Epidemiologic methods,Applied data analysis,mquinlan22@yahoo.com,,Michelle Quinlan,Graduate Student,University of Nebraska-Lincoln,340 Hardin Hall North,402-472-2903,,mquinlan22@yahoo.com,Proposed Methodology for Shelf Life Estimation,1,Michelle,,Quinlan,University of Nebraska-Lincoln,James,,Schwenke,"Boehringer Ingelheim Pharmaceuticals, Inc.",Walt,,Stroup,University of Nebraska-Lincoln,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As part of the research efforts of the Product Quality Research Institute (PQRI) Stability Shelf Life Working Group, statistical methodology is being developed to directly estimate the shelf life of a pharmaceutical product. The proposed methodology presented here estimates shelf life based on the overall mean response among batches of a pharmaceutical product for a stability limiting characteristic. Incorporating random batch effects into the statistical model and using calibration techniques allow for direct estimation of shelf life as opposed to indirect methods of pooling data or relying on a worst batch scenario. The estimated shelf life is the storage time corresponding to the point where the predicted mean response intersects the specification limit or acceptance criteria. A lower interval estimate is constructed about the calibrated point to determine the labeled shelf life. Results of a computer simulation will be presented to validate the statistical methodology and to demonstrate the advantages over the current approach for estimating shelf life as defined by ICH guidelines. An example using real life data will be presented to highlight the proposed methodology for shelf life estimation.Key words: shelf life, random batch effects, calibration ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biopharmaceutical research,Random effects,weisun@email.unc.edu,,Wei Sun,,UNC Biostatistics,1100 NC highway 54 Bypass Apt 24,310-430-8650,,weisun@email.unc.edu,A geometric interpretation of the permutation p-value and its application in eQTL studies,1,Wei,,Sun,"Department of Biostatistics, UNC",Fred,,Wright,"Department of Biostatistics, UNC",Wei,,Sun,,Wei,,Sun,,Wei,,Sun,,Wei,,Sun,,Wei,,Sun,,Wei,,Sun,,Wei,,Sun,,Wei,,Sun,,"Permutation p-values have been widely used to assess the significance of linkage or association statistics in genetic studies. However their application in large-scale studies is hindered by a heavy computational burden. We propose a method to estimate permutation p-values based on a geometric interpretation of permutation p-values. Suppose a quantitative/qualitative trait and p genetic markers are studied in n individuals. The genotype profile for each of the p markers is a point in the n dimensional genotype space. We show how the permutation p-value is related to the volume of a sub-space occupied by the p markers and their neighborhood regions and propose a method to estimate permutation p-value based on this geometric view. An application to a study of gene expression quantitative trait loci (eQTL) shows that our method provides reliable estimates of permutation p-values while requiring less than 5% of the computational time compared with direct permutations. In fact, our method takes a constant time to estimate permutation p-values, no matter how small the p-value. Therefore it enables a study of the relationship between nominal p-values and permutation p-values, providing a geometric perspective on the effective number of independent tests performed.",FALSE,FALSE,,FALSE,FALSE,TRUE,"I need to teach on Monday and Wed, and I registered the Junior workshop on March 13-14. So I can only present on March 15. Otherwise, I have to cancel the abstract submission. ",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Multiple testing,wreichmann@partners.org,,William Reichmann,,Brigham and Women's Hospital,75 Francis Street,617-732-5081,,wreichmann@partners.org,Performance Characteristics for a Product Estimator and its 95% Confidence Interval in a Simulation Study,1,William,M,Reichmann,Brigham and Women's Hospital,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,jlgast@gwu.edu,,Joseph L Gastwirth,Professor,"Department of Statistics, GWU",Department of Statistics,2029946548,2029946917,jlgast@gwu.edu,Utiliziong Biostatistical Methods in the Analysis of Data in Discrimination Cases,1,Joseph,L,Gastwirth,"Department of Statistics, George Washington University",Qing,,Pan,"Department of Statistics, George Washington University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the Diaz v. Eagle Produce case, both parties did not analyze the statistical information available to them. In reversing a lower courts summary judgment decision for the employer the appellate opinion described patterns of some simple measures, e.g. the average age of new hires decreased over time and that older workers were at a higher risk of being discharged after a new supervisor took charge. Using the Cochran-Armitage trend test to examine the hiring data and the proportional hazards model to analyze the discharge data, one finds statistical support for the appellate courts finding. Although there were only 44 employees, using Fishers summary chi-square test to combine the p-values of both tests rejects the null hypothesis that both processes were fair with a p-value < .01. This area of application indicates that more research on combining the results of analyses of small datasets as well as further study of the power of these methods are needed. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Survival analysis,Legal Applicationsccrainic@jhsph.edu,,Ciprian,Assistant Professor,Johns Hopkins University,615 N Wolfe Street,410-955-3505,,ccrainic@jhsph.edu,Adjustment uncertainty in effect estimation,1,Ciprian,M,Crainiceanu,"Department of BiostatisticsJohns Hopkins University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Often there is substantial uncertainty in the selection of confounders when estimating the association between an exposure and health. We define this type of uncertainty as 'adjustment uncertainty'. We propose a general statistical framework for handling adjustment uncertainty in exposure effect estimation for a large number of confounders, we describe a specific implementation, and we develop associated visualization tools. Theoretical results and simulation studies show that the proposed method provides consistent estimators of the exposure effect and its variance. We also show that, when the goal is to estimate an exposure effect accounting for adjustment uncertainty,Bayesian model averaging with posterior model probabilities approximatedusing information criteria can fail to estimate the exposure effect and can over- or underestimate its variance. We compare our approach to Bayesian model averaging using time series data on levels of fine particulate matter and mortality.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Environmental and ecological applications,Spatial/temporal modeling,dthomas@usc.edu,,Duncan C. Thomas,Professor,University of Southern California,Dept of Preventive Medicine,323-442-1218,323-442-2349,dthomas@usc.edu,Multistage Sampling for Latent Variable Models in Environmental and Genetic Epidemiology,1,Duncan,C,Thomas,University of Southern California,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Multistage sampling schemes will be discussed for latent variablemodels where surrogate measurements of the latent variables are madeon a subset of subjects.  Such models arise when detailed exposuremeasurements are combined with exposure predictors to assign exposuresto unmeasured subjects, when biomarkers are used to assess anunobserved disease process, or in various other situations.  Analyticcalculations of the optimal design are possible when all variables arebinary, when all are normally distributed, or when the latent variableand its measurement are normally distributed but the outcome isbinary.  In these scenarios, it is often possible to improve thecost-efficiency of the design considerably by appropriate selection ofthe sampling fractions.  More complex situations arise when exposuresare spatially correlated.  Applications to the Children's Health Studyof the health effects of air pollution and candidate gene interactionswill be used to illustrate sampling designs for an analysis involvingmeasurements of local variation in air pollution levels on onesubsample of homes and of biomarkers of the oxidative stress andinflammatory pathways on an overlapping sample of individuals.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Epidemiologic methods,Latent variables,zhux0130@umn.edu,,Yanni Zhu,,"Division of Biostatistics, School of Public Health",A460 Mayo Building,612-991-5261,,zhux0130@umn.edu,Network-based support vector machine for classification of microarray samples,1,Yanni,,Zhu,"Division of Biostatistics, School of Public Health, University of Minnesota, Minneapolis, MN 55455",Xiaotong,,Shen,"School of Statistics, University of Minnesota, Minneapolis, MN 55455",Wei,,Pan,"Division of Biostatistics, School of Public Health, University of Minnesota, Minneapolis, MN 55455",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"        The importance of network-based approach to identifying biological markers for diagnostic classification and prognostic assessment in the context of microarrays has been increasinglyrecognized. To our knowledge, there have been few, if any, statistical tools that explicitly incorporate the prior information of gene networks into classifier building. The main idea of this paper is to take full advantage of the biological observation that neighboring genes in a network tend to function together in biological processes and to embed this information into a formal statistical framework.        We propose a network-based support vector machine for binary classification problems by constructing a penalty term from the F-infinity norm being applied to pairwise gene neighbors with the hope to improve predictive performance and gene selection. Simulation studies in both low- and high-dimensional data settings as well as two real microarray applications indicate that the proposed methodis able to identify more clinically relevant genes while maintaining a sparse model with either similar or higher prediction accuracy compared with the standard and the L1 penalized support vector machines.        The proposed network-based support vector machine has the potential to be a practically useful classification tool for microarrays.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Microarray analysis,Machine learning,nonesunj@missouri.edu,,(Tony) Jianguo Sun,Professor,University of Missouri,146 MIddlebush Hall,5738826667,,sunj@missouri.edu,Regression Analysis of Longitudinal Data with Dependent Observation Process,1,(Tony) Jianguo,,Sun,University of Missouri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Longitudinal data frequently occur in many studies such aslongitudinal follow-upstudies. To develop statistical methods and theory for the analysis ofthem,independent or noninformative observation and follow-up times aretypically assumed,which naturally leads to inference procedures conditional onobservation andfollow-up times. In many situations, however, this may not be true orrealistic.That is, observation times may depend on or be related with thelongitudinalresponses and the same could be true for the follow-up time. This talkdiscussesthe analysis of such longitudinal data and a joint modeling approachthat uses somelatent variables to characterize the correlations is presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Latent variables,wenge.guo@gmail.com,,Wenge Guo,,National Institute of Environmental Health Science,5011 S. Alston Ave. APT# G203,5202043425,,wenge.guo@gmail.com,"Controlling false discoveries in multidimensional directionaldecisions, with applications to gene expression data on ordered categories",1,Wenge,,Guo,"Biostatistics Branch, National Institute of Environmental Health Sciences",Sanat,K,Sarkar,"Department of Statistics, Temple University",Shyamal,D,Peddada,"Biostatistics Branch, National Institute of Environmental Health Sciences",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Time-course or dose-response microarrays gene expression studies arecommon in biomedical research. A goal of such studies is to identifygene expression patterns over time, dose, or tumor stage etc. In thisproject, we formulated this problem as a multiple testing problemwhere for each gene the null hypothesis of no difference between thesuccessive mean gene expressions are tested and further directionaldecisions are made if it is rejected. Much of the existing multipletesting procedures are devised for controlling the usual FDR ratherthan the mixed directional FDR (mdFDR), the expected proportion ofType I and directional errors among all rejections. In this project,we considered the problem of controlling the mdFDR involvingmultidimensional parameters. To deal with this problem, we developed aprocedure extending Benjamini and Yekutieli (2005)'s one-dimensionaldirectional BH procedure based on the Bonferroni test for each gene.We proved that the proposed procedure controls the mdFDR when theunderlying test statistics are independent across the genes. We alsoapplied the proposed methodology to a time-course microarray data andobtained several biologically interesting results.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Multiple testing,Microarray analysis,high dimensional databocai@gwm.sc.edu,,Bo Cai,,University of South Carolina,"800 Sumter St., Suite 205",803-777-5053,,bocai@gwm.sc.edu,Bayesian Semiparametric Frailty Selection in Multivariate Event Time Data,1,Bo,,Cai,University of South Carolina,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Biomedical studies often collect multiple event time data frommultiple clusters (either individual subjects or groups of subjects)within each of which event times for subjects are correlated and thecorrelation may vary in different classes. In such survival analyses,heterogeneity among clusters for overall and specific classes can beaccommodated by incorporating parametric gamma frailty or log-normalfrailty terms into the model. In thisarticle, we propose a Bayesian approach to relax the parametricdistribution assumption for overall and subject-specific frailties byusing a Dirichlet process prior while also allowing for theuncertainty of heterogeneity for different classes. Subject-specificfrailty selection relies on variable selection-type mixture priors byapplying mixtures of point masses at zero and inverse gammadistributions to the log-frailty variances. This selection allowsfrailty with zero variance to effectively drop out of the model. Areparameterization of log frailty is performed to reduce dependenceamong the parameters resulting in faster MCMC convergence. Simulateddata examples are presented and a real data example is also used forillustration.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Survival analysis,jxs1021@psu.edu,,Jianping Sun,,"Dept. of Statistics, Penn State University",326 Thomas Building,814-441-9436,,jxs1021@psu.edu,Composite likelihood: issues in efficiency,1,Jianping,,Sun,"Department of Statistics, Penn State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Maximum likelihood is a popular statistical method largely because it provides estimators with optimal statistical efficiency. However, many realistic statistical models are so complex in structure that it becomes computationally infeasible to find the MLE, especially in large data sets. One approach to solving this problem is the method of composite likelihood, which can reduce the complexity of computation at the price of some loss of efficiency. Because of its promising features, composite likelihood has recently become more and more popular in many fields such as longitudinal data, survival analysis, time series, spatial data and genetic data. A composite likelihood is constructed by taking a product of likelihood terms, each one of which is a likelihood, conditional or marginal, for some subset of the data. The statistical efficiency of such a composite likelihood then depends on the how it was constructed. In this talk, we will introduce the composite likelihood approach and then compare several methods for constructing them from an optimal efficiency point of view. To illustrate the method I will consider its use in a recombination model for DNA sequence data. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Statistical genetics,composite likelihoodxingao@umich.edu,,Xin (Cindy) Gao,Graduate Student Research Assistant,"University of Michigan, Department of Biostatistic",1420 Washington Heights,734-355-5968,,xingao@umich.edu,A Markov Compliance Class and Outcome Model for Causal Analysis in the Longitudinal Setting,1,Xin,,Gao,"Department of Biostatistics, School of Public Health, University of Michigan",Michael,R,Elliott,"Department of Biostatistics, School of Public Health, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a Markov compliance class and outcome model for analyzinglongitudinal randomized studies when non-compliance is present. Weconsider longitudinal studies where subjects are randomized to thetreatment or control group only at baseline, but subjects' compliancebehaviors may vary over time.  The proposed model considers theproblem in the potential outcome framework, and provides causalestimates on the effect of the treatment within principal strata,which are a function of the subject's adherence to various possiblerandomization assignments.  Previous research in this area (Lin, TenHave, and Elliott 2008) considered the effect of subjects' jointcompliance behavior on the joint distribution of the longitudinaloutcomes, but not the effect of outcomes at time t-1 on the compliancebehaviors at time t, which is often of great interest toinvestigators.  The proposed Markov compliance class and outcome modelprovides estimates both on the effect of the adherence on thefollowing outcome, and on the effect of the outcome on the followingadherence.  The model requires assumptions to be made about theunobservable correlation among a subject's potential outcomes.  Weconduct a sensitivity analysis by varying the correlation.  We analyzethe longitudinal Suicide CBT Study using the proposed method.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Missing data,sunni.barnes@baylorhealth.edu,,Sunni A Barnes,Biostatistician,Baylor Health Care System,8080 North Central Expressway,214-265-3634,,Sunni.Barnes@baylorhealth.edu,Sample Size Determination for a 5 Year Longitudinal Clinical Trial In Children: Using Simulation,1,Yahya,A,Daoud,Baylor Health Care System,Sunni,A,Barnes,Baylor Health Care System,Dunlei,,Cheng,Baylor Health Care System,Ed,,DeVol,,Richard,C,Boland,,,,,,,,,,,,,,,,,,,,,,"As consulting statisticians we are often faced with unique and challenging problems.  In a recent grant application we were faced with a request to estimate required sample size for a unique 5-year trial involving children.  The overarching goal of the grant is to determine whether the JCV virus plays an active role in the development of colorectal polyps, which are the precursors for cancer.  The specific hypothesis to be tested is that children with familial adenomatous polyposis (FAP) develop colonic polyps in association with infection by JCV.  The challenge in this case is the fact that the probability of developing JCV and the conditional probability of developing polyps given JCV seroconversion both change with age.  Since this is a 5-year study, the probability that a patient will get the JCV infection and then develop polyps increases each year they are in the study.  There was no well established method for determining the required sample size for this study. We developed a simulation program using S-Plus to estimate the required sample size.  We also compared our results with methods in the literature for estimating sample size when the probability of disease is constant by using mean probabilities and mean conditional probabilities.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Power analysis/sample size,Biopharmaceutical research,yyi@uwaterloo.ca,,Grace Y. Yi,Dr.,Univesity of Waterloo,200 University Ave. W.,5198884567ext.35110,,yyi@uwaterloo.ca,Marginal Analysis of Longitudianl Data with both Response and Covariates Subject to Missingness,1,Grace,Y,Yi,University of Waterloo,Baojiang,,Chen,,Richard,,Cook,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Data from longitudinal studies often feature both missing responses andmissing covariates. When the response is missing, the probability a particular covariate is missing is often higher, as a result of a positive association between the missingness for the response and covariates at each follow-up assessment.  The impact of missing data in these settings depends on the frequency data are missing and the strength of the association among the missing data processes and response process.  In the setting of incomplete response and covariate data, it is important to take the association between these processes into account when analysing data. Inverse probability weighted generalized estimating equations offer a method for doing this and we develop this here. Empirical results demonstrate that the proposed method yields consistent estimators, and is more efficient than alternativemethods which ignore the association between the missing data processes.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Missing data,soltani@kuc01.kuniv.edu.kw,,Ahmad Reza Soltani,Professor,Kuwait University,Department of statistics college of sdiences,+965 24985397,+965 2483 7332,soltani@kuc01.kuniv.edu.kw,A class of distributions with normal shape densities on finite intervals,1,Ahmad Reza,,Soltani,Kuwait University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In a series of papers van Dorp and Kotz (2002-2006) introduced  classes of distributions, called two sided power distributions and their generalizations, on finite intervals to model real data with finite range, mostly economical and medical data. They were apparently interested in normal densities on finite intervals. Symmetric two sided power distributions lack this property, due to certain irregularities. Peak of densities is too high, and densities are too narrow near their peaks. In this work by using certain transforms of certain Dirichlet distributions we present new classes of symmetric distributios on finite intervals that  their densities are very similar to the densities of normal distributions. The density is formulated using certain Guass Hypergeometric functions. Potentials of these distributions in modelling real data is brough into light. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Applied data analysis,Distribution Theorykjarcher@vcu.edu,,Kellie J. Archer,Assistant Professor,Virginia Commonwealth University,P.O. Box 980032,(804) 827-2039,(804) 828-8900,kjarcher@vcu.edu,Variable selection for ordinal response models with applications to high dimensional data,1,Kellie,J,Archer,"Department of BiostatisticsVirginia Commonwealth University",Andre,,Williams,"Department of BiostatisticsVirginia Commonwealth University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Although prediction accuracy is often a goal in predictive modeling, often it is of interest to identify features most predictive of the observed response. For high-dimensional datasets such as those arising from gene expression microarray studies, many have used penalized models for automatic identification of important features. Others have used variable importance measures from random forests for identifying important features within a high-dimensional dataset. Penalized models and random forests have been described and applied to continuous, nominal class, and survival responses. However, in a large number of biomedical applications, the response to be predicted may be inherently ordinal. Examples of ordinal responses include TNM stage (I, II, III, IV), drug toxicity (none, mild, moderate, severe), and response to treatment (complete response, partial response, stable disease, progressive disease). Herein, we propose a method for L1 penalized ordinal response models. We also describe measures of variable importance from bootstrapped classification trees using our proposed ordinal impurity function. Results will be presented for both simulated and genomic datasets.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Variable subset selection/model selection,krbroglio@mdanderson.org,,Kristine Broglio,,U.T. M.D. Anderson Cancer Center,PO Box 301402,713-563-4288,,krbroglio@mdanderson.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,mrelliot@umich.edu,,Michael R. Elliott,Associate Professor,University of Michigan,M4041 SPH II,734-647-5160,734-763-2215,mrelliot@umich.edu,Bayesian Inference for Mediation Effects Using Principal Stratification,1,Michael,R,Elliott,University of Michigan Dept. of Biostatistics,Trivellore,E,Raghunathan,University of Michigan Dept. of Biostatistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Most investigations in the social and health sciences aim to understand the causal relationship between a treatment or risk factor and outcome. Furthermore, given the multitude of pathways through which the treatment or risk factor may affect the outcome, there is also interest on decomposing the effect of a risk factor into ``direct' and ``mediated' effects. Building on the potential outcome framework for causal inference, we develop a Bayesian approach to estimate direct and mediating effects. This approach recognizes that direct and mediating effects can only be expressed as a range of plausible values of the population parameters constructed from potential populations. This range can be reduced by making further assumptions. For example, monotonicity or exclusionary restrictions used in the causal inference from randomized experiments reduces this range to a single estimable parameter. Such assumptions may not be reasonable in observational studies, or might be reasonable only in a stochastic, rather than deterministic, fashion. Here we use principal stratification (Rubin and Frangakis 2002) to draw inferences on the range of plausible values conditional on the observed data and perform sensitivity analysis using different prior distributions. The methodology is illustrated using both real and simulated examples. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Bayesian methods,leanna.g.stork@monsanto.com,,LeAnna G. Stork,Statistician,Monsanto Co.,Mail Zone O3A,314-694-7091,,leanna.g.stork@monsanto.com,Testing for Sufficient Similarity in Dose-Response in Complex Chemical Mixtures: Do Interaction and Dose Scale Matter?,1,LeAnna,G,Stork,Monsanto Co.,Scott,L,Marshall,"Department of Biostatistics, Virginia Commonwealth University",Chris,,Gennings,"Department of Biostatistics, Virginia Commonwealth University",Linda,K,Teuschler,"National Center for Environmental Assessment, U.S. EPA",John,,Lipscomb,"National Center for Environmental Assessment, U.S. EPA",Mike,,Devito,"National Health and Environmental Effects Research Laboratory, U.S. EPA",Kevin,,Crofton,"National Health and Environmental Effects Research Laboratory, U.S. EPA",,,,,,,,,,,,,"Chemical mixtures in the environment are often the result of a dynamic process. When dose-response data are available on random samples throughout the process, equivalence testing can be used to determine whether the mixtures are sufficiently similar in dose-response based on a pre-specified biologically important similarity region. Now consider a full mixture of c chemicals and suppose that s (s<c) of them are not dose-responsive individually. It may be reasonable to assume (under the assumption of no interaction) that a subset mixture of only the dose-responsive chemicals (in the same relative proportions) should be sufficiently similar to the same mixture with the addition of the chemicals that are not dose-responsive. The total dose scale of the full mixture is adjusted to account for the proportion of the non-dose-responsive chemicals that are present. A nonlinear mixed-effects model is fit to both mixtures to account for the random variability in the total dose. Equivalence testing logic is then applied to test for sufficient similarity in dose-response. An example using five organophosphorous pesticides is demonstrated. Partially supported by NIEHS #T32 ES007334 and #R01ES015276. Does not reflect USEPA policy and is not associated with Monsanto.           ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Toxicology/dose-response,Environmental and ecological applications,jgao@emory.edu,,Jingjing Gao,Ms,Emory University,1518 Clifton Rd,4047278210,,jgao@emory.edu,Evaluation of Individual Observer Agreement from Data with Repeated Measurements,1,Jingjing,,Gao,"Department of Biostatistics and Bioinformatics, Emory University",Michael,,Haber,"Department of Biostatistics and Bioinformatics, Emory University",Huiman,,Barnhart,Duke University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Coefficients of individual agreement between observers or methods ofmeasurement are based on the comparison of the between andwithin-observer mean squared deviation (MSD).   Methods for estimationof these coefficients from data where each observer makes replicatedmeasurements on each subject have been developed.  In thispresentation we introduce a simple method for estimation ofcoefficients of individual agreement when data consists of matchedsets of repeated measurements made under different conditions.  Theconditions may represent different time points, raters, laboratories,treatments, etc.  Our approach allows the values of the measuredvariable and the magnitude of disagreement to vary across theconditions.  The new approach is illustrated via two examples fromstudies designed to compare (a) methods of evaluating carotid stenosisand (b) methods of measuring percent body fat.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Agreement,Longitudinal data,zhang@hsph.harvard.edu,,Lingsong Zhang,,Harvard School of Public Health,Department of Biostatistics,617-432-4924,,zhang@hsph.harvard.edu,On margin-based classification methods,1,Lingsong,,Zhang,"Department of Biostatistics, Harvard School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"SVM and DWD are two margin-based classifiction methods. In this paper,we investigate and integrate these two methods into a more generalsetting. Some Theoretical and empirical properties are explored as well.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Data mining/massive data sets,Machine learning,shuc@amgen.com,,Lisa Chen,,Amgen Inc,One Amgen Center Dr. MS 24-2-C,805-447-7199,,shuc@amgen.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,jincaowu@umich.edu,,Jincao Wu,Ph.D. Candidate,"Department of Biostatistics, University of Michiga",1952 Traver Road Apt 202,734-272-9631,,jincaowu@umich.edu,A Bayesian Generalized Non-Linear Predictive Model of Treatment Efficacy Using qMRI,1,Jincao,,Wu,"Department of Biostatistics, University of Michigan, Ann Arbor, MI",Timothy,D,Johnson,"Department of Biostatistics, University of Michigan, Ann Arbor, MI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The prognosis for patients with high-grade gliomas is poor with a median survival of one year after diagnosis.  The assessment of treatment efficacy is typically unavailable until about 8 to 10 weeks post treatment.  Investigators hypothesize that recently developed Quantiative MRI (qMRI) techniques can predict the treatment efficacy only 3 weeks from the initiation of the therapy thereby allowing second line therapies to begin earlier. The purpose of this work is to build a predictive model for the treatment efficacy based on qMRI data and baseline prognostic factors. We use 1 year survival status as the outcome and propose a Bayesian joint model. In the first stage, we smooth the qMRI data using a pairwise-difference prior and derive summary statistics.  In the second stage, these statsistics are used in a generalized non-linear model with a Multivariate Adaptive Regression Spline (MARS) basis in the systematic component and a probit link. Gibbs sampling and reversible jump Markov chain Monte Carlo are applied iteratively between the two stages to estimate the posterior.  Bayesian model averaging is employed to derive the final predictive model.",FALSE,FALSE,,FALSE,FALSE,TRUE,short course: SC5,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Imaging,Bayesian methods,hubbard.r@ghc.org,,Rebecca Hubbard,,Group Health Center for Health Studies,1730 Minor Ave,206-287-2066,,hubbard.r@ghc.org,Modeling the cumulative risk of a false positive screening mammogram,1,Rebecca,A,Hubbard,Group Health Center for Health Studies,Diana,L,Miglioretti,Group Health Center for Health Studies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Mammography is the only screening modality shown to reduce breast cancer mortality among women 50 and older in clinical trials. However, screening mammography is associated with adverse outcomes including false-positive recalls and benign biopsies. False-positive results lead to increased cost as well as patient anxiety. There are several existing statistical methods for estimating the cumulative risk of a false positive screening exam.  Specifically, regression models have been developed for estimating the effect of fixed and random effects on the cumulative risk of a false positive screening exam.  However, these models make use of possibly unrealistic assumptions about independence of the history of exam results and the number of screens obtained.  Additional methods have been developed that do not rely on this assumption, but these methods do not allow for the inclusion of random effects.  We will review existing methods and discuss their limitations and assumptions.  We then apply these methods to a population based study of screening mammography, comparing inference on the false positive recall rate and investigating the appropriateness of modeling assumptions. Based on the performance of existing statistical methods, we will propose extensions to address limitations or unrealistic assumptions.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Diagnostic and screening tests,Cancer applications,ndyanez@hotmail.com,,nd,yanez,uw,box 357232,206-543-1044,,ndyanez@hotmail.com,a test,1,a,,test,uw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,ajolly@sph.emory.edu,,Anna Jolly,,Emory University,352 Cherokee Ave. SE,404-310-3580,,ajolly@sph.emory.edu,A Two-Stage Approach for Detecting Clusters of Peaks with Periodicity in NMR Spectra,1,Anna,K,Jolly,Emory University,Amita,,Manatunga,Emory University,Tianwei,,Yu,Emory University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Nuclear magnetic resonance (NMR) spectroscopy has been usedincreasingly in recent years as a means of obtaining metabolicinformation from individuals.  In our study, we consider NMR spectraobtained from the blood plasma of subjects every hour over a 25-hourperiod.  The identification of peaks with periodic behavior is ofinterest.  A two-stage process is proposed, the first stage of whichuses periodic regression to estimate the parameters corresponding toperiod for the various peaks.  In the second stage, a mixture model isused to develop clusters of peaks, taking into account the variabilityof the estimates obtained in the first stage.  Using simulationstudies, we demonstrate the performance of the two-stage method andthen apply the method to blood plasma spectra.Key words:  Periodicity, NMR Spectra, Clustering",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Clustered data methods,Metabolomics,Periodicityrsabo@vcu.edu,,Roy T. Sabo,Assistant Professor,Virginia Commonwealth University,"730 East Broad Street, Theater Row",804-828-3047,804-828-8900,rsabo@vcu.edu,Estimation Methods for an Autoregressive Familial Correlation Structure,1,Roy,T.,Sabo,"Department of Biostatistics,Virginia Commonwealth University",N.,R.,Chaganty,"Department of Mathematics and Statistics,Old Dominion University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this paper we apply an autoregressive correlation structure to the analysis of familial clustered data in the one-parent case with homogeneous intra-class variance. We use the quasi-least squares procedure to derive estimators of the correlation parameters and compare them with maximum likelihood and moments estimators. Asymptotically, the quasi-least squares estimators are nearly as efficient as the maximum likelihood estimators. The small-sample case is analyzed through simulation, and the quasi-least squares estimators are found to be more robust than the maximum likelihood estimators. A simple example is given to show the application of the estimation procedures. We also allude to the heterogeneous intra-class variance case, where the quasi-least squares procedure has certain advantagesover the maximum likelihood procedure.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Clustered data methods,Longitudinal data,dingrui0417@gmail.com,,Rui Ding,,Jong-Min Kim,600 EAST 4TH ST UMM MAIL 139,952-221-5985,,dingrui0417@gmail.com,Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression using Java Visualizaiton,1,Rui,,Ding,"Statistics Discipline, Division of Science and Mathematics, University of Minnesota, Morris",Jong-Min,,Kim,"Statistics Discipline, Division of Science and Mathematics, University of Minnesota, Morris",Deukwoo,,Kwon,"Division of Cancer Epidemiology and Genetics, National Cancer Institute",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this research, we do Java visualization approach to molecular classification of cancer based on gene expression and our new approach is applied to human acute leukemias as an example case. First, we perform a Bayesian variable selection for finding some meaningful genes from 7128 genes. And then with the selected genes, we do Java visualization of gene data for classification, since visualization is a good way to classify data. We randomly choose two genes out of the selected gene data to make a 2D figure, and also randomly choose three out of the selected gene data to make a 3D Figure, meanwhile, we do a dimension reduction from 7128 gene data to three-dimensional view by one of statistical multivariate methods, Principal Component Analysis. Finally, we create the Java visualization website of this research for other researchers who are interested in Molecular Classification of Cancer.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Graphical models,stanek@schoolph.umass.edu,,Ed Stanek,Professor of Biostatistics,"Div of BIostat&Epi, UMASS- Amherst",401 Arnold House,413-545-3812,413-545-1645,stanek@schoolph.umass.edu,Meta Analysis of Soil Ingestion Intake for Childhood Risk Assessment,1,Edward,J.,Stanek III,"Division of Biostatistics and EpidemiologyU of Mass, Amherst",Edward,J.,Calabrese,"Division of Environmental HealthU of Mass, Amherst",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Meta Analysis of Soil Ingestion Intake for Childhood Risk AssessmentCasual ingestion of soil by young children frequently is the mostimportant source of contaminant exposure when assessing risk ofcontaminated sites.  Estimates of soil ingestion in the US are basedon several mass-balance soil ingestion studies, but the methodologyhas the potential for bias and high variability.  We develop aconceptual framework for a soil ingestion model, and use it toestimate the distribution of soil ingestion in a Monte-Carlo exposureassessment of children.  The research uses data from four previouslyconducted mass-balance soil ingestion studies among children in theUS.  We describe a basic deterministic mass-balance model, andintroduce random variables to translate it into a stochastic model. We present steps used to estimate reliability in the stochastic model,and based on the reliability estimates, identify data values that areconsidered to have high potential to include bias.  We show how thisprocess leads to identifying data for the meta analysis, and guidesanalysis decisions.  In so doing, we discuss limitations of data,identify critical decisions and assumptions, and present the  formalmeta analysis plan and results.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Environmental and ecological applications,Applied data analysis,ProschaM@mail.nih.gov,,Michael Proschan,Mathematical Statistician,National Institute of Allergy and Infectious Disea,"6700A Rockledge Drive, Room 5140",301-451-5129,301-480-0912,ProschaM@mail.nih.gov,Are Things Really As Un-Rosi As They Appear?,1,Michael,A,Proschan,National Institute of Allergy and Infectious Diseases,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A recent meta-analysis implicated the diabetes drug rosiglitazone in an excess of myocardial infarctions and cardiovascular deaths compared to control.  Many of the trials in the meta-analysis had very few events, and a naïve approach of combining all control arms and comparing to the combined rosiglitazone group actually showed a higher proportion of events on control.  Is the evidence against rosiglitazone really convincing?  What is the appropriate analysis when several trials have very few events?  What sort of sensitivity analyses can be done to either bolster or cast doubt on the results?  These are some of the topics covered in this talk.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Categorical data,mzhangst@umich.edu,,Min Zhang,Assistant Professor,University of Michigan,"University of Michigan, Biostatistics Department",734-763-9385,734-763-2215,mzhangst@umich.edu,Inference on treatment effects from a randomized clinical trial in the presence of premature treatment discontinuation: The SYNERGY trial,1,Min,,Zhang,"Department of Biostatistics, University of Michigan",Anastasios,A.,Tsiatis,"Department of Statistics, North Carolina State University",Marie,,Davidian,"Department of Statistics, North Carolina State University",Karen,S.,Pieper,"Duke Clinical Research Institute, Durham, North Carolina",Kenneth,,Mahaffey,"Duke Clinical Research Institute, Durham, North Carolina",,,,,,,,,,,,,,,,,,,,,"The SYNERGY trial was a randomized, open-label clinicaltrial designed to compare two anti-coagulant drugs on the basis ofvarious time-to-event endpoints. As usual, the protocol dictatedcircumstances, such as occurrence of a serious adverse event, underwhich it was mandatory for a subject to discontinue his/her assignedtreatment.  In addition, as in the execution of many trials, somesubjects did not complete their assigned treatment regimens but ratherdiscontinued study drug prematurely for other, 'optional' reasonsnot dictated by the protocol; e.g., switching to the other studytreatment or stopping treatment altogether at their or theirprovider's discretion.  In this situation, as an adjunct to the usualintent-to-treat analysis, interest may focus on inference on the``true' treatment effect; i.e., the difference in survivaldistributions were all subjects in the population to follow theassigned regimens and, if to discontinue treatment, do so onlyfor mandatory reasons.  Approaches toinference on this effect used commonly in practice are ad hoc andhence are not generally valid.  We use SYNERGY as a motivating casestudy to propose generally-applicable methods for estimation andtesting of this 'true' treatment effect by placing the problem inthe context of causal inference on dynamic treatment regimes.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Survival analysis,marshallsl2@vcu.edu,,Scott Marshall,,Graduate Student VCU Biostatistics,730 East Broad Street,8078272060,,marshallsl2@vcu.edu,Investigating Statistical Distance as a Similarity Measure for Determining Sufficient Similarity in Dose-Response in Chemical Mixtures,1,Scott,,Marshall,Biostatistics VCU,Chris,,Gennings,Biostatistics VCU,LeAnna,G,Stork,Monsanto,Linda,,Teuschler,U.S. EPA/NCEA ,John,,Lipscomb,U.S. EPA/NCEA ,Mike,,DeVito,U.S. EPA/NHEERL,Kevin,,Crofton,U.S. EPA/NHEERL,,,,,,,,,,,,,"Chemical mixtures in the environment are often the result of a dynamic process.  For the purposes of risk assessment it is of interest to test whether these resulting candidate mixtures are sufficiently similar to a reference mixture when only the mixing ratios are available for the candidate mixtures. Using statistical equivalence testing logic and mixed model theory an approach has been developed, that extends the work of Stork et al (JABES,2008), to define sufficient similarity in dose-response for chemical mixtures containing the same chemicals with different ratios or a subset of chemicals. Total dose of the mixture can be adjusted for chemicals not in the subset.  Four similarity measures based on combinations of adjusted total dose and weights based on potencies are described.  A simulation study was conducted to assess the power of the approach. The current work estimated how often a resulting candidate mixture was sufficiently similar in dose-response to the reference mixture. (Partially supported by NIEHS #T32 ES007334 and # R01ES015276 and does not reflect USEPA policy.  This research is not associated with Monsanto.)",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Toxicology/dose-response,Environmental and ecological applications,yanbing.zheng@uky.edu,,Yanbing Zheng,Assistant Professor,University of Kentucky,873 Patterson Office Tower,8592573742,,yanbing.zheng@uky.edu,Hierarchical Dynamic Modeling of Spatial-Temporal Binary Data,1,Yanbing,,Zheng,"Department of Statistics, University of Kentucky",Jun,,Zhu,"Department of Statistics, University of Wisconsin - Madison",Brian,,Aukema,"Natural Resources Canada, Canadian Forest Service and Ecosystem Science and Management Program, University of Northern British Columbia",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We develop spatial-temporal generalized linear mixed models for spatial-temporal binary data observed on a spatial lattice and repeatedly over discrete time points. To account for spatial and temporal dependence, we introduce a spatial-temporal random effect in the link function and model by a diffusion-convection dynamic model. We propose a Bayesian hierarchical model for statistical inference and devise Markov chain Monte Carlo algorithms for computation. We illustrate the methodology by an example of outbreaks of mountain pine beetle on the Chilcotin Plateau of British Columbia, Canada. We examine the effect of environmental factors while accounting for the potential spatial and temporal dependence. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Environmental and ecological applications,lu@stat.ncsu.edu,,Wenbin Lu,Assistant Professor,North Carolina State University,2501 Founders Drive,919-515-1915,,lu@stat.ncsu.edu,Penalized Estimating Equations for Semiparametric Linear Transformation Models,2,Hao,,Zhang,North Carolina State University,Wenbin,,Lu,North Carolina State University,Hansheng,,Wang,Peking University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Semiparametric linear transformation models have received much attention due to its high flexibility in modeling survival data. However, the problem of variable selection for linear transformation models is less studied since a convenient loss function is not available. In this paper, we propose a penalized estimating equation (PEE) approach for joint parameter estimation and variable selection for linear transformation models. The new procedure consists of computing the 'profiled score' from the estimating equations for the finite dimensional parameter,constructing the variance weighted L_2-distance based on the profile score, and minimizing the distance subject to some shrinkage penalty. The resulting estimator is shown to be consistent in both parameter estimation and variable selection, and asymptotically normal with improved efficiency. An efficient one-step algorithm is further proposed, which makes it possible to obtain the entire solution path of the estimate. Numerical studies show that the PEE performs competitively with other likelihood based methods.",FALSE,FALSE,,FALSE,FALSE,TRUE,I will chair the invited session proposed by Dr. Limin Peng for quantile regression with censored survival data.,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Variable subset selection/model selection,Estimating equations,liangy@uthscsa.edu,,Yuanyuan Liang,,UT Health Science Center at San Antonio,"7703 Floyd Curl Drive, MSN 7933",(210) 567-0854,,liangy@uthscsa.edu,On the Role of Baseline Measurements for Crossover Designs under the Self and Mixed Carryover Effects Model,1,Yuanyuan,,Liang,University of Texas Health Science Center at San Antonio,Keumhee Chough,,Carriere,University of Alberta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Generally, crossover designs are not recommended when carryover effects are present and when the primary goal is to obtain an unbiased estimate of the treatment effect. In some cases, baseline measurements are believed to improve design efficiency. This paper examines the impact of baselines on optimal designs using two different assumptions about carryover effects during baseline periods and employing a non-traditional crossover design model. As anticipated, baseline observations improve design efficiency considerably for two-period designs, which use the data in the first period only to obtain unbiased estimates of treatment effects, while the improvement is rather modest for three- or four- period designs. Further, we find little additional benefits for measuring baselines at each treatment period as compared to measuring baselines only in the first period. Although our study of baselines did not change the results on optimal designs that are reported in the literature, the problem of strong model dependency problem is generally recognized. The advantage of using multi-period designs is rather evident, as we found that extending two-period designs to three- or four-period designs significantly reduced variability in estimating the direct treatment effect contrast.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Experimental design,Clinical trials,mdswartz@mdanderson.org,,Michael Swartz,,Department of Epidemiology,University of Texas M. D. Anderson Cacner Cetner,713-792-8264,,mdswartz@mdanderson.org,Reducing Costs of Two Stage Genome Wide Association Studies,1,Michael,D.,Swartz,"Department of Epidemiology, University of Texas M. D. Anderson Cancer Center, Houston, TX, USA",Sanjay,,Shete,"Department of Epidemiology, University of Texas M. D. Anderson Cancer Center, Houston, TX, USA.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genome Wide Association (GWA) Studies continue to increase inpopularity.  Although the cost of genotyping continues to go down, GWAstudies continue to carry a high price tag.  This is due to the largesample of individuals required to detect the small effect contributedto disease risk from individual SNPs.    As a result, methodologiesfor GWA studies continue to strike a balance between cost and power.  This presentation proposes a two stage design that reduces the costof a GWA study by reducing the genotyped individuals in stage 2without sacrificing power.  We introduce an ascertainment scheme forindividuals in stage 1 such that only a subset of the stage 1individuals are brought forward to stage 2.  We used the simulateddata from the Genetic Analysis Workshop 15 to evaluate our method andcompare its performance to the typical two stage design.  Oursimulation studies show that by ascertaining individuals from stage 1from cases and controls neither increases false positives nordecreases power, and still can substantially reduce the cost of thestudy. ",FALSE,FALSE,,FALSE,FALSE,TRUE,SC 6 INTERMEDIATE BAYESIAN DATA ANALYSIS,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Epidemiologic methods,steve-hillis@uiowa.edu,,Stephen L Hillis,Senior Biostatistician,VA Iowa City Medical Center,CRIISP (152),319-338-8801 x7680,,steve-hillis@uiowa.edu,Rexamination and further development of the Roe and Metz simulation model for multiple reader ROC decision data,1,Stephen,L,Hillis,VA Iowa City Medical Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The simulation model proposed by Roe and Metz (RM) is the most commonly used model for evaluating the performance of methods designed to analyze multireader ROC data that take into account both reader and case variability.  In this talk I examine the RM model in more detail.  I reformulate the model using more standard statistical notation to show that the model is actually a four-factor model with certain effects set to zero, discuss the conceptual motivation for the model, discuss extensions of the model, derive the population parameters of interest, and show the relationship between the RM variance components and the variance components for the two estimation methods proposed by Dorfman, Berbaum & Metz,  and Obuchowski & Rockette.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,ROC analysis,Diagnostic and screening tests,sinhad@stat.fsu.edu,,debajyoti sinha,hobbs endowed professor,"dept of statistics, florida state university",3042 ST ANDREWS WAY,8506457928,,sinhad@stat.fsu.edu,Analysis of Cure Rate Survival Data Under Proportional Odds Model,1,debajyoti,,sinha,"dept of statistics, florida state university",sudipto,,banerjee,"dept of biostaistics, university of minnesota",yu,,gu,"dept of statistics, florida state university",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"With rapid improvements in medical treatment and healthcare, many survival data sets now reveal a substantial portion ofpatients who are cured (that is, who never experience theendpoint). Extended survival models called cure rate modelsaccount for the probability of a subject being cured. Our present workproposes a new class of cure rate models that has a proportional odds structure as a function of the covariates. This class also has someunique properties which are different from those of classicalproportional odds survival models. In this article we address issues such as regression effects on thecure fraction, associated Bayesian analysis andpropriety of the associated posterior distributions underdifferent modelling assumptions.  Finally, we illustrate withreanalysis of two data sets (one on melanoma and the other on breastcancer) ourmodel's distinguishing features of our models and implementation ofBayesian data analytic tolls for this model. We also develop a new setof methods for Bayesian model selection and model assessment for thecure-rate as well as classical survival models.",FALSE,FALSE,,FALSE,FALSE,TRUE,Biometrics Editorial Board meeting,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Bayesian methods,slsimpso@wfubmc.edu,,Sean L Simpson,Assistant Professor,Wake Forest University School of Medicine,Medical Center Blvd.,336-716-8369,336-716-6427,slsimpso@wfubmc.edu,A Kronecker Product Linear Exponent AR(1) Family Of Correlation Structures,1,Sean,L,Simpson,"Department of Biostatistical Sciences,Wake Forest University School of Medicine ",Lloyd,J,Edwards,"Department of Biostatistics,University of North Carolina at Chapel Hill",Keith,E,Muller,"Division of Biostatistics,Department of Epidemiology and Health Policy Research,University of Florida at Gainesville ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The linear exponent autoregressive (LEAR) correlation structure,introduced by Simpson et al. (2009), is a flexible two-parametercorrelation model that can be applied in situations in which thewithin subject correlation is believed to decrease exponentially intime or space.  It allows for an attenuation or acceleration of theexponential decay rate imposed by the commonly used continuous-timeAR(1) structure.  We propose a Kronecker product LEAR correlationstructure for multivariate repeated measures data in which thecorrelation between measurements for a given subject is induced by twofactors.  The model allows for an imbalance in both dimensions acrosssubjects.  This four-parameter structure is especially attractive forthe High Dimension, Low Sample Size cases so common in medical imagingand various kinds of '-omics' data.  Excellent analytic and numericalproperties make the Kronecker product LEAR model a valuable additionto the suite of parsimonious correlation structures for multivariaterepeated measures data.  We employ the model in the analysis of alongitudinal imaging data example.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Imaging,Spatial/temporal modeling,sportnoy@uiuc.edu,,Xuming He,Professor,University of Illinois,Statistics Department,217 333 2167,,sportnoy@uiuc.edu,Quantile Regression for Doubly Censored Data,2,Guixian,,Lin,"University of Illinois, Statistics Department",Xuming,,He,"University of Illinois, Statistics Department",Stephen,,Portnoy,"University of Illinois, Statistics Department",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Quantile regression offers a semiparametric approachfor analyzing data with possible heterogeneity. It isparticularly powerful for censored responses, wherethe conditional mean functions are unidentifiablewithout strict parametric assumptions on thedistributions. Recent work by Portnoy (2003) and byPeng and Huang (2008) demonstrated how theKaplan-Meier estimator and the Nelson-Aaronestimator for the univariate samples can begeneralized for estimating the conditional quantilefunctions with right censored data. We propose anew algorithm for quantile regression when theresponse variable is doubly censored. The algorithmdistributes probability mass of each censored pointto its left or right in a self-consistent manner,taking the idea of Turnbull (1976) to a broaderplatform. The algorithm is insensitive to startingvalues, and can be used to estimate a set ofquantile functions with a small number of iterations.Computational and Asymptotic properties of the methodwill be summarized and issues concerning implementationand inference will be discussed. Extensions to moregeneral forms of censoring will also be explored.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Survival analysis,quantile regressionx-he@uiuc.edu,,Xuming He,Professor,University of Illinois,Statistics Department,217 333 2167,,x-he@uiuc.edu,Quantile Regression for Doubly Censored Data,2,Guixian,,Lin,"University of Illinois, Department of Statistics",Xuming,,He,"University of Illinois, Department of Statistics",Stephen,,Portnoy,"University of Illinois, Department of Statistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Quantile regression offers a semiparametric approachfor analyzing data with possible heterogeneity. It isparticularly powerful for censored responses, wherethe conditional mean functions are unidentifiablewithout strict parametric assumptions on thedistributions. Recent work by Portnoy (2003) and byPeng and Huang (2008) demonstrated how theKaplan-Meier estimator and the Nelson-Aaronestimator for the univariate samples can begeneralized for estimating the conditional quantilefunctions with right censored data. We propose anew algorithm for quantile regression when theresponse variable is doubly censored. The algorithmdistributes probability mass of each censored pointto its left or right in a self-consistent manner,taking the idea of Turnbull (1976) to a broaderplatform. The algorithm is insensitive to startingvalues, and can be used to estimate a set ofquantile functions with a small number of iterations.Computational and Asymptotic properties of the methodwill be summarized and issues concerning implementationand inference will be discussed. Extensions to moregeneral forms of censoring will also be explored. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Survival analysis,quantile regressionyyuan@mdanderson.org,,Ying Yuan,,MD Anderson Cancer Center,1400 Pressler Street,7135634271,,yyuan@mdanderson.org,Meta-Analysis of Studies with Missing Data,1,Ying,,Yuan,"Department of Biostatistics, University of Texas M.D. Anderson CancerCenter",Roderick,,Little,"Department of Biostatistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Consider a meta-analysis of studies with varying proportions ofpatient-level missingdata, and assume that each primary study has made certain missing dataadjustments so that the reported estimates of treatment effect size andvariance are valid. These estimates of treatment effects can be combinedacross studies by standard meta-analytic methods, employing arandom-effectsmodel to account for heterogeneity across studies. However, we note that ameta-analysis based on the standard random-effects model will lead tobiasedestimates when the attrition rates of primary studies depend on thesize ofthe underlying study-level treatment effect. Perhaps ignorable within eachstudy, this type of missing data is in fact not ignorable in ameta-analysis. We propose three methods to correct the bias resulting fromsuch missing data in a meta-analysis: re-weighting the DerSimonian-Lairdestimate by the completion rate; incorporating the completion rate into aBayesian random-effects model; and inference based on a Bayesianshared-parameter model that includes the completion rate. We illustratethese methods through a meta-analysis of 16 published randomizedtrials thatexamined combined pharmacotherapy and psychological treatment fordepression.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Random effects,swang@smu.edu,,Xinlei Wang,Assistant Professor,Southern Methodist University,104 Heroy Science Hall,214-768-2459,,swang@smu.edu,Modelling Three dimensional Chromosome Structures Using Gene Expression Data,2,Guanghua,,Xiao,"Division of BiostatisticsDepartment of Clinical SciencesUniversity of Texas Southwestern Medical Center",Xinlei,,Wang,"Department of Statistical ScienceSouthern Methodist University",Arkady,,Khodursky,"Dept. Biochemistry, Molecular Biology and BiophysicsUniversity of Minnesota",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recently, many genomic studies have shown that significant chromosomal spatial correlation exists in gene expression of many organisms. Co-expression has been frequently observed among genes that are far apart along a chromosome chain, but brought into physical proximity by three-dimensional chromosome structures. Ignoring such correlation in statistical modeling can greatly reduce the efficiency of estimation and the power of statistical inference. Further, modeling the spatial correlation explicitly will be extremely useful for biologists to identify co-regulated genes and understand the underlying transcriptional regulation mechanism. In this paper,  we construct a mathematical model for a spiral/helix-like folding structure suggested by several biological studies, and propose a statistical method to incorporate the induced correlation structure. The proposed method, can improve the estimation of gene expression. More importantly, it will be the first to model and infer the local 3D chromosome structure, and directly test its role in gene regulation.",FALSE,FALSE,,FALSE,FALSE,FALSE,"Cannot be on Tuesday, March 18.",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Microarray analysis,Bayesian Spatial Methodsmaarten.bekaert@ugent.be,,Maarten Bekaert,Phd-student,University of Ghent,"Krijgslaan 281,S9",003292644881,,maarten.bekaert@ugent.be,Estimation of marginal structural survival models in the presence of competing risks,1,Maarten,,Bekaert,"Department of Applied Mathematics and Computer Science, Ghent University, Ghent, Belgium",Stijn,,Vansteelandt,"Department of Applied Mathematics and Computer Science, Ghent University,Ghent, Belgium",Karl,,Mertens,"Epidemiology Unit, Scientific Institute of Public Health, Brussels, Belgium",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The effect on mortality of acquiring a nosocomial infection in the intensive care unit (ICU) is poorly understood because of informative censoring of the survival time by discharge from the ICU and because of time-dependent confounders, which lie on the causal path from infection to mortality. Standard statistical analyses may be severely misleading in such settings and have shown contradictory results. To accommodate informative censoring and because physicians' interest is often in 30-day ICU mortality, we will consider discharge from the ICU as a competing risk. To additionally accommodate time-dependent confounding, we propose marginal structural models for the counterfactual subdistribution hazard which express the subdistribution hazard that would be observed if the entire study population were to acquire infection at a given time in the ICU. We develop inference for marginal structural subdistribution hazard models and use it to quantify the causal effect of nosocomial infection on mortality in the ICU using data from the National Surveillance Study of Nosocomial Infections in ICU's (Belgium).Key words: Causal inference; Competing risk; ICU; Nosocomial infection; Time-dependent confounding.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Survival analysis,fridley.brooke@mayo.edu,,Brooke L Fridley,,Mayo Clinic,200 First St SW,507-538-3646,,fridley.brooke@mayo.edu,Incorporation of prior knowledge from linkage studies into genetic association studies,1,Brooke,L,Fridley,Mayo Clinic,Daniel,,Serie,Mayo Clinic,Kristin,L,White,Mayo Clinic,Gregory,,Jenkins,Mayo Clinic,William,,Bamlet,Mayo Clinic,Ellen,L,Goode,Mayo Clinic,,,,,,,,,,,,,,,,,"In the last decade, numerous genome-wide linkage and association studies of complex diseases have been completed. These datasets and/or results are often available to investigators through collaborations or dbGaP.  Critical questions remain regarding how best to use this valuable information to inform current, on-going or future genetic association studies to improve study design and statistical analysis. One promising approach to incorporating prior knowledge from linkage scans or other information is to up- or down-weight p-values resulting from genetic association study in either a frequentist or Bayesian approach. An alternative approach would use a fully Bayesian analysis for the genetic association study, where the specification of the prior distributions is based on the prior knowledge.  We propose a Bayesian mixture model for analysis of genetic association studies to incorporate prior knowledge based on biology or linkage results. We illustrate the proposed method using a genetic association study of colon polyps and a genome-wide linkage study of colon cancer. In addition to application to genetic studies of colon cancer, we present results from a variety of simulation studies.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Bayesian methods,david.fardo@uky.edu,,David Fardo,,University of Kentucky,College of Public Health - Biostatistics,859-218-2070,859-257-4665,david.fardo@uky.edu,A novel test for quality control in family-based genome-wide association studies,1,David,,Fardo,University of Kentucky,Iuliana,,Ionita-Laza,Harvard School of Public Health,Christoph,,Lange,Harvard School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Allele transmissions in family pedigrees provide an intuitive andunique way to evaluate the genotyping quality of a particular probandin a family-based association study. We propose a transmission testthat is based on this feature and that can be used for quality controlfiltering of genome-wide genotype data for individual probands. Thetest has one degree of freedom and assesses the average genotypingerror rate of the genotyped SNPs for a particular proband. As we showin simulation studies, the test is sufficiently powered to identifyprobands with unreliable genotyping quality that cannot be detectedwith standard quality control filters. This feature of the test isfurther exemplified by an application to a genome-wide associationstudy. The test seems to be ideally suited as the final layer ofquality control filters in the cleaning process of genome-wideassociation studies since it is able to identify probands with poorgenotyping quality who have slipped through the standard qualitycontrol filtering.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Genomics,lche2@fhcrc.org,,Lin,,Fred Hutchinson Cancer Research Center,14802 Bothell Way NE #311,206-667-4413,,lche2@fhcrc.org,Mining pathway-based SNP sets in GWAS study with sparse logistic regression,1,Lin,S,Chen,Fred Hutchinson Cancer Research Center,Ulrike,,Peters,Fred Hutchinson Cancer Research Center,Li,,Hsu,Fred Hutchinson Cancer Research Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genome-wide association study (GWAS) conducts marginal tests forassociation between disease status and markers across the entiregenome.  The huge number of SNP markers and often the high dependenceamong them make it challenging to detect genetic variationscontributing to the complex disease of interest, and to interpret theidentified significant SNPs with sound and convincing biologicalevidence.  To more efficiently mining and interpreting SNP informationacross the entire genome, we propose to map SNPs to the gene level,and explore gene-gene and/or gene-environment interaction within apriori defined disease-related pathways or gene sets. Specifically, weperform principal component analysis on SNPs mapped to each gene toconstruct eigen-SNPs, each of which is a pseudo SNP that captureindependent genetic structure within the gene. Applying lasso logisticregression to each gene, we are able to flexibly select diseaseassociated eigen-SNPs based on gene structure. We further apply grouplasso logisticregression to the selected eigen-SNPs to characterize gene-gene andgene-environment interaction within a pathway. As an example, wepresent an application to a colon cancer GWAS.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Statistical genetics,xyshi@email.unc.edu,,Xiaoyan Shi,,University of North Carolina at Chapel Hill,5031 Wallingford Dr Apt A,9192601787,,xyshi@email.unc.edu,Intrinsic Regression Models for Medial Representation of Subcortical Structures,1,Xiaoyan,,Shi,Univ of North Carolina at Chapel Hill,Hongtu,,Zhu,Univ of North Carolina at Chapel Hill,Joseph,G,Ibrahim,Univ of North Carolina at Chapel Hill,Faming,,Liang,Texas A&M University,Martin,,Styner,Univ of North Carolina at Chapel Hill,,,,,,,,,,,,,,,,,,,,,"The aim of this paper is to develop a statistical framework for  describing the variability of the medial representation (m-rep) of subcortical structures and its association with covariates in Euclidean space. Because an m-rep does not form a vector space, applying classical multivariate regression techniques may be inadequate in establishing the association between an m-rep and covariates of interest in real applications. Our proposed regression model as a semiparametric model avoids specifying a probability distribution on a Riemannian manifold. We develop an estimation procedure based on the annealing evolutionary stochastic approximation Monte Carlo (AESAMC) algorithm to obtain parameter estimates and establish their limiting distributions. We use Wald statistics to test linear hypotheses of unknown parameters. Simulation studies are used to evaluate the accuracy of our parameter estimates and examine the finite sample performance of the Wald statistics. We apply our methods to the detection of the difference in the morphological changes of the left and right hippocampi between schizophrenia patients and healthy controls using medial shape description.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,Imaging,chenhu@umich.edu,,Chen Hu,,"Department of Biostatistics, University of Michiga",1420 Washington Heights,734-358-2110,,chenhu@umich.edu,Joint Modeling of Survival and Binomial Data Based on Generalized Self-consistency with Application to Prostate Cancer Stage-specific Incidence,1,Chen,,Hu,"Department of Biostatistics, School of Public Health, University of Michigan",Alex,,Tsodikov,"Department of Biostatistics, School of Public Health, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Stage-specific cancer incidence represents a random vector of jointbivariate response represented by the age at diagnosis, and cancerstage.  Factors affecting the unobserved tumor progression and thehistory of metastasis before diagnosis are of particular interest. Semiparametric models with time-dependent covariates are consideredfor the joint response.  We extend the framework of the generalizedself-consistency approach (Tsodikov 2003 JRSSB) and use EM algorithmfor maximum likelihood estimation and model building.  This method isillustrated by real data from the Surveillance, Epidemiology and EndResults (SEER) program and by simulation studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Cancer applications,dsotres@bios.unc.edu,,Daniela Sotres-Alvarez,,PhD student,"3101 McGavran-Greenberg, CB#7420",919-338-2637,,dsotres@bios.unc.edu,Latent transition models to study change in dietary patterns over time,1,Daniela,,Sotres-Alvarez,"UNC Chapel Hill, Biostatistics Department",Amy,H,Herring,"UNC Chapel Hill, Biostatistics Department",Anna Maria,,Siega-Riz,"UNC Chapel Hill, Epidemiology Department",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Latent class models (LCM) have been shown empirically to be more appropriate to derive dietary patterns (DP) than cluster analysis since they allow different outcome distributions, correlated measurement errors, and adjustment for energy intake and covariates. The latent transition model (LTM) might be useful to study change as characterized by the movement between discrete DP. In practice, LTM have been mostly used in the social sciences and for applications with few nominal outcomes, and have not been used to study movement between DP. In addition to the problem on how to determine the number of classes, there are several challenges particular to DP analysis: large (>80) number of food-items, non-standard mixture distributions (continuous with a mass point at zero for non-consumption), and typical assumptions (conditional independence given the class and timepoint, time-invariant conditional responses, and invariant transition probabilities) may not be realistic. We review the LTM and illustrate a model selection strategy using an example from nutritional epidemiology. We investigate the implications in interpretation of the classic and identifiability assumptions, and provide guidance for potentially problematic situations (small sample size or intermediate item-response probabilities). We estimate LTM using the free SAS procedure LTA, and a software-package for latent models, Mplus.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Applied data analysis,Latent variables,taowang@mcw.edu,,Tao Wang,Ph.D.,Medical College of Wisconsin,Division of Biostatistics,414-456-4339,414-456-6513,taowang@mcw.edu,Contribution of Genetic Effects to Genetic Variance Components with Epistasis and Linkage,1,Tao,,Wang,"Division of Biostatistics, Department of Population Health, Medical College of Wisconsin, Milwaukee, WI 53226",Zhao-Bang,,Zeng,"Bioinformatics Research Center, Department of Statistics, North Carolina State University, Raleigh, NC 27695",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genetic models provide a basis to analyze genetic properties in a study population. For quantitative traits, a popular model that has been widely used in genetic association studies is referred to as the F_infinity model whose parameters are often defined as the additive, dominance and epistasis effects. Another type of models, which has long been used in experimental designed populations for analysis of quantitative trait loci (QTL), are the so-called Fisherian or Cockerham models. The Cockerham model focuses on partition of genotypic variance into additive, dominance and epistatic variances, and its parameters are called the average additive, dominance or epistasis effects. Over years, there has been some confusion about the definition and interpretation of additive, dominance and epistatic effects of QTL, and their relationship to the additive, dominance and epistatic variances. In this study, we explore differences and links between the F_infinity and Cockerham models. We discuss ways of establishing the relationship between the average effects and genetic effects parameters. For a two-locus biallelic QTL model with epistasis and linkage disequilibrium, we present details for partition of genetic variances in terms of the genetic effects by transforming formulas derived from the Cockerham's model based on the average effects. Some practical issues related to using of reduced models instead of full-parameterized models are also addressed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Statistical genetics,Genetic modelstaowang@mcw.edu,,Tao Wang,Ph.D.,Medical College of Wisconsin,Division of Biostatistics,414-456-4339,414-456-6513,taowang@mcw.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,nliu@uab.edu,,Nianjun Liu,Assistant professor,University of Alabama at Birmingham,"1665 University Boulevard, 420A",205-9759190,205-9752541,nliu@uab.edu,Modeling SNP Genotype Data with Informative Missingness in Samples of Unrelated Individuals,1,Nianjun,,Liu,"Department of Biostatistics, University of Alabama at Birmingham, Birmingham, AL",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Even with the advancement of modern technology, data with missinggenotypes are still common in genetic studies. Although somestatistical methods can handle missing data, they usually assume thatgenotypes are missing at random either explicitly or implicitly, thatis, at a given marker, different genotypes and different alleles aremissing with the same probability. This assumption is over-simplifiedand may not hold in practice. In this study, we demonstrate that theviolation of this assumption may lead to serious bias in allelefrequency estimates, and association analysis based on this assumptioncan be biased. To address this limitation in the current methods, wepropose a novel missing data model which can estimate allele frequencyand missing rate without assumption about the missing datadistribution. Analytically, we prove that the allele frequency andmissing probability are identifiable under our model. Empirically,simulation studies illustrate that our proposed model can reduce thebias for allele frequency estimates and association analysis due toincorrect assumption on the missing data mechanism. In addition, weevaluate the impact of departure from Hardy-Weinberg equilibrium onthe model. Lastly, we illustrate the utilities of our method throughits application to HapMap data and another real data. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Missing data,taowang@mcw.edu,,Tao Wang,Ph.D.,Medical College of Wisconsin,Division of Biostatistics,414-456-4339,414-456-6513,taowang@mcw.edu,Contribution of Genetic Effects to Genetic Variance Components with Epistasis and Linkage Disequilibrium,1,Tao,,Wang,"Division of Biostatistics, Department of Population Health, Medical College of Wisconsin, Milwaukee, WI 53226",Zhao-Bang,,Zeng,"Bioinformatics Research Center, Department of Statistics, North Carolina State University, Raleigh, NC 27695",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genetic models provide a basis to analyze genetic properties in a study population. For quantitative traits, a popular model that has been widely used in genetic association studies is referred to as the F_infinity model whose parameters are often defined as the additive, dominance and epistasis effects. Another type of models, which has long been used in experimental designed populations for analysis of quantitative trait loci (QTL), are the so-called Fisherian or Cockerham models. The Cockerham model focuses on partition of genotypic variance into additive, dominance and epistatic variances, and its parameters are called the average additive, dominance or epistasis effects. Over years, there has been some confusion about the definition and interpretation of additive, dominance and epistatic effects of QTL, and their relationship to the additive, dominance and epistatic variances. In this study, we explore differences and links between the F_infinity and Cockerham models. We discuss ways of establishing the relationship between the average effects and genetic effects parameters. Some practical issues related to using of reduced models instead of full-parameterized models are also addressed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Statistical genetics,Genetic modelsyyang@math.niu.edu,,Yarong Yang,,Northern Illinois University,593 Knolls St. W,224-848-1606,,yyang@math.niu.edu,DualKS:defining gene sets with tissue set enrichment analysis,2,Eric,J,Kort,Van Andel Research Institute,Yarong,,Yang,Northern Illinois University,Zhongfa,,Zhang,Van Andel Research Institute,Bin,T,Teh,Van Andel Research Institute,Nader,B,Ebrahimi,NorthernI Illinois University,,,,,,,,,,,,,,,,,,,,,"Gene set enrichment analysis (GSEA) is an analytic approach which simultaneously reduces the dimensionality of microarray data and enables ready inference of the biological meaning of observed geneexpression patterns. Here we invert the GSEA process to identify class-specific gene signatures enabling tissue diagnosis. This can be conceptualized as tissue-set enrichment analysis. Because our approach uses the KS approach both to define class specific signatures and to classify samples using those signatures, we have termed this methodology Dual-KS (DKS).The optimum gene signature identified by the DKS algorithm was smaller than other methods to which it was compared in 5 out of 10 datasets. The estimated error rate of DKS using the optimum gene signature was smaller than the estimated error rate of the random forest method in 4 out of the 10 datasets, and was equivalent in two additional datasets. DualKS outperformed otherbenchmarked algorithms to a similar degree.DKS is an efficient analytic methodology that can identify highly parsimonious gene signaturesuseful for classification in the context of microarray studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,High dimensional data,yuan-wu@uiowa.edu,,Yuan,,University of Iowa,216 hawkeye court,3194006955,,yuan-wu@uiowa.edu,Partially monotone spline estimation with bivariate current status data,1,Yuan,,Wu,University of Iowa ,Ying,,Zhang,University of Iowa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Assuming that the data structure of two failure times is referred as bivariate current status data, a partially monotone spline estimation of the joint distribution is proposed. The consistency properties of the estimation are studied. A bootstrap test for the dependency between the two failure times is implemented. Simulation results are presented. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Multivariate survival,Nonparametric methods,langholz@usc.edu,,Bryan Langholz,,University of Southern California,USC Dept of Preventive Medicine,323-442-1212,,langholz@usc.edu,Multiphase case-control sampling designs,1,Bryan,,Langholz,University of Southern California,Ly,,Thomas,University of Southern California,Rakovski,,Cyril,Chapman University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Multiphase case-control designs can be useful when there is a relatively inexpensive correlate of the variable of interest.  With a focus individually matched case-control studies, a direct link between survey sampling and case-control designs will be described and it is shown that the finite population properties of the conditional logistic likelihood score function are precisely those corresponding survey sampling Lahiri-Midzuno-Sen ratio estimator.  A multiphase nested case-control study of endometrial hyperplasia and endometrial cancer will be described to illustrate the methods. ",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Epidemiologic methods,Survey research data,yarongyang78@yahoo.com,,Yarong Yang,,Northern Illinois University,593 Knolls St. W,2248481606,,yarongyang78@yahoo.com,DualKS: defining gene sets with tissue set enrichment analysis,2,Eric,J,Kort,Van Andel Research Institute,Yarong,,Yang,Northern Illinois University,Zhongfa,,Zhang,Van Andel Research Institute,Bin,,Teh,Van Andel Research Institute,Nader,,Ebrahimi,Northern Illinois University,,,,,,,,,,,,,,,,,,,,,"Gene set enrichment analysis (GSEA) is an analytic approach which simultaneously reduces the dimensionality of microarray data and enables ready inference of the biological meaning of observed gene expression patterns. Here we invert the GSEA process to identify class-specific gene signatures enabling tissue diagnosis. This can be conceptualized as tissue-set enrichment analysis. Because our approach uses the KS approach both to define class specific signatures and to classify samples using those signatures, we have termed this methodology Dual-KS (DKS).The optimum gene signature identified by the DKS algorithm was smaller than other methods to which it was compared in 5 out of 10 datasets. The estimated error rate of DKS using the optimum gene signature was smaller than the estimated error rate of the random forest method in 4 out of the 10 datasets, and was equivalent in two additional datasets. DualKS outperformed other benchmarked algorithms to a similar degree.DKS is an efficient analytic methodology that can identify highly parsimonious gene signaturesuseful for classification in the context of microarray studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Microarray analysis,Nonparametric Classificationjzhang@gwm.sc.edu,,Jiajia Zhang,Assistant Professor,University of South Carolina,Dept of Epidemiology & Biostatistics,803-777-4474,803-777-2524,jzhang@gwm.sc.edu,Accelerated Hazards Mixture Cure Model,1,Jiajia,,Zhang,University of South Carolina,Yingwei,,Peng,Queen's University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a new cure model for survival data with a surviving or cure fraction. The new model is a mixture cure model where the covariate effects on the proportion of cure and the distribution of the failure time of uncured patients are separately modeled. Unlike the existing mixture cure models, the new model allows covariate effects on the failure time distribution of uncured patients to be negligible at time zero and to increase as time goes by. Such a model is particularly useful in some cancer treatments when the treat effect increases gradually from zero, and the existing models usually cannot handle this situation properly. We develop a rank based semiparametric estimation method to obtain the maximum likelihood estimates of the parameters in the model. We compare it with existing models and methods via a simulation study, and apply the model to a breast cancer data set. The numerical studies show that the new model provides a useful addition to the cure model literature.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Cancer applications,SLIPSITZ@PARTNERS.ORG,,Stuart Lipsitz,Professor,Brigham and Women's Hospital,Division of General Internal Medicine,(617)525-8559,(617)732-7072,SLIPSITZ@PARTNERS.ORG,Classical and Bayes Estimation for Additive Hazards Regression Models,1,Stuart,R,Lipsitz,"Brigham and Women's Hospital,  Boston MA,  USA",Debajyoti,,Sinha,"Dept. of Statistics, Florida State University, USA",M. Brent,,McHenry,"Bristol-Myers Squibb , USA",Malay,,Ghosh,"Dept. of Statistics, University Of Florida, USA",,,,,,,,,,,,,,,,,,,,,,,,,"For additive hazards regression models,   when the hazard rates are close to 0 and/or censoring is high, there often exist convergence problems in the Newton-Raphson algorithm since the MLE is on or close to the boundary of the parameter space.   As alternatives, we propose a weighted least squares (WLS) method-of-moments techniques,  as well as a novel empirical Bayesian framework. The integrated likelihood in the empirical Bayesian framework is obtained via integrating the unknown prior of the nonparametric baseline cumulative hazard, and can be maximized using standard statistical software. Unlike the corresponding full Bayes method, our empirical Bayes estimates of regression parameters, survival curves and their corresponding standard errors have easy to compute closed form expressions and require no elicitation of hyperparameters of the prior.  We illustrate the implementation and advantages of our methodology via a reanalysis of a survival dataset and a simulation study using existing statistical software such as SAS.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Bayesian methods,suejane.wang@fda.hhs.gov,,Sue-Jane Wang,,CDER/FDA,10903 New Hampshire Ave.,301-796-0831,,suejane.wang@fda.hhs.gov,Non-inferiority margin in the presence of constancy violation or different patient populations,1,Sue-Jane,,Wang,US FDA,H.M.James,,Hung,FDA,Robert,T,O'Neill,FDA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Fixed margin method and synthesis method have been proposed for establishing the efficacy of an experimental therapy through a non-inferiority analysis of a two-arm active controlled trial.  Either approach can be very sensitive to the validity of the two critical assumptions:  assay sensitivity and constancy of the control effect, although the study objectives may differ.When historical data are available, the current practice in defining the margin generally uses the worst 95% confidence limit, say, of the control effect, and considers preservation of some fraction of the control effect obtained from the historical placebo controlled trials.  In this presentation, we consider a statistical approach that predefines the margin anticipating the presence of constancy assumption violation or different patient populations. The consideration is predicated on anticipated unavoidable differences in trial design due to ethical constraints to performing a placebo controlled trial. In such cases, the implications of a closely matched active controlled trial would call into question, and, it may not be possible to formulate a non-inferiority margin based on the frequently used method described above. ",FALSE,FALSE,,FALSE,FALSE,FALSE,"Please do not schedule on March 18, 2009 (I have informed invited session chair Dr. Craig Borkowf of CDC, who has also informed you, I'd be leaving the conference for another meeting on this date). Thanks.",invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Biopharmaceutical research,jinghe@mail.med.upenn.edu,,Jing He,,University of Pennsylvania,Room 501  Blockley Hall,2155895396,,jinghe@mail.med.upenn.edu,Detecting gene-gene interaction via optimally weighted markers,1,Jing,,He,"Department of Biostatistics and Epidemiology, University of Pennsylvania School of Medicine",Mingyao,,Li,"Department of Biostatistics and Epidemiology, University of Pennsylvania School of Medicine",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Gene-gene interactions play an important role in complex human diseases. The detection of gene-gene interactions has long been a challenge due to its complexity. The standard method that aims at detecting gene-gene interaction via pairwise interactions between genetic markers may be inadequate since it does not model linkage disequilibrium (LD) between markers in the same gene and may lose power under complicated genetic models. To improve power over this simple approach, we propose a gene-based interaction test by combining optimally weighted markers. A unique feature of our method is its ability to incorporate LD information provided by a reference dataset such as the HapMap. We analytically derived the optimal weight for both quantitative and binary traits. Since markers in the same gene are correlated, to reduce the degrees of freedom, we summarized the information and tested the interactions using the principle components of the weighted genotype scores. We then applied our method to both simulated and real datasets to evaluate its performance. The preliminary results show that our method generally has greater power in detecting gene-gene interactions than other methods. We believe our method will provide a useful tool in discovering disease susceptibility genes for complex diseases.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,katherine_giacoletti@merck.com,,Katherine Giacoletti,Biometrician,Merck Research Labs,UG 1CD-38,267-305-7584,267-305-6538,katherine_giacoletti@merck.com,Proportion of Similar Response (PSR) and Receiver Operating Characteristics (ROC) Methodologies in Assessing Correlates of Protection For Vaccine Efficacy,1,Katherine,E.D.,Giacoletti,Merck Research Labs,Joseph,F.,Heyse,Merck Research Labs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A question of interest in many vaccine clinical development programs is whether a level of antibody response can be determined that is considered a 'protective level.' Such a finding has important implications in decisions regarding booster programs, as well as in the interpretation of the persistence or waning of antibody levels in the years following vaccination.  Traditionally, analyses to answer this question have been based on modelling the probability of developing disease as a function of antibody level among vaccinated subjects.  Such methods are often underpowered due to high vaccine efficacy resulting in very few cases to use for the model; furthermore, the models require many assumptions regarding the distribution of antibody responses.  Two non-parametric approaches will be considered as alternative ways of addressing this question.  These methods, PSR and ROC, have advantages over parametric statistical models in terms of interpretability and easy graphical representations and require few, if any, assumptions about the distributional properties of the antibody response data.  An example based on a vaccine clinical trial will be presented, as well as more general applications beyond vaccine development programs.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Applied data analysis,sourish.das@gmail.com,,Sourish Das,Dr,"SAMSI, Duke University",Old Chemistry Building,860-933-7594,,sourish.das@gmail.com,Analysis of Extreme Drinking in Patients with Alcohol Dependence Using Pareto Regression,1,Sourish,,Das,"SAMSI, Duke University",Ofer,,Harel,"Unversity of Connecticut, Statistics",Dipak,K,Dey,"Unversity of Connecticut, Statistics",Jonathan,,Covault,"Unversity of Connecticut Health Center, Psychiatry",Henry,R.,Kranzler,"Unversity of Connecticut Health Center, Psychiatry",,,,,,,,,,,,,,,,,,,,,"We developed a novel Pareto regression model with unknown shape parameter to analyze extreme drinking in patients with Alcohol Dependence (AD). We used a generalized linear models (GLM) framework and a log-link between the shape parameter of the random and systematic components and a Monte Carlo based Bayesian method to implement the analysis. We examined two issues of importance in the study of AD: First, we tested whether a single nucleotide polymorphism within GABRA2 gene, which encodes a subunit of the GABA_A receptor and has been associated to AD, influenced extreme alcohol intake and second, the efficacy of three psychotherapies for alcoholism in treating extreme drinking behavior.  Following 3-month treatment period, during which one of the three psychotherapy treatment, participants were followed up. We also found that women with the high-risk GABRA2 allele had a significantly higher probability of extreme drinking behavior than women with no high-risk allele.  Among men, there was no significant effect of GABRA2 genotype on extreme drinking behavior. We found that women who received cognitive behaviorial therapy had better outcomes than those those receiving either of the other two therapies. Among men, motivational enhancement therapy was the best treatment for the extreme drinking behavior.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Applied data analysis,Bayesian methods,yunbai@umich.edu,,Yun Bai,,"Biostatistics Department, UMICH",2013 Medford Rd Apt H162,7346604566,,yunbai@umich.edu,Two-stage Generalized Method of Moments Estimation with Applications in Spatio-temporal Models,1,Yun,,Bai,"PhD in Biostatistics, University of Michigan",Peter X.K.,, Song,"Professor Biostatistics Department, University of Michigan",Trivellore Raghunathan,,Raghunathan,"Professor Biostatistics Department, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Spatio-temporal process modeling has received increasing attention inrecent statistical research. However, due to the high dimensionalityof the data, likelihood-based approaches for estimation of thespatio-temporal covariance structure are computationally prohibitive.In this paper, we propose a two-stage generalized method of moments(GMM) method to estimate spatio-temporal covariance structures.Estimating equations are formulated separately for spatial andtemporal processes using pair-wise composite likelihood, whichsignificantly reduces the dimensionality. This often results in alarger set of estimating equations than the number of parameters, sowe apply GMM to construct a quadratic inference function to forestimation. The optimal weight matrix is the covariance between thespatial and temporal score functions, which accounts forspatio-temporal inter-correlation and hence improves the efficiency ofparameter estimation. To deal with the issue of numerical instability,a well-known difficulty in the implementation, we propose a two-stageestimation procedure, in a similar spirit to the method of inferencefunctions for margins (IFM). Theoretically, the method will yieldconsistent estimation and the estimation efficiency can be improvedthrough the GMM over the analysis ignoring the spatio-temporalinter-correlation. We conducted simulations of the proposed methodswith various covariance structures to illustrate ideas.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Estimating equations,mas196@pitt.edu,,Abdus Sattar,Graduate Student Researcher,"University of Pittsburgh,",305 S. Fairmount St.,412-383-7614,412-383-8956,mas196@pitt.edu,Analysis of Non-ignorable Missing and Left-Censored Longitudinal Biomarker Data Using Weighted Pseudo Likelihood Method,1,Abdus,,Sattar,University of Pittsburgh,Lisa,,Weissfeld,University of Pittsburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In a longitudinal study of biomarker data collected during a hospital stay, observations may be missing due to administrative reasons, the death of the subject or the subject's discharge from the hospital, resulting in non-ignorable missing data. In addition to non-ignorable missingness, there are left-censoring in biomarker measurements due to the inherent limit of detection and the quantification limit of the bioassays. Standard likelihood- based methods for the analysis of longitudinal data, e.g, mixed model, do not include a mechanism that accounts for the different reasons for missingness and left-censoring. We have proposed to extend the theory of random effects Tobit regression for the left-censored data to weighted random effects Tobit regression using weighted pseudo likelihood theory for the non-ignorable missing and left-censored longitudinal biomarker data. The proposed method is applied to non-ignorable missing and left-censored interleukin-6 (IL-6) biomarker data obtained from the Genetic and Inflammatory Markers of Sepsis (GenIMS) study, a large, multicenter, cohort study. An extensive simulation study was performed to compare the performance of the proposed model with a number of widely used models.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Missing data,cgwang@cog.ufl.edu,,Chenguang Wang,,University of Florida,303 Diamond Village #22,(352) 328-4182,,cgwang@cog.ufl.edu,Identification Strategies for Pattern Mixture Models with Covariates,1,Chenguang,,Wang,"Department of Statistics, University of Florida",Michael,J.,Daniels,"Department of Statistics, University of Florida",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Pattern mixture modeling is a popular approach for handling incomplete longitudinal data.Such models are not identifiable by construction since the distribution of the missing data is not specified given the observed data.  Identifying restrictions, such as complete case missing value (CCMV) constraints or available case missing value (ACMV) constraints etc., are one approach to mixture model identification and have been well discussed in the literature (Little, 1994; Little and Wang, 1996; Molenberghs et al., 2002; Kenward et al., 2003; Daniels and Hogan, 2008). However, identification strategies can be difficult in models with covariates; in particular, baseline covariates with time-invariant coefficients. An alternative identifying restriction based on residuals is proposed and connections between the proposed constraint and the common missing at random (MAR) constraint and conducting sensitivity analysis is explored. We illustrate this approach using data from a recent clinical trial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Missing data,dz2007@gmail.com,,Yichuan Zhao,Associate Professor,Georgia State University,Department of Mathematics & Statistics,4044136446,,dz2007@gmail.com,Empirical likelihood confidence intervals for the ratio and difference of two hazard functions,1,Yichuan,,Zhao,Georgia State University,Meng,,Zhao,Georgia State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In biomedical research and lifetime data analysis, the comparison of two hazard  functions usually plays an important role. In this talk, we consider the standard two-sample framework under right censoring. We construct useful confidence intervals for the ratio and difference of two hazard functions using smoothed empirical likelihood (EL) method. The empirical log-likelihood ratio is derived  and its asymptotic distribution is a chi-squared distribution. Simulation studies show that the  proposed EL confidence intervals have better performance in terms of coverage accuracy  and average length of confidence intervals than the traditional normal approximation method. Finally, our  methods are illustrated with clinical trial data. It is concluded that the proposed EL methods provide better inferential results.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Diagnostic and screening tests,Missing data,dzeng@bios.unc.edu,,Donglin Zeng,Associate Professor,University of North Carolina,Department of Biostatistics,9199667273,,dzeng@bios.unc.edu,Semiparametric Efficient Estimation in Case-Cohort Study,1,Donglin,,Zeng,University of North Carolina,Danyu,,Lin,University of North Carolina,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In case-cohort study, some important but expensive risk covariates for failure timeare only measured in a subsample randomly selected from the full cohort. Although manymethods have been developed to estimate risk effects in the case-cohort design, most of methodsrestrict to considering the proportional hazards model. Furthermore, no estimation issemiparametrically efficient especially when the expensive covariates depend on the other confoundingvariables. In this work, we consider estimating the risk factors in general transformation models.We propose an efficient approach by maximizing a modified likelihoodfunction via the expectation-maximization algorithm. Particularly, the nuisance parameterof conditional densities among covariates is obtained via maximizing a local likelihood function in theM-step. We derive the asymptotic results for the obtained estimators and show that they are consistentand asymptotically efficient. The small-sample performance of the proposed method is illustrated viaextensive numerical studies and applications to real data.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Missing data,hji@jhsph.edu,,Hongkai Ji,Assistant Professor,"Department of Biostatistics, Johns Hopkins Bloombe",615 North Wolfe Street,410-955-3517,,hji@jhsph.edu,A correlation motif based hidden Markov model for pooling information from multiple ChIP-chip experiments,1,Hongkai,,Ji,"Department of BiostatisticsJohns Hopkins Bloomberg School of Public Health",Hao,,Wu,"Department of BiostatisticsJohns Hopkins Bloomberg School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Chromatin immunoprecipitation coupled with genome tiling arrays (ChIP-chip) is a widely used technology to identify transcription factor binding sites in the genome. It is common to obtain noisy data from such experiments due to various reasons such as unoptimized protocols, varying qualities of antibodies produced at different time, etc. Due to the relatively high cost, most ChIP-chip experiments contain only a few replicates. Reliably detecting transcription factor binding sites is challenging when data are noisy and the number of replicates within the experiment is small. However, when the same transcription factor has been studied by multiple labs, it is possible to pool information from multiple data sets to improve the inference for each individual data set. Effective information pooling depends on adequate but parsimonious modeling of the unknown correlation structures among data sets. We propose a correlation motif based hidden Markov model to jointly identify such correlations and transcription factor binding sites.  Both simulations and real data analyses show that the proposed method can substantially increase the sensitivity and specificity of transcription factor binding site detection compared to the approach that analyzes each data set individually.   ",TRUE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Microarray analysis,Statistical Methods in Genome-wide Gene Regulation Studiesyashih@mdanderson.org,,Ya-Chen Tina Shih,Associate Professor,"University of Texas MD Anderson Cancer Center, Dep",PO BOX 301402,713-563-4309,,yashih@mdanderson.org,A Decomposition of Changes in Medical Care Expenditure Distribution in the US Households: Do We Fare Better Twenty Years After?,1,Ya-Chen,T,Shih,"Section of Health Services Research, Department of BiostatisticsUniversity of Texas M.D. Anderson Cancer Center",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Introduction: Medical expenditures have risen drastically in the US over the past two decades, so has the concern about inequalities in the allocation of healthcare resources.  This study examines changes in the distribution of medical expenditures from 1987 to 2006 and identifies possible sources of variations. Methods: The study used the 1987 National Medical Expenditure Survey and the 2006 Medical Expenditure Panel Survey. Both data were nationally representative probability surveys on the financing and utilization of medical services for non-institutionalized individuals in the US. Quantile regression method was combined with Oaxacas decomposition technique to disentangle factors contributing to the changes in the distribution of medical expenditure. Results: Findings suggested that disparities in medical expenditures between households at high and low socioeconomic status increased over time, after controlling for demographic and institutional characteristics. The observed discrepancies were largest at the lower percentiles of the distribution and narrowed at the higher percentiles, suggesting that low utilization of basic care among households at low socioeconomic status may have led to high medical expenditures associated with catastrophic health events.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Health policy applications,jmgt@umich.edu,,Jeremy M G Taylor,Professor,University of Michigan,Department of Biostatistics,734 936 3287,734 763 2215,jmgt@umich.edu,A family of cure models,1,Jeremy,MG,Taylor,"Department of Biostatistics, University of Michigan",Ning,,Smith,"Department of Biostatistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Cure models are a useful approach for failure time data when it isknown that a fraction of the subjects would never experience the eventof interest even if they could be followed for a long time. Twogeneral types of cure models have been developed. A mixture curemodel, developed by Boag, Farewell and others, and bounded cumulativehazard cure model, developed by Yakovlev, Tsodikov, Chen and others.In this paper we present a family of cure models indexed by an extraparameter in which both of these types are special cases. The familyinvolves a Box-Cox transformation of a distribution function, and theextra parameter is the power parameter of the Box-Cox family. Wedemonstrate that large samplesizes will be needed to distinguish between the two type of cure models.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Variable subset selection/model selection,Transformationsn.h.augustin@bath.ac.uk,,Nicole H Augustin,Dr,University of Bath,Department of Mathematical Sciences,00441225318494,,n.h.augustin@bath.ac.uk,Modelling spatio-temporal trends of forest health monitoring data,1,Nicole,H,Augustin,"University of BathMathematical SciencesBath, UK",Monica,,Musio,"Department of Mathematics and Computer ScienceUniversity of CagliariItaly",Klaus,,von Wilpert,"Forest Research Centre Baden-W\'urttembergFreiburgGermany",Edgar,,Kublin,"Forest Research Centre Baden-W\'urttembergFreiburgGermany",Simon,N,Wood,"University of BathMathematical SciencesBath, UK",Martin,,Schumacher,"University Hospital FreiburgUniversity FreiburgFreiburg, Germany",,,,,,,,,,,,,,,,,"Forest health monitoring surveys are in operation in Europe since theearly 1980s due to forest eco-system damage by air pollution. Here wemodel yearly data on  spruce tree defoliation from a monitoring surveycarried out in Baden-W\'urttemberg, Germany since 1983. On anirregular grid defoliation and other site specific variables arerecorded. The temporal trend of defoliation differs between areasbecause of site characteristics and pollution levels, making itnecessary to allow for space-time interaction. We use a generalizedadditive mixed model combined with scale invariant tensor productsmooths of the space-time dimension. In addition to a temporal trenddue to site characteristics and other conditions modelled with thespace-time smooth, we account for random temporal correlation at sitelevel using an auto-regressive moving average process. The resultsshow that since 2003 there is significant evidence for an increasedtrend in defoliation in spruce. The defoliation can mainly beassociated with recent drought years  due to climate change andcumulative effects of pollution.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Tutorial3: genetic and microarray data analysis",presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Forestry/agriculture applications,Spatial/temporal modeling,jschen24@hotmail.com,,Jinsong Chen,,Mr.,902 B South Main St.,540-818-1866,,jschen24@hotmail.com,Generalized Varying Coefficient Single-Index Mixed Model,1,Jinsong,,Chen,"Department of Statistics, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061, USA",Inyoung,,Kim,"Department of Statistics, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061, USA",George,R.,Terrell,"Department of Statistics, Virginia Polytechnic Institute and State University, Blacksburg, VA 24061, USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Generalized linear mixed model (GLMM) is widely used for the analysisof correlated data/clustered data, and time independent functionalform of the predictor in this model is usually assumed. However, thelinear model is not complex enough to capture the underlyingrelationship between the response and its associated covariates. Andalso time-independent functional form may be too restrictive torepresent true fundamental covariate effects. Therefore, in thispaper, we generalize this model to nonparametric  single-index mixed model and also allow this model to have varying coefficients. We callthis model a generalized varying coefficient single-index mixed model(GVSIMM). We propose a penalized likelihood approach to estimatevarying single-index coefficient. Using bootstrapping approach andasymptotic theory, we make a inference for parameters. Simulationstudies are performed to compare our GVSIMM with GLMM. The study ofthe association between daily air pollutants and daily mortality invarious counties of North Carolina is applied to demonstrate theadvantage of our approaches.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,Longitudinal data,sgreven@jhsph.edu,,Sonja Greven,,"Department of Biostatistics, Johns Hopkins Univers",615 North Wolfe Street,410-5026911,,sgreven@jhsph.edu,Spatial Modeling of Air Pollution and Mortality Time Trends in the United States,1,Sonja,,Greven,"Department of Biostatistics, Johns Hopkins University",Francesca,,Dominici,"Department of Biostatistics, Johns Hopkins University",Scott,,Zeger,"Department of Biostatistics, Johns Hopkins University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We are interested in the association between long-term exposure toparticulate matter (PM) and mortality. In cross-sectional comparisonsof mean pollution concentrations and mortality between cities, it isdifficult to fully control for all potential confounding factors. Weinstead compare local trends in PM and mortality, with each locationacting as its own control, thus minimizing confounding effects. Ourdata includes PM time series for seven years in 814 locations in theUS, as well as individual level data on survival from alocation-matched subset of the Medicare cohort. While a survivalanalysis approach reflects that pollution will likely affect longevityrather than overall mortality rates, the size of the data set withover 3 million deaths makes a direct implementation of a survivalmodel impractical. We use an equivalent Poisson regression model,adjusting for location-specific hazard functions changing smoothlywith age. We model potential spatial correlation in the data usingpenalized splines. To fit this complex model to the high-dimensionaldata, we develop a suitable backfitting algortihm.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Epidemiologic methods,mel20@pitt.edu,,Meredith Lotz,,University of Pittsburgh,300 North Dithridge Street #312A,4129776868,,mel20@pitt.edu,Incorporating Rate of Change into Tree Structured Models with Time Varying Covariates,1,Meredith,J.,Lotz,"University of Pittsburgh Department of BiostatisticsPittsburgh, PA 15261",Stewart,J,Anderson,"University of Pittsburgh Department of BiostatisticsPittsburgh, PA 15261",Sati,,Mazumdar,"University of Pittsburgh Department of BiostatisticsPittsburgh, PA 15261",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Tree-structured survival analysis creates prognostic groups which canbe used to predict risk and assist clinicians in making difficulttreatment decisions.  Typically, only baseline covariates are utilizedin tree-structured survival analysis.  However, covariate values areoften measured multiple times after baseline, and often, their rate ofchange in addition to their baseline value may assist in predictingthe event of interest.   We propose a time-dependent tree-structuredsurvival analysis model which can be used to update an individual'srisk based on their changing covariate values.  For each timedependent covariate, a linear model is used to regress the covariateversus time for each individual assuming that the slope is random.  The slopes and the baseline values of the time dependent covariate are then included along with other baseline covariates in a tree-structured survival analysis model.  The result is a model which provides prognostic groups based on not only baseline covariate values but also the rate of change over time of the time varying covariates. An illustrative example of our method is provided.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Survival analysis,Tree-Structured Modelsaeloyan@ncsu.edu,,Ani Eloyan,Graduate Student,North Carolina State University,2022 Gorman Street,919-521-1078,,aeloyan@ncsu.edu,Smooth Density Estimation with Moment Constraints Using Mixture Densities,1,Ani,,Eloyan,Graduate Student,Sujit,K,Ghosh,Professor of Statistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"One of the common issues in statistical analysis is the estimation of a density function based on a sample of observations drawn from an unknown density. There are a number of approaches to solving this problem using both nonparametric and parametric methods. One of thenonparametric methods is the adaptation of P-spline smoothing techniques to estimate a density. The motivation is that any continuous density can be approximated by a mixture of densities with appropriately chosen moments and weights. In many problems (e.g., random effects) we may have specific information about the moments of the density such as restrictions on the mean and variance. A novel method based on EM-algorithm is proposed for estimating the weights of the mixture density under the constraints on the moments of the density. In addition, the proposed method also obtains an estimate of the number of components in the mixture density needed for optimal approximation. The proposed method is compared with the usual Kernel-based density estimation using simulated data and it is shown that the proposed estimate outperforms the Kernel-based method in terms of minimizing the Kullback-Leibler divergence. The proposed method is illustrated by applying it to several real data examples.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Computational methods,Density Estimationnpajewski@ms.soph.uab.edu,,Nicholas M. Pajewski,Postdoctoral Fellow,University of Alabama at Birmingham,1530 3rd Ave S,2059759223,,npajewski@ms.soph.uab.edu,Simultaneous Bayesian Multiple Shrinkage Inference for Genetic Association Studies Allowing for Mode of Inheritance Uncertainty,1,Nicholas,M,Pajewski,"Section on Statistical Genetics, University of Alabama at Birmingham ",Purushottam,W,Laud,"Division of Biostatistics, Medical College of Wisconsin",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A desirable model for use in high-dimensional genetic association studies would simultaneously consider the effect of all genetic markers and covariates. These studies commonly require considering a large number of markers, most of which are unrelated to the phenotype. Moreover, at each marker the model should allow a variety of modes of inheritance, namely, additive, dominant, recessive, or over-dominant effects. Recently, MacLehose and Dunson (2007) described a flexible multiple shrinkage approach to high-dimensional model building via Bayesian nonparametric priors. The use of these priors facilitates data-driven shrinkage to a random number of random prior locations. Adapting such techniques, we develop Bayesian bivariate semi-parametric shrinkage priors that can be used to allow flexible shrinkage towards the various inheritance modes and, within each mode, shrinkage towards a random number of random effect sizes. The proposed method offers improved power over parametric alternatives, while naturally incorporating the uncertainty in the choice of inheritance mode. We illustrate the proposed method on simulated data based on the International HapMap Project for both quantitative traits and case-control designs. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,High dimensional data,buzas@cems.uvm.edu,,Jeff Buzas,,University of Vermont,16 Colchester Ave,802-656-2971,802-656-2552,buzas@cems.uvm.edu,Split-plot designs in serial dilution bioassay using robots,1,Jeff,,Buzas,University of Vermont,Carrie,,Wager,Precision Bioassay,David,,Lansky,Precision Bioassay,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Serial dilution bioassay is routinely used for relative potencyestimation of lot release and in stability studies of biotechnologyproducts.  Typically, samples and dilution order are separatelyassigned (ideally at random) to rows and columns of 96 well plates,resulting in a strip-plot design.  Robots are increasingly used toimplement serial-dilution designs, and robots with individual tipcontrol can implement split-plot designs, i.e. designs where doseorder need not be the same for each sample compound. For a givensplit-plot design, there are many possible paths a robot can take tofill wells.  We show the shortest path is equivalent to the shortestcommon supersequence (SCS) problem, and describe an algorithm forfinding the SCS useful in bioassay applications.  We also describe analgorithm for the reverse process:  We describe how to generatesplit-plot designs that can be filled in nearly the same number ofsteps as strip-plot designs.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,"Biologics, pharmaceuticals, medical devices",Experimental design,matyxf@langate.gsu.edu,,Yixin Fang,Assistant Professor,Georgia State University,"Dept of Math and Stat, GSU",4044136417,,matyxf@langate.gsu.edu,Testing for familial aggregation of functional traits,1,Yixin,,Fang,"Department of Mathematics and Statistics,Georgia State University",Yuanjia,,Wang,"Department of Biostatistics,Columbia University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In genetic epidemiology, the first and foremost task is testing forfamilial aggregation; if no familial aggregation is found, it isunnecessary to conduct further genetic analysis. For functionaltraits, we propose a test statistic, which is actually the leadingfunctional principal component of heritability. The p-value can beobtained by a permutation procedure or a theorem of Johnstone andForrester (2004). The methods are applied to the cholesterol data fromFramingham Heart Study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Clustered data methods,Functional data analysis,xingao@umich.edu,,Xin(Cindy) Gao,Graduate Student Research Assistant,"Department of Biostatistics, University of Michiga","Department of Biostatistics, School of Public Health",7343555968,,xingao@umich.edu,A Markov Compliance Class and Outcome Model for Causal Analysis in the Longitudinal Setting,1,Xin,,Gao,"Department of Biostatistics, School of Public Health, University of Michigan",Michael,R,Elliott,"Department of Biostatistics, School of Public Health, University of MIchiganSurvey Research Center, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a Markov compliance class and outcome model for analyzing longitudinal randomized studies when non-compliance is present.  In any longitudinal studies, subjects are randomized to the treatment or control group only at baseline, but subjects compliance behaviors may vary over time.  The proposed model considers the problem in the potential outcome framework, and provides causal estimates on the effect of the treatment within principal strata, which are a function of the subjects adherence to various possible randomization assignments.  Previous research in this area (Lin, Ten Have, and Elliott 2008) considered the effect of subjects joint compliance behavior on the joint distribution of the longitudinal outcomes, but not the effect of outcomes at time t-1 on the compliance behaviors at time t, which is often of great interest to investigators.  The proposed Markov compliance class and outcome model provides estimates both on the effect of the adherence on the following outcome, and on the effect of the outcome on the following adherence.  The model requires assumptions to be made about the unobservable correlation among a subjects potential outcomes.  We conduct a sensitivity analysis by varying the correlation.  We analyze the longitudinal Suicide CBT Study using the proposed method and estimate the parameters and causal effects using both expectation-maximization (EM) and Markov chain Monte Carlo (MCMC) methodology.   ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Missing data,rlin@schoolph.umass.edu,,Rongheng Lin,Assistant Professor,University of Massachusetts Amherst,"715 N. Pleasant St., Rm 411",413-545-1934,413-545-1645,rlin@schoolph.umass.edu,Statistical analysis of HIV-1 env sequences and their role in selection process of viral variants in MTCT,1,Rongheng,,Lin,University of Massachusetts Amherst,Mohan,,Somasundaran,University of Massachusetts Medical School,Michael,,Kishko,University of Massachusetts Medical School,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Mother-to-child transmission (MTCT) accounts for 16% of new infectionsof HIV-1 virus every year. Recent studies show that in MTCT somestrains of virus preferentially get transmitted.  Although a varietyof viral, host, and obstetric factors can affect the selectivetransmission process, transmission and pathogenesis is potentiallyinfluenced by HIV-1 genetic variation.  HIV-1 env gene encodes thegp160 protein and is believed to be associated with the selectionprocess.  A clear understanding of the diversity of the early viralquasispecies, whether selective viral variants are transmitted, andwhether they change over time within individuals or populations may beobtained by the genetic characterization of viruses from mother-infantpairs.  Characterization of changes in viral sequences in individualsand populations could provide important data about the sensitivity oftransmitted viruses to antibody neutralization or other selectivepressures. In this talk, we'll present our recent study of a data set consistingof 159 virus sequences from 5 mother-child pairs. We'll explore theassociation between specific sequence mutations and transmissibilityfrom mother to child and discuss the statistical challenges presentedby the data set, including partially observed transmissibility,non-perfect alignment of the sequences and high dimensionality, etc.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Genomics,High dimensional data,yanhui@uab.edu,,Yanhui Sun,,UAB,630 Idlewild Circle Apt K,205-239-5435,,yanhui@uab.edu,Estimating Mediation Effects in Survival Analysis with Censored Data,1,Yanhui,,Sun,"Department of BiostatisticsUniversity of Alabama at Birmingham",Chichi,,Aban,"Department of BiostatisticsUniversity of Alabama at Birmingham",Gary,R,Cutter,"Department of BiostatisticsUniversity of Alabama at Birmingham",David,L,Roth,"Department of BiostatisticsUniversity of Alabama at Birmingham",,,,,,,,,,,,,,,,,,,,,,,,,"This study examines the estimates of mediated effect calculated by theproduct of coefficients method using two survival analyses:log-survival and log-hazard time models. Study duration and loss-to-follow-up time are introduced to simulateand control the amount of censored data in the analysis. The mediationeffects assessed by statistical tests varying sample size, studyduration and parameter combinations were examined using Sobel firstorder formula, PRODCLIN method, Goodman unbiased formula and empiricalmethod. Our results show all four methods yielded similar results that all thetests with study duratione5 and sample size >500 and over 80% of thetests with study duration=3 show significant mediation effects. Whenstandard errors fall between 0 and 1, both Goodman and Sobel methodsproduce smaller estimates of standard errors than empirical method inboth procedures. Standard errors were comparable between log-survivaland log-hazard models. When standard errors fall between 0 and 1,standard errors from log-hazard model are about 4 times larger thanlog-survival model.No significant difference was found for testing mediation effectsamong four methods within same study duration and same sample size.The amount of censored data affects results most. Log-survival modelis recommended since it produces more stable standard errors.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Biostatistics Education,Survival analysis,scott.keith@jefferson.edu,,Scott W. Keith,,Thomas Jefferson University,"1015 Chestnut St., Suite M100",215-503-9876,,scott.keith@jefferson.edu,Obesity and mortality analysis by logistic regression modeling in complex survey data using free-knot splines,1,Scott,W,Keith,"Thomas Jefferson University, Department of Pharmacology and Experimental Therapeutics, Division of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,stanley.pounds@stjude.org,,Stanley Pounds,Dr.,Dept. of Biostatistics,St. Jude Children's Research Hospital,901-595-5052,901-544-8843,stanley.pounds@stjude.org,Association Pattern Testing: A Powerful Statistical Tool to Identify Biologically Interesting Genomic Variables,1,Stanley,B,Pounds,"Dept. of BiostatisticsSt. Jude Children's Research HospitalMemphis, TN",Cheng,,Cheng,"Dept. of BiostatisticsSt. Jude Children's Research HospitalMemphis, TN",Xueyuan,,Cao,"Dept. of BiostatisticsSt. Jude Children's Research HospitalMemphis, TN",James,R,Downing,"Dept. of PathologySt. Jude Children's Research HospitalMemphis, TN",Raul,C,Ribeiro,"Dept. of OncologySt. Jude Children's Research HospitalMemphis, TN",Kristine,R,Crews,"Dept. of Pharmaceutical SciencesSt. Jude Children's Research HospitalMemphis, TN",Jatinder,,Lamba,"Dept. of Experimental and Clinical PharmacologyUniversity of MinnesotaMinneapolis, MN",,,,,,,,,,,,,"The association pattern test (APT) is proposed as a general procedure to identify genomic variables that exhibit a specific biologically interesting pattern of association with multiple phenotype variables. Prior biological knowledge is used to specify a pattern of interest. Next, the pattern of interest is used to define a statistic that measures the evidence that a genomic variable exhibits this pattern of association with the phenotypes. Statistical significance is determined via permutation. In contrast to classical multivariate procedures, APT can easily handle different types of phenotype variables (such as categorical and survival-type endpoints). Simulation studies show that APT has greater power to detect the specified biologically interesting pattern than other methods. In an example application, APT is used to identify genes with expression levels that show an interesting pattern of association with two pharmacokinetic endpoints, two pharmacodynamic endpoints, and three clinical endpoints.  A number of biologically interesting genes were identified that would otherwise have been overlooked, including oncogenes, regulators of pharmacologically relevant genes, and cell-cycle genes. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Cancer applications,cook.aj@ghc.org,,Andrea J Cook,Assistant Investigator,Group Health Center for Health Studies,"1730 Minor Avenue, Suite 1600",(206)287-4257,,cook.aj@ghc.org,Spatial cluster detection for weighted outcomes using cumulative geographic residuals,1,Andrea,J,Cook,"Group Health Center for Health StudiesSeattle, WA 98101",Yi,,Li,"Department of Biostatistics, Harvard School of Public Health and the Dana Farber Cancer InstituteBoston, MA  02115",David,,Arterburn,"Group Health Center for Health StudiesSeattle, WA",Ram,C,Tiwari,"Office of Biostatistics, CDR, FDASilver Spring, MD  USA",,,,,,,,,,,,,,,,,,,,,,,,,"Spatial cluster detection is an important methodology to robustly detect spatial clusters of outcomes without making strong model assumptions on the spatial dependence structure. For health outcomes, e.g. body mass index or obesity, the spatial dependence may be difficultto model since its magnitude may be dependent on measures that are difficult to quantify. This talk proposes a robust spatial cluster detection method for point or aggregate data for general outcomes, given the first two moments can be specified, including continuous,binary, and count data. This new method readily incorporates different weighting structures, such as the regional population, to allow the weighting of information on different regions to not be equal, which is key for aggregate data. The proposed method also incorporates the ability for covariate adjustment as there are no previous methods for weighted outcomes with covariate adjustment available. A simulation study is conducted to evaluate the performance of the method. The proposed method is then applied to assess spatial clustering of high Body Mass Index in a HMO population in the Seattle, Washington USA area.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Environmental and ecological applications,jonathan.schildcrout@vanderbilt.edu,,Jonathan Schildcrout,Assistant Professor,Vanderbilt University,1161 21st Ave South,615-343-5432,,jonathan.schildcrout@vanderbilt.edu,"On planning a retrospective, outcome dependent sampling study for longitudinal binary response data",1,Jonathan,S,Schildcrout,"Department of BiostatisticsVanderbilt University",Patrick,J,Heagerty,"Department of BiostatisticsUniversity of Washington",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The wide availability of longitudinal data through ongoing cohortstudies and other resources permits examination of any number of novelhypotheses.  Often, we have all information available to conductanalyses except for a key exposure or confounding variable.  Ifascertainment costs are high, we must be judicious about who issampled, and in such circumstances, outcome dependent sampling designspermit efficient estimation.  In this presentation, we introduce aclass of designs that sample with probability related to whether ornot subject-specific response variability was observed, and we proposemaximum conditional likelihood for estimation and inference.  However,the focus of the discussion will regard study planning.  Estimationefficiency of these designs depends highly on the distribution of thetarget covariate.  We will discuss this dependence and will comparethe efficiency of various sampling strategies as a function of thetarget covariate distribution.  We will also propose monte-carlo basedpower calculations that can be used to examine study feasibility usingall available information prior to exposure ascertainment.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Epidemiologic methods,Longitudinal data,ac103@stat.duke.edu,,AVISHEK CHAKRABORTY,,PhD STUDENT,"214, OLD CHEMISTRY BUILDING",919-672-5890,919-684-8594,ac103@stat.duke.edu,LATENT SPATIAL MODELLING FOR SPECIES ABUNDANCE,1,AVISHEK,,CHAKRABORTY,"PhD STUDENT, DEPARTMENT OF STATISTICAL SCIENCEDUKE UNIVERSITY",ALAN,E,GELFAND,"PROFESSOR,DEPARTMENT OF STATISTICAL SCIENCEDUKE UNIVERSITY",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Modeling abundance pattern for one or more species using environmentalfeatures is of increasing importance in current ecological studies.The Cape Floristic Region (CFR) in South Africa provides a rich classof species data for such modeling. Here we propose a two stageBayesian hierarchical model for explaining the species abundance overthe region. Ordinally categorized abundance figures are given forabout 10000 grid cells along with cell-wise environmental andsoil-type factors. We formulate the empirical abundance pattern as adegraded version of the potential pattern, with the degradation effectcoming from land transformation and classification error. Since mostof the CFR region was sparsely sampled, the potential abundance  is ofinterest from a predictive as well as conservation perspective.An areal level spatial regression model was used for modeling thedependence of species abundance on the environmental factors.Categorical abundance statistics was induced by a continuous latentsurface and a conditionally autoregressive prior (CAR) was specifiedfor the distribution of spatial random effects. Parallelizedcomputation techniques were employed to improve the runtime. Varioustypes of inference such as comparing different parts of region interms of species richness, mutual comparison of  two or more speciesetc. naturally follow from our model.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Latent variables,Environmental and ecological applications,rli2@emory.edu,,Ruosha Li,,Emory University,"3433 North Druid Hills Road, Apt G",404-695-1156,,rli2@emory.edu,Robust Inference for Sparse Clustered Count Data,2,John,J.,Hanfelt,"Department of Biostatistics & Bioinformatics, Rollins School of Public Health,Emory University, 1518 Clifton Road, N.E., Atlanta, Georgia, 30322, U.S.A.",Ruosha,,Li,"Department of Biostatistics & Bioinformatics, Rollins School of Public Health,Emory University, 1518 Clifton Road, N.E., Atlanta, Georgia, 30322, U.S.A.",Yi,,Pan,"Department of Biostatistics & Bioinformatics, Rollins School of Public Health,Emory University, 1518 Clifton Road, N.E., Atlanta, Georgia, 30322, U.S.A.",Pierre,,Payment,"Institute Armand-Frappier, 531 boulevard des Prairies, Laval (Quebec), Canada",,,,,,,,,,,,,,,,,,,,,,,,,"Standard methods for the analysis of cluster-correlated count datafail to yield valid inferences when the study is finely stratified andthe interest is in assessing the intra-cluster correlation structure.We present an approach, based upon exactly adjusting an estimatingfunction for the biasinduced by the fitting of stratum-specific effects, that requiresmodeling only the first two joint moments of the observations and thatyields consistent and asymptotically normal estimators of thecorrelation parameters. The approach is motivated by a study of thehealth effects of the consumption of drinking water, where theoutcomes were the counts of gastrointestinal illness episodes for eachperson within households from a population stratified by smallgeographic area.Key words: Intra-cluster correlation; Nuisance parameters;Overdispersion; Plug-in bias; Profile estimating function; Sparse data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Estimating equations,Categorical data,albertp@mail.nih.gov,,Paul S Albert,,National Cancer Institute,6130 Executive Blvd,301-496-4051,301-402-0560,albertp@mail.nih.gov,On Estimating the Relationship Between Longitudinal Measurements and Time-to-Event Data Using a Simple Two-stage Procedure,1,Paul,S,Albert,"Biometric Research Branch National Cancer Institute ",Joanna,H,Shih,"Biometric Research BranchNational Cancer Institute",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Joint modeling of longitudinal measurements and time-to-event data is an active area of biostatistical research. Many of these methods require complex methodology which cannot easily be implemented with standard software packages. Recently, various authors have proposed a two-stage regression calibration approach which is simpler to implement than a joint modeling approach. In the first stage, the posterior expectation of an individual's random effects from a mixed model is fit without regard to the time-to-event data. In the second stage, the posterior expectation of an individual's random effects from the mixed-model are included as covariates in a Cox model. Although this approach is conceptually appealling, we demonstrate that this regression calibration approach may be biased due to informative dropout. We propose an alternative regression calibration approach which alleviates much of this bias. This new approach is developed for both discrete-time and continuous-time  time-to-event data. Using simulation studies, we demonstrate that, in both cases, this new regression calibration procedue is nearly unbiased.  Thus, this new regression calibration approach provides a simpler alternative to more complex joint modeling. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Joint models for longitudinal and survival data,Longitudinal data,hongzhu@jhsph.edu,,Hong Zhu,PhD. Candidate,"Department of Biostatistics, Johns Hopkins Univers",615 North Wolfe Street,410-502-3357,410-955-0958,hongzhu@jhsph.edu,Nonparametric and semiparametric estimations for bivariate failure time distribution with interval sampling,1,Hong,,Zhu,"Department of Biostatistics, Johns Hopkins Universtiy",Mei-Cheng,,Wang,"Department of Biostatistics, Johns Hopkins Universtiy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In medical follow-up studies, ordered bivariate survival data are frequently encountered when bivariate failure events are used as the outcomes to identify the progression of a disease. In cancer studies interest could be focused on bivariate failure times, for example, time from birth to cancer-onset and time from cancer onset to death. This paper considers a sampling scheme where the first failure event is (cancer-onset) identified within a calendar time interval, the time-origin (birth) can be retrospectively confirmed, and the occurrence of the second event (death) is observed subject to right censoring. To analyze the bivariate failure time data, it is important to recognize the presence of bias arising due to the complex interval sampling. In this paper, nonparametric and semiparametric methods are developed for estimating the bivariate failure time distribution under stationary and semi-stationary conditions. Statistical methods based on a nonparametric model and a semiparametric copula model are developed to address the problems. The research could also be extended to the situation when the bivariate failure events are observed sub ject to both interval and prevalent sampling. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Multivariate survival,tguennel@vcu.edu,,Tobias Guennel,,Virginia Commonwealth University,701 N Allison Street,8048282527,,tguennel@vcu.edu,Contrasting Performance Criteria for Classification Systems Families,1,Tobias,,Guennel,Virginia Commonwealth University,Christine,,Schubert,Virginia Commonwealth University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Comparing the performance of two groups consisting of similarclassiers, so called classication system families, has played an im-portant role in numerous statistical areas. For a two class problem,Receiver Operating Curves (ROC) have been used widely to analyzethe performance of classication system families by calculating thearea under the curve (AUC) as performance criterion. The demandfor methods evaluating the performance of classication system fam-ilies for problems with three or more classes has increased over thelast two decades. This paper contrasts and criticizes available optionsincluding the extension of AUC to the three class problem, Volumeunder The Surface (VUS), and methods for nding optimal points forclassication system families.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Diagnostic and screening tests,Classificationli.luo@uth.tmc.edu,,Li Luo,,University of Texas School of Public Health,1200 Herman Pressler,7135009849,,li.luo@uth.tmc.edu,Genomics of Complex Diseases,1,Li,,Luo,"Human Genetics Center,The University of Texas School of Public Health ",Gang,,Peng,"Theoretic Systems Biology Laboratory, School of Life Science, Fudan University ",Eric,,Boerwinkle,"Human Genetics Center,The University of Texas School of Public Health ",Momiao,,Xiong,"Human Genetics Center,The University of Texas School of Public Health ",,,,,,,,,,,,,,,,,,,,,,,,,"Recent deep-resequencing reveals that there are a large number of rarevariants that play an important role in causing complex diseases. Mosttraditional statistical methods in current genome-wide associationstudies (GWAS) have mainly focused on investigation of common variantsindividually. Due to their rarity, to individually test association ofrare variants with diseases has little power and may not be robust. Inthis report, we propose to collectively analyze multiple rare variantsas a general framework for association studies of rare variants. Wedevelop three novel statistics that employ information about Poissonprocess characterization of occurrence of rare variants along asegment of genome. The number of rare variants each individual carriescan be approximated by a Poisson random variable. The intensity ofPoisson process can be interpreted as the mutation rate. The developedstatistics compare the mutation rates between cases and controls. Tostudy validity and evaluate performance of the proposed statistics, weestimate type 1 error rates by assuming infinite allele models andlarge simulation; we also compare power with the standard chi-squaretest and apply them to real examples. Preliminary results show thatthe proposed statistics have higher power and smaller P-value than thestandard chi-square test.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Genomics,gsyin@mdanderson.org,,Guosheng Yin,,M. D. Anderson Cancer Center,"Department of Biostatistics, Unit 1411",713-563-4277,,gsyin@mdanderson.org,Dose finding by Jointly Modeling Toxicity and Efficacy as Time-to-Event Outcomes,2,Ying,,Yuan,M. D. Anderson Cancer Center,Guosheng,,Yin,M. D. Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In traditional phase I and II clinical trial designs, toxicity and efficacy are often modeled as binary outcomes. This method ignores information on when the outcome event occurs (experiencing toxicity or achieving cure/remission), and also has difficulty accommodating a high accrual rate under which toxicity and efficacy cannot be observed timely, which results in treatment assignment delays. To address these issues, we propose a Bayesian adaptive phase I/II design that jointly models toxicity and efficacy as time-to-event outcomes. At each decision-making time, patients who have not experienced toxicity or efficacy are naturally censored. We apply the marginal cure rate model to explicitlyaccount for patients insusceptible to efficacy due to drugresistance. The correlation between the bivariate time-to-toxicityand -efficacy outcomes is properly adjusted through the Claytonmodel. After screening out the excessively toxic or futile doses, weadaptively assign each new patient to the most appropriate dosebased on the ratio of the areas under the predicted survival curvescorresponding to toxicity and efficacy. We conducted extensivesimulation studies to examine the operating characteristics of theproposed method. Our design selects the target dose with a highprobability and treats most patients at the desirable dose.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Clinical trials,nyi@ms.soph.uab.edu,,Nengjun Yi,Associate Professor,University of Alabama at Birmingham,Department of Biostatistics,205-934-4924,,nyi@ms.soph.uab.edu,High-Resolution QTL Mapping via Simultaneous Analysis of Dense Markers,1,Nengjun,,Yi,"Department of BiostatisticsUniversity of Alabama at BirminghamBirmingham, AL 35294",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Dense sets of polymorphic markers have been widely used in the molecular dissection of complex traits in experimental organisms and human populations. However, strong statistical correlation or linkage disequilibrium (LD) between densely distributed markers makes it difficult to resolve genetic effects into sufficiently small intervals, especially when analyzing one locus at a time. We propose a high-resolution mapping method using hierarchical generalized linear models that simultaneously analyzes all effects, thereby accommodating relationship among markers.  The key to our approach is the use of continuous prior distributions on effects that favor sparseness in the fitted model, enabling us to distinguish causative variants from neighboring correlated noncasative loci. We show that even with a typical inbred F2 cross we can map a causative locus that accounts for 5% of the phenotypic variation to within 1 cM. Our method can handle various continuous or discrete phenotypes, and accommodate covariates and interactions, and is generally applicable to large-scale genetic linkage and association studies in animal, plant and human.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,joannefyffe@hotmail.com,,Joanne Daggy,,Purdue University,402 Quincy Place,317-569-2848,,joannefyffe@hotmail.com,Joint modeling of zero-inflated data using copulas,1,Joanne,K.,Daggy,Purdue University,Bruce,A.,Craig,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Zero-inflated data arise in many real-world problems.  In the healthcare arena, zeroes typically arise for two reasons.  If we consider annual in-patient hospital costs, the value could be a zero because the subject never entered the hospital that year or the value could be a zero because the subject did not incur any costs when in the hospital (i.e., covered by insurance).  In this talk, we will discuss the joint modeling of correlated healthcare costs.  Because of the large proportion of zeroes, one cannot use the multivariate normal even after an appropriate transformation.  We propose using zero-inflated or two-part models to marginally describe each cost and using a copula to correlate the variables.  We compare this approach to other alternatives such as a two-part Gamma with random effects.  A simulation study and real data analysis are presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Multivariate methods,Health services research,zhiwei.zhang@fda.hhs.gov,,Zhiwei Zhang,Mathematical Statistician,Food and Drug Administration,1350 Piccard Dr.,240-276-3139,240-276-3131,zhiwei.zhang@fda.hhs.gov,Reverse Regression in Randomized Clinical Trials,1,Zhiwei,,Zhang,Food and Drug Administration,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In clinical trials, treatment comparisons are often cast in a regression framework that evaluates the dependence of the relevant clinical outcome(s) on treatment assignment and possibly other baseline characteristics. This presentation introduces a reverse regression approach to randomized clinical trials, with focus on the dependence of treatment assignment on the clinical outcome(s) of interest. A reverse regression model is essentially a semiparametric density ratio model for the outcome distributions in the two treatment groups. The resulting inferences can be expected to be more robust than those based on fully parametric models for the outcome distributions and more efficient than nonparametric inferences. In the presence of multiple endpoints, the reverse regression approach leads to a novel procedure for multiplicity adjustment that is readily available in standard logistic regression routines.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Biopharmaceutical research,wang@wald.ucdavis.edu,,Jane-Ling Wang,Professor,"Univ. of California, Davis","Department of Statistics, 1 Shields Ave. Univ. of California",530-752-2361,530-752-7099,wang@wald.ucdavis.edu,Analysis of Longitudinal Data with Informative Dropout Time from an  Extended Hazards Model,3,Yi-Kuan,,Tseng,"National Central University, Taiwan",Meng,,Mao,"Univ. of California, Davis",Jane-Ling,,Wang,"Univ. of California, Davis",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In medical follow-up studies, patients may drop out of the study due to adisease related cause or may die during the study. Such events lead toinformative dropout of the longitudinal measurements. An effective way todeal with this informative dropout is to model the longitudinal processjointly with the dropout process. In this talk, we model the dropoutprocess with a new extended hazards model that includes both the Coxproportional hazards model and the accelerated failure time (AFT) model.We illustrate how to implement the joint modeling approach by maximizing apseudo joint likelihood function where random effects from thelongitudinal process are treated as missing data. A Monte Carlo EMalgorithm is employed to estimate the unknown parameters, including theunknown nonparametric baseline hazard function of the dropout time.Identifiability issues and statistical inference will be discussed. Oneadvantage of the extended hazards model is to facilitate model selection between the Cox and AFT model, for which we propose a nonparametric likelihood ratio test.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Joint models for longitudinal and survival data,Random effects,fuhaoda@gmail.com,,Haoda,Research Scientist,Eli Lilly and Company,2140 Mustang Chase Drive,608-335-5642,,fuhaoda@gmail.com,Bayesian Nonparametric Emax Model,1,Haoda,,Fu,Eli Lilly and Company,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"NDLM (Normal Dynamic Linear Model) has been used widely in Bayesianadaptive design. Our new method improved the NDLM in terms of itsability to control the shape of the curve, estimating Emax, ED50, andobtaining a monotone smoothed dose response curve. This method can belooked at as a monotone nonparametric curve fitting method. Severalexamples on using this new method on adaptive design will provide. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Biopharmaceutical research,janet@statcollab.com,,Janet Wittes,Ph.D.,Statistics Collaborative,"1625 Massachusetts Ave., NW",202-247-9700,202-247-9701,janet@statcollab.com,Non-inferiority  in orphan diseases - can we improve upon existing therapies,1,Janet,,Wittes,Statistics Collaborative,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The prototypical non-inferiority trial deals with common diseases for which ample sample size is available to define a clinically rational non-inferiority margin.  For orphan diseases, especially diseases caused by genetic abnormalities, the population at risk may be very small.  The small population leads to approval of drugs on the basis of a single small (under 50 patients) Phase 3 trial.  If a new clinically attractive therapy becomes available, it must be compared to the approved therapy; however, if the new therapy is anticipated to be no more beneficial than the approved therapy, a non-inferiority margin is necessary.  In such a case, setting the non-inferiority margin becomes very difficult.  This talk addresses the problems setting a non-inferiority margin in the context of orphan diseases with Gaucher disease as a specific example.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Biopharmaceutical research,wangl@stat.sc.edu,,Lianming Wang,Dr.,University of South Carolina,1523 Greene Street,803-777-2834,,wangl@stat.sc.edu,Semiparametric Bayes Multiple Testing:  Applications to Tumor Data,1,Lianming,,Wang,University of South Carolina,David,B,Dunson,Duke University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In National Toxicology Program (NTP)studies, investigators want to assess whether a test agent iscarcinogenic overall and specific to certain tumor types, whileestimating the dose-response profiles. Because there are potentiallycorrelations among the tumors, joint inference is preferred toseparate univariate analysis for each tumor type. In this regard, wepropose a random effect logistic model with a matrix of coefficientsrepresenting log-odds ratios for the adjacent dose groups for tumorsat different sites. We propose appropriate nonparametric priors forthese coefficients to characterize the correlations and to allowborrowing of information across different dose groups and tumortypes. Global and local hypotheses can be easily evaluated bysummarizing the output of a single MCMC chain. Two multiple testingprocedures are applied for testing local hypotheses based on theposterior probabilities of local alternatives. Simulation studiesare conducted and a NTP tumor data set is analyzed illustrating theproposed approach.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Multiple testing,Toxicology/dose-response,Darfiana.Nur@newcastle.edu.au,,Darfiana Nur,Dr,The University of Newcastle Australia,School of Mathematical and Physical Sciences,+61249215547,+61249216898,Darfiana.Nur@newcastle.edu.au,Change-Point Identification in Hidden Markov models for DNA sequence segmentation modeling,1,Darfiana,,Nur,"School of Mathematical and Physical Sciences,University of Newcastle, Australia",Kerrie,L,Mengersen,"School ofMathematical Sciences, Queensland University of Technology,Australia",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many genome sequences display heterogeneity in base composition inthe form of segments with similar structure. Early evidence ofsegmental genomic structure was noticed early on that in thesalivary glands of Drosophila melanogaster whereas theproblem of statistically segmenting DNA sequence has a historyabout four decades. One approach describesDNA sequence structure by a hidden Markov model (HMM). Change-pointdetection is an identification of abrupt changes in the generated parameters ofsequential data. It has proven to be useful in application such as DNAsegmentation modeling.This talk focuses on the various change-point identificationof  a Bayesian hidden Markov model describing homogeneoussegments of DNA sequences. A simulation study will be used to evaluate thechange-points followed by the real-life examples.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Statistical genetics,jane-pendergast@uiowa.edu,,Jane Pendergast,Professor,The University of Iowa,"Biostatistics, C22 General Hospital",319.384.5028,319.384.5018,jane-pendergast@uiowa.edu,NIH Mandate on Sharing Research Data:  The Regulatory Landscape,1,Jane,,Pendergast,The University of Iowa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The mandated sharing of research data generated through NIH support is intended to broaden the scope of investigation of these data -- thus generating more information and expedited translation of findings into useful knowledge, products, and procedures to improve human health.  Yet other federal legislation restricts the sharing of health data, with the goal of protecting the privacy of research participants and keeping their health data secure and confidential.  Biostatisticians/statisticians are often responsible for the creation, documentation, maintenance, and security of such datasets and are caught in the middle of these two mandates - making data available while keeping it protected from others.   Just what are the rules and investigator rights behind these two seemingly contradictory mandates? How do they impact informed consent documents?  Will the benefits of data sharing outweigh the costs?  This presentation will address these issues with a goal of clarifying the regulatory landscape in today's research environment.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Health policy applications,NIH Policyjun.zhu.e@gmail.com,,Jun Zhu,,Department of Statistics,University of Wisconsin,1-608-263-3615,,jun.zhu.e@gmail.com,paper,1,Jun,,Zhu,University of Wisconsin - Madison,Jeff,,Tracey,"Colorado State University",Kevin,,Crooks,"Colorado State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Movement of animals in landscape is an important subject in ecologyand conservation biology, yet many of the models used by ecologists donot account for landscape features and thus may not be conducive tothe analysis of animal movement data.  Here we present new statisticalmodels that feature animal movement in relation to objects in alandscape.  Statistical inference including parameter estimation andmodel assessment is developed.  For illustration, we show results ofsimulated data and a real movement data  set collected on rattlesnakesin San Diego, California.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Nonlinear models,bokai@psu.edu,,Bo Kai,,Penn State University,322 Vairo Blvd Apt C,8144415026,,bokai@psu.edu,Rank Inference for Varying Coefficient Models,2,Lan,,Wang,University of Minnesota,Bo,,Kai,The Pennsylvania State University,Runze,,Li,The Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Wilcoxon rank regression is a well developed technique in classicallinear models, but less so in nonparametric regression models. Varyingcoefficient models have been demonstrated to be very popular models toadd the flexibility without incurring the curse of dimensionality. Thegoal of this work is to extend rank regression into varying coefficient models and develop nonparametric inferences for rank-based method. We have developed new estimation procedures and derived the asymptotic normality results. Our simulation shows that the newlyproposed methods compare favorably with traditional techniques basedon least squares.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Nonparametric methods,Robust Estimationjcheng@biostat.ufl.edu,,Jing CHeng,Assistant Professor,"Division of Biostatistics, University of Florida C","1329 SW 16th Street, Room 5130",352-265-8035,,jcheng@biostat.ufl.edu,Semiparametric Estimation and Inference for Distributional and General Treatment Causal Effects,1,Jing,,Cheng,"Division of Biostatistics, University of Florida College of Medicine",Jing,,Qin,"National Institute of Allergy and Infectious Diseases, NIH",Biao,,Zhang,"Department of Mathematics, University of Toledo",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This talk considersevaluating the treatment effects on the outcome distribution and its general functions in randomized trials with noncompliance. For distributional treatment effects, fully nonparametric and fully parametric approaches have been proposed, where the fully nonparametric approach could be inefficient and the fully parametric approach could be non-robust to the violation of distribution assumptions. In this work, we develop a semiparametric instrumental variable approach by using empirical likelihood methodwith a density ratio model, under which the underlying densities of the latent compliance classes are linked together by exponential tilts, however, the baseline density is left unspecified. Our method can be applied to general outcomes and general functions of outcome distributions, and allows us to predict a subject's latent compliance class based on an observed outcome value in observed assignment and treatment received groups. Asymptotic results for the estimators and likelihood ratio statistic are derived, and finite sample performance is examined by a simulation study. The method is illustrated by an analysis of data from a randomized trial of an encouragement intervention to improve adherence to prescribed depression treatments among depressed elderly patients in primary care practices. ",FALSE,FALSE,,FALSE,FALSE,TRUE,A speaker of our session can only present on Tuesday. ,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Empirical likelihood,tombraun@umich.edu,,Thomas Braun,Associate Professor,University of Michigan,1420 Washington Heights,734-936-9844,,tombraun@umich.edu,Incorporating Patient Heterogeneity in Adaptive Phase I Trial Designs,1,Thomas,M,Braun,University of Michigan School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There is often significant variability among patients enrolled in oncology Phase I trials with regard to the actual number and types of treatments they have received prior to enrolling in a Phase I trial.  As a result, there often exists a non-negligible level of variability in the probability of dose-limiting toxicity (DLT) among patients receiving the same dose of an experimental agent.  However, existing methodology for adaptive Bayesian designs assumes a homogeneous probability of toxicity among subjects receiving the same dose.  We examine the impact of patient heterogeneity, parameterized through frailty models, on the performance of existing Phase I adaptive Bayesian designs and show that in most realistic settings, patient heterogeneity will lead to underestimation of the maximum tolerated dose (MTD).  Such a result increases the likelihood of moving forward to a Phase II trial that examines a safe, yet ineffective dose of the agent.  We present both theoretic and simulation-based evidence and propose modifications to existing methods that account for patient heterogeneity and improve identification of the MTD.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Clinical trials,dahl@stat.tamu.edu,,David B. Dahl,Assistant Professor,Texas A&M University,2113 Rolling Rock Place,979-845-3141,,dahl@stat.tamu.edu,Modeling the Joint Distribution of Pairs of Dihedral Angles for Protein Structure Prediction,1,David,B,Dahl,"Department of StatisticsTexas A&M University",Ryan,,Day,"Department of ChemistryUniversity of the Pacific",Jerry,W,Tsai,"Department of ChemistryUniversity of the Pacific",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There is considerable interest in the prediction of a protein'sstructure ---  particularly its backbone --- from its underlying aminoacid sequence.  A description of a protein's backbone can be describedby the torsion angle pairs of each of the protein's residues.  Mostmethods for torsion angles focus on the bivariate angles for a givenresidue.  In this talk, I will present a method to jointly model theangles at several positions and demonstrate its use for proteinstructure prediction.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Clustered data methods,mjoffe@mail.med.upenn.edu,,Marshall M. Joffe,,University of Pennsylvania,602 Blockley Hall,215 573-7395,,mjoffe@mail.med.upenn.edu,Extended Instrumental Variables Estimation for Overall Effects,1,Marshall,M,Joffe,University of Pennsylvania,Dylan,,Small,University of Pennsylvania,Thomas,,Ten Have,,Steven,,Brunelli,,Harold,I,Feldman,,,,,,,,,,,,,,,,,,,,,,"We consider a method for extending instrumental variables methods inorder to estimate the overall effect of a treatment or exposure. Theapproach is designed for settings in which the instrument influencesboth the treatment of interest and a secondary treatment alsoinfluenced by the primary treatment. We demonstrate that, whileinstrumental variables methods may be used to estimate the jointeffects of the primary and secondary treatments, they cannot bythemselves be used to estimate the overall effect of the primarytreatment. However, instrumental variablesmethods may be used in conjunction with approaches for estimating theeffect of the primary on the secondary treatment to estimate theoverall effect of the primary treatment. We consider extending theproposed methods to deal with confounding of the effect of theinstrument, mediation of the effect of the instrument by othervariables, failure-time outcomes, and time-varying secondarytreatments. We motivate our discussion by considering estimation ofthe overall effect of the type of vascular access among hemodialysispatients.  We also consider application of these methods and variantsto estimation of the joint effects of multiple phenotypes.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Graphical models,nhzhang@umich.edu,,Nanhua Zhang,,University of Michigan,Department of Biostatistics,734-615-9810,,nhzhang@umich.edu,When ICCs go AWRY: A case study from a school-based smoking prevention study in South Africa,2,Ken,,Resnicow,"School of Public HealthUniversity of Michigan109 Observatory (SPH I)Ann Arbor, MI 48109-2029Ann Arbor, MI 48109-2029",Nanhua,,Zhang,"Department of BiostatisticsSchool of Public HealthUniversity of Michigan1420 Washington HeightsAnn Arbor, MI 48109-2029",Roger,D.,Vaughan,"Department of BiostatisticsColumbia UniversityNY, NY",Sasiragha,P.,Reddy,"Medical Research Council of South AfricaCape Town, South Africa",,,,,,,,,,,,,,,,,,,,,,,,,"It is common in public health interventions to randomize and thenintervene with intact social groups, such as schools, churches, orworksites, rather than individuals. In Group Randomized Trials (GRTs),it is important to account for the correlation among the individualsin the same group, which is captured by the intraclass correlationcoefficient (ICC). In designing Group Randomized Trials, ICCs must beaccounted for to ensure adequate statistical power. In a school-basedsmoking prevention trial conducted in South Africa, the observed ICCsare considerably higher than those previously reported; some are ashigh as 0.10, which reduces our sample size by a factor of 16 for somepsychosocial outcomes. In this paper, we investigate the causes forthe high ICCs and use appropriate method to reduce the impact of thelarge ICCs. Reporting these ICCs helps future investigators withsample size calculation if they want to conduct similar trials. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Clustered data methods,Power analysis/sample size,kbroglio@mdanderson.org,,Kristine Broglio,,U.T. M.D. Anderson Cancer Center,PO Box 301402,713-563-4288,713-563-4242,kbroglio@mdanderson.org,Association between Progression-free and Overall Survival in Randomized Clinical Trials,1,Kristine,,Broglio,U.T. M.D. Anderson Cancer Center,Donald,,Berry,U.T. M.D. Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Overall survival (OS) is the gold standard endpoint for new drug development in oncology.  Progression-free survival (PFS) is a more subjective endpoint, but is assessed prior to OS, allowing for perhaps smaller and faster studies.  There is debate over whether PFS can be considered a surrogate marker for OS and whether PFS is a measure of direct clinical benefit. We evaluated the relationship between PFS and OS in three settings including 1) trial design 2) trial analysis and 3) meta-analysis.We simulated survival data for hypothetical clinical trials where patients were randomized to either a standard of care or an experimental treatment arm.  We assumed that median PFS was 6 months for standard of care and 12 months for the experimental regimen.  OS was considered as the sum of PFS and survival subsequent to disease progression (SS).  We calculated 1) the probability of finding a statistically significant difference in OS for varying lengths of SS 2) the probability of finding a statistically significant difference in OS based on the observed p-value for PFS 3) the association between hazard ratios for PFS and OS in a meta-analysis setting.When SS is 12 months, the probability of detecting a significant OS benefit is 33%, 37% and 32% for studies designed to have 80%, 85%, and 90% power to detect the 6 month difference in PFS respectively.  If the p-value for PFS observed in a particular study is highly statistically significant (p = 0.0001), the probability of also estimating a statistically significance OS benefit ranges from 90% if SS is 4 months to 30% when SS is 24 months.  As SS increases, the strength of the association between the estimated treatment effects for PFS and OS will decrease.  Even when there is a true treatment benefit as measured by PFS, the probability of observing a statistically significant benefit as measured by OS depends on the size of the observed PFS benefit and the play of chance, but most importantly, the length of SS.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Survival analysis,kbroglio@mdanderson.org,,Kristine Broglio,Sr. Stat Analyst,UT MD Anderson Cancer Center,1400 Pressler Street,713-563-4288,713-563-4242,kbroglio@mdanderson.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,heping.zhang@yale.edu,,Heping Zhang,Professor,Yale University,60 College Street,203-785-5185,,heping.zhang@yale.edu,Family-based Association Test for Multiple Traits Speaker,1,Heping,,Zhang,Yale University,Ching-Ti,,Liu,Boston University,Xueqin,,Wang,Sun-Yat Sen University,Wensheng,,Zhu,Yale University,,,,,,,,,,,,,,,,,,,,,,,,,"Early family studies of psychiatric disorders began about a century ago, but our understanding for the genetics of mental and behavioral disorders remains limited. One challenge arises from the fact that multiple phenotypes are needed to characterize psychiatric disorders that usually do not occur alone. In fact, comorbidity is a rule other than exception. To address this challenge, I will first demonstrate the usefulness of considering multiple traits in genetic studies of complex disorders.  Then, I will present a non-parametric test to studying the association between multiple traits and a candidate marker. After a brief summary for the theoretical properties of the test, the nominal type I error and power of the proposed test will be compared with existing test through simulation studies. The advantage of the proposed test will also be demonstrated by a study of alcoholism. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Multivariate methods,ychung@psu.edu,,Yeojin Chung,Graduate Student,The Pennsylvania State University,330 Thomas,8144412636,,ychung@psu.edu,A multivariate likelihood-tuned density estimator,1,Yeojin,,Chung,The Pennsylvania State University,Bruce,G,Lindsay,The Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider an improved multivariate nonparametric density estimatorwhich arises from treating the kernel density estimator as an elementof the model that consists of all mixtures of the kernel, continuousor discrete. One can obtain the kernel density estimator withlikelihood-tuning by using the uniform density as the starting valuein an EM algorithm. The second tuning leads to a fitted density withhigher likelihood than the kernel density estimator. In the univariatecase, the two-step likelihood-tuned density estimator reducesasymptotic bias and performs robustly against a type of the truedensity. In addition, starting EM from a normal density centered atthe sample mean, the esitmator is more robust against outliers thanthe estimator using the uniform initial value. We compare theperformance of the new density estimator with other modified densityestimators in higher dimensions. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,Multivariate methods,kenkingsun@hotmail.com,,Wenguang Sun,Assistant Professor of Statistics,NC State University,Department of Statistics,919-513-2445,,kenkingsun@hotmail.com,Simultaneous Testing of Grouped Hypotheses: Finding Needles in Multiple Haystacks,1,Wenguang,,Sun,"Department of Statistics, NC State University",Tony,,Cai,"Department of Statistics, University of Pennsylvania",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In large-scale multiple testing problems, hypotheses are oftencollected from heterogeneous sources and hence form into groups thatexhibit different characteristics. Conventional approaches,including the pooled and separate analyses, fail to efficientlyutilize the external grouping information. We develop a compounddecision theoretic framework for testing grouped hypotheses andintroduce an oracle procedure that minimizes the false non-discoveryrate subject to a constraint on the false discovery rate. It isshown that both the pooled and separate analyses can beuniformly improved by the oracle procedure. We then propose adata-driven procedure that is shown to be asymptotically optimal. Anumerical study shows that our procedures enjoy superior performanceand yield the most accurate results in comparison with both thepooled and separate procedures. The results demonstrate thatexploiting external information of the sample can greatly improvethe efficiency of a testing procedure, and provide additionalinsights on how to optimally combine the simultaneous inferencesmade for multiple groups.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Multiple testing,nelson.jl@ghc.org,,Jennifer C. Nelson,Assistant Investigator,Group Health Center for Health Studies,"Metropolitan Park East, Suite 1600",206-287-2004,206-287-2871,nelson.jl@ghc.org,Extending group sequential methods to observational medical product safety surveillance,1,Jennifer,C,Nelson,"Group Health Center for Health Studies;Department of Biostatistics, University of Washington",Andrea,,Cook,"Group Health Center for Health Studies;Department of Biostatistics, University of Washington",Shanshan,,Zhao,"Group Health Center for Health Studies;Department of Biostatistics, University of Washington",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Conducting large-scale, proactive, and rapid post-marketing medical product safety surveillance is important for detecting rare adverse events potentially not identified in pre-licensure studies. To detect adverse events as early as possible after the introduction of a new product, continuous monitoring methods such as Wald's classical and Kulldorff's maximized sequential probability ratio test (SPRT) have been proposed. Although continuous monitoring is advantageous for rapid detection, such frequent monitoring may not be feasible or desirable in some instances. And while SPRT-based methods can be applied on a less frequent basis, they may not be optimal in terms of statistical power. A more natural methodology for testing on a periodic basis is group sequential interim monitoring. Group sequential methods are well developed and widely used in clinical trials for monitoring drug safety and efficacy. However, their use in observational settings has not been considered. We will discuss the issues that arise when extending group sequential testing methods to observational safety surveillance settings. We will also present results of a simulation study that evaluates the performance of continuous SPRT-based monitoring methods in an observational study setting compared to several standard group sequential designs.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Epidemiologic methods,"Biologics, pharmaceuticals, medical devices",dupuis@bu.edu,,Josee Dupuis,Professor of Biostatistics,Boston University School of Public Health,Boston University Medical Campus,617-638-5880,617-638-6484,dupuis@bu.edu,Statistical methods for gene mapping using high density SNPs in family samples,1,Josée,,Dupuis,"Department of Biostatistics,Boston University School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genome wide association scans have been performed on multiple studies,including some family based cohorts.  While association analysis infamily based samples presents some statistical challenges because ofthe correlated nature of the observations, the advantages of familydesigns in genetic studies greatly outweigh the added analysiscomplexity.  We present statistical approaches to exploit familyfeatures when looking for genetic variants influencing quantitativetraits of interest.  We illustrate our methods with a high densityscan in the Framingham Heart Study cohorts. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Generalized linear models,man.jin@cancer.org,,Man Jin,Assistant Program Director of Statistics,American Cancer Society,"250 Williams Street, NW, Suite 6D",404-329-7965,,man.jin@cancer.org,Choose An Optimal Ridge Parameter in Penalized Principal-Components Based on Heritability,1,Man,,Jin,American Cancer Society,Yuanjia,,Wang,Columbia University,Yixin,,Fang,Georgia State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"To analyze high-dimensional data, a ridge penalized principal-components approach based on heritabilitywas applied to avoid over-fitting.  The optimal regularization parameter can be chosen bycross-validation. Due to computational intensity, a generalizedcross-validation formula was developed in this paper to select theoptimal parameter. From the simulation studies in four settings, the penalized principal-components of heritability analysis hadsubstantially larger coefficients for the traits with genetic effectthan for the traits with no genetic effect, while the non-regularizedanalysis failed to identify the genetic traits. Thus the penalized principal-components approach based on heritabilitycan effectively handle large number of traits with family structure.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Epidemiologic methods,High dimensional data,lunds@iastate.edu,,Steven Lund,Student,Iowa State University,1106 Pinon Dr Unit 1,612-964-5698,,lunds@iastate.edu,Incorporating Gene Effects into Parametric Empirical Bayes Methods for Microarrays,1,Steven,P,Lund,"Department of Statistics, Iowa State University",Dr. Dan,,Nettleton,"Department of Statistics, Iowa State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The log-normal normal (LNN) and gamma-gamma (GG) models (Kendziorski CM, Newton MA, Lan H, et al., (2003), Statistics in Medicine, 22:3899-3914) are two popular parametric empirical Bayes approaches used to identify differentially expressed genes among multiple treatment groups from gene expression profiles.  An assumption of these models is that observed expression levels between treatment groups with different means are independent, even within genes.  If false, this assumption will bias conclusions towards the null hypothesis of equivalent expression.  We introduce a gene effect to form a log-normal normal normal model and show that it identifies differential expression as well as or better than the LNN and GG models through a variety of simulation studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Microarray analysis,Statistical genetics,kbetts@fas.harvard.edu,,Keith Betts,,Harvard School of Public Health,655 Huntington Avenue,617-642-8418,,kbetts@fas.harvard.edu,Prediction and Misclassification in Right Censored Time-to-Event Data,1,Keith,A,Betts,"Department of Biostatistics, Harvard School of Public Health",David,P,Harrington,"Department of Biostatistics, Harvard School of Public Health and Department of Biostatistics and Computational Biology, Dana-Farber Cancer Institute",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A common goal in medical studies with survival data is to stratifypatients according to predicted risk based on their covariate values. Typically, this consists of fitting a proportional hazards regressionmodel and dividing patients by their subject specific estimatedrelative risk.  Using this method, it is difficult to assess themodel's predictive accuracy in an intuitive and interpretable manner. We reframe the problem in terms of prediction error.  We proposemethodology using working survival models to make predictions in termsof failure time intervals.  We propose two measures of predictionerror which are consistently estimated regardless of whether the modelwas correctly specified.  We demonstrate a resampling technique thatapproximates the large sample distribution of the error statistics,and can be used to differentiate between two models on the basis ofprediction error.  We demonstrate our methodology through simulationstudy as well as through a data application.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Survival analysis,Prediction / Classificationhfang@som.umaryland.edu,,Hong-Bin Fang,Assistant Professor,University of Maryland at Baltimore,Greenebaum Cancer Center,410-706-4103,410-706-8548,hfang@som.umaryland.edu,Analysis for Temporal Gene Expressions under Multiple Biological Conditions,1,Hong-Bin,,Fang,"Division of Biostatistics, University of Maryland Greenebaum Cancer Center",Dianliang,,Deng,"Department of Mathematics and Statistics, University of Regina,  Canada",Jiuzhou,,Song,"Department of Animal and Avian Sciences, University of Maryland",Ming,,Tan,"Division of Biostatistics, University of Maryland Greenebaum Cancer Center",,,,,,,,,,,,,,,,,,,,,,,,,"Temporal gene expression data are of particular interest to researchers as it contains rich information in characterization of gene function and have been widely used in biomedical studies and cancer early detection. However, the current temporal gene expressions usually have few measuring time series levels, extracting information and identifying efficient treatment effects without loss temporal information are still in problem. A dense temporal gene expression data in bacteria shows that the gene expression has various patterns under different biological conditions. Instead of analysis of gene expression levels, we consider the relative change-rates of gene in the observation peroid in this paper.  We propose a semi-parametric model to characterize the relative change-rates of genes,  in which individual expression trajectory is modeled as longitudinal data with changeable variance and covariance structure. Then, based on the parameter estimates, a chi-square test is proposed to test the equality of gene expressions. Furthermore, the Mahalanobis distance is used for the classification of genes. The proposed methods are applied to the dataset of 32 genes in  P. aeruginosa expressed in 39 biological conditions. The simulation studies show that our methods are well performance for analysis of temporal gene expressions.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Functional data analysis,Longitudinal data,spyridoula.tsonaka@med.kuleuven.be,,Roula Tsonaka,Dr,Interuniversity Institute for Biostatistics and st,"Kapucijnenvoer 35, Blok D, bus 7001",0031652661088,,spyridoula.tsonaka@med.kuleuven.be,Non-Ignorable Models for Intermittently Missing Categorical Longitudinal Responses,1,Roula,,Tsonaka,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Katholieke Universiteit Leuven, Belgium",Dimitris,,Rizopoulos,"Department of Biostatistics, Erasmus Medical Center, the Netherlands",Geert,,Verbeke,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Katholieke Universiteit Leuven, Belgium",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The statistical literature over the last two decades has recognizedthat an advisable strategy for the analysis of incomplete longitudinalresponses is to perform a sensitivity analysis mainly due to the factthat the observed data do not contain enough information todistinguish between competing models. The majority of the sensitivityanalysis techniques has focused on continuous responses and typicallyinvolves variations to a basic model i.e., changing assumptions aboutthe error distribution, the mean structure, etc. In this work we focuson categorical longitudinal responses that are allowed to be missingintermittently. In particular, we develop a general class of selectionmodels that encompasses the traditional selection models (Little,JASA: 1995, pp. 1112-1121) as well as the shared parameter models(Follmann et al., Biometrics:1995, pp. 151-168). Thus, a broadersensitivity analysis, than usual, can be undertaken since differentassumptions for the missing data mechanism are possible. Furthermoreand in order to avoid the algebraic complexity of marginal models forcategorical responses (Fitzmaurice et al., JRSSA: 2005, pp. 723-735),we consider marginalized mixed effects models based on the work ofHeagerty and Zeger (2000, StatSc, pp. 1-26) in the incomplete datacontext.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Longitudinal data,ekim@bios.unc.edu,,Eunhee Kim,,University of North Carolina,210-5 Conner Drive,9192609029,,ekim@bios.unc.edu,Semiparametric ROC Models with Multiple Biomarkers,1,Eunhee,,Kim,"Department of Biostatistics, University of North Carolina at Chapel Hill",Donglin,,Zeng,"Department of Biostatistics, University of North Carolina at Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In medical diagnostic research, biomarkers are used as the basis for detecting or predicting disease. There has been an increased interest in using the receiver operating characteristic (ROC) curve to assess the accuracy of biomarkers. Even though numerous methods have been developed for a single biomarker, few statistical methods exist to accommodate multiple biomarkers simultaneously. In this paper, we propose a multivariate binormal ROC model to assess multiple biomarkers.  Our model assumes that biomarkers follow multivariate normal distribution after unknown and marker-specific transformations. Random effects are introduced to account for within-subject correlation among biomarkers. Nonparametric maximum likelihood estimation is used for inference and parameter estimators are shown to be asymptotically normal and efficient. Both simulation study and real data application are used to illustrate the proposed method.     ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Biomarkers/surrogate markers,ROC analysis,blg@lilly.com,,Brenda Gaydos,Dr.,Eli Lilly,Lilly Corporate Center,317-277-1982,317-651-9959,blg@lilly.com,Developing an Adaptive Phase 2/3 Design through Trial Simulation,1,Brenda,L,Gaydos,Eli Lilly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For some applications, a seamless design can reduce the time to submission while providing increased information at the time of filing. However, careful consideration is needed prior to selecting a seamless approach and finalizing the design. Comparisons to alternative clinical plans and trial designs should be performed.  The use of trial simulation is essential throughout the development process. Initially, trial simulation is needed to provide a quantitative assessment of risk/benefit prior to deciding on a seamless approach, and after, to finalize design variants.  But the role of trial simulation is not limited to internal decision making, trial documentation and implementation planning. For example, simulation output is needed to clearly communicate trial details to the data monitoring committee, ethical review boards and regulatory reviewers as well as to clinical trialists that will be conducting the study. In this presentation, a case-study will be used to illustrate the statistical challenges in designing and communicating a seamless 2/3 design with emphasis on the trial simulations used to determine the design and evaluate the Type I error rate. The application of trial simulation from internal decision making to regulatory review and implementation will also be described.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Clinical trials,yangh@medimmune.com,,Harry Yang,Senior Director,MedImmune,One MedImmune Way,301-398-4405,,yangh@medimmune.com,Bayesian Experimental Design for Stability Studies,1,Harry,,Yang,MedImmune,Lanju,,Zhang,MedImmune,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recently methods for the selection of stability designs have been proposed.  However there are few publications that directly deal with the optimal designs for shelf life estimation.  This paper addresses such optimal designs from a Bayesian perspective, taking advantage of historical data concerning drug product stability.  Criteria based on Bayesian decision theory are developed to optimize designs for shelf life estimation. The method is developed assuming the stability profile can be characterized by a linear model.  However, it can be readily generalized to the cases of generalized linear and non-linear models.  Simulations are conducted to evaluate the optimality criteria. An example based on the proposed method and real-life data is presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Experimental design,eli@stat.tamu.edu,,Erning Li,,Texas A&M University,"Department of Statistics, TAMU3143",979-862-7556,,eli@stat.tamu.edu,Functional Latent Feature Models for Data with Longitudinal Covariate Processes,1,Erning,,Li,"Department of Statistics, Texas A&M University,College Station, TX 77843-3143, U.S.A.",Yehua,,Li,"Deapartment of Statistics, University of Georgia, Athens, GA 30605, U.S.A.",Nae-Yuh,,Wang,"The Johns Hopkins University School of Medicine, Baltimore, MD 21205, U.S.A.",Naisyin,,Wang,"Department of Statistics, Texas A&M University,College Station, TX 77843-3143, U.S.A.",,,,,,,,,,,,,,,,,,,,,,,,,"We consider a joint model approach to study the association between nonparametric latent features of longitudinal processes and a primary endpoint. Our modeling strategy is closely related to generalized functional linear models (GFLM), but has several marked differences. We argue that the key assumption in the common GFLM approach that the estimation variation in eigenfunctions is negligible is not necessarily true and is purely determined by the nature of data. We propose estimation procedures and supportive theory that allow the investigation regardless of the validity of this assumption. Our approach takes into account the estimation uncertainty embedded in the estimated eigen-system and allows users to have a thorough understanding of where the estimation uncertainty/variation lies so that the choice of a final model and future research plan can be made accordingly. To the best of our knowledge, the theoretical properties we have developed are the first that takes into account the uncertainty of the estimated eigen-components in the resulting parametric estimators and could be adopted by other estimators that use estimated eigenfunctions or eigenvalues. Numerical performances are evaluated in simulations and through a study of the impacts of BMI and SBP readings during adulthood on hypertension status later in life.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Functional data analysis,Longitudinal data,raphael.gottardo@ircm.qc.ca,,Raphael Gottardo,Research Unit Director,Clinical Research Institute of Montreal,"110, avenue des Pins Ouest",514-987-5747,,raphael.gottardo@ircm.qc.ca,Automated gating of flow cytometry data via robust model-based clustering,3,Kenneth,,Lo,University of British Columbia,Ryan,,Brinkman,BC Cancer Agency,Raphael,,Gottardo,Clinical Research Institute of Montreal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The capability of flow cytometry to offer rapid quantification of multidimensional characteristics for millions of cells has made this technology indispensable for health research and medical diagnosis. However, the lack of statistical and bioinformatics tools to parallel recent high-throughput technological advancements has hindered this technology from reaching its full potential. We propose a flexible statistical model-based clustering approach for identifying cell populations in flow cytometry data based on t mixture models with a Box-Cox transformation. This approach generalizes the popular Gaussian mixture models to account for outliers and allow for non-elliptical clusters. We describe an Expectation-Maximization (EM) algorithm to simultaneously handle parameter estimation and transformation selection. Using two publicly available datasets, we demonstrate that our proposed methodology provides enough flexibility and robustness to mimic manual gating results performed by an expert researcher. The proposed clustering methodology is well-adapted to automated analysis of flow cytometry data. It tends to give more reproducible results, and helps reduce the significant subjectivity and human time cost encountered in manual gating analysis.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Genomics,High dimensional data,hw3r@virginia.edu,,Hongkun Wang,,University of Virginia,Department of Public Health Sciences,434-924-8514,,hw3r@virginia.edu,A study on confidence intervals for incremental cost-effectiveness,1,Hongkun,,Wang,University of Virginia,Hongwei,,Zhao,"Texas A&M Health Science Center",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In health policy and economics studies, the incrementalcost-effectiveness ratio (iCER) has long been used to compare theeconomic consequences relative to the health benefits of therapies.Due to the skewed distributions of the costs and ICERs, much researchhas been done on how to obtain confidence intervals of ICERs,using either parametric or nonparametric methods, with or without thepresence of censoring. In this talk, we will examine and compare thefinite sample performance of many approaches via simulationstudies. For the special situation when the health effect of thetreatment is not statistically significant, we will propose a newbootstrapping approach to improve upon the bootstrap percentile  method that is currently available. The most efficient way of constructing confidence intervals will be identified and extended to the censored data case. Finally, a data example from a cardiovascular clinical trial is used to demonstrate the application of these methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Health policy applications,mbaiocch@wharton.upenn.edu,,Michael Baiocchi,Graduate Student,The Wharton School/Penn,3730 Walnut Street,415-517-3892,,mbaiocch@wharton.upenn.edu,A Nonparametric Approach to Instrumental Variables Analysis with Binary Outcomes,1,Michael,,Baiocchi,University of Pennsylvania,Paul,,Rosenbaum,University of Pennsylvania,Dylan,,Small,University of Pennsylvania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"An instrumental variable is a variable that is effectively randomlyassigned and influences the treatment but affects the outcome onlythrough its influence on the treatment.  Instrumental variables areuseful for estimating the causal effect of a treatment in anobservational study in which not all confounders can be measured. When the outcome is binary, a number of parametric models, such as themultivariate probit, have been developed for using instrumentalvariables to estimate the causal effect of a treatment.  Wedemonstrate that these methods are sensitive to the parametricassumptions and develop a nonparametric, permutation inferenceapproach.  Our approach uses matching methods to account for exogenouscovariates.  We apply our approach to estimate the effects ofattending a regional perinatal center versus a local hospital onmortality of prematurely born children using the mothers excessdistance from the regional perinatal center compared to the localhospital as an instrumental variable.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Health policy applications,zhangla@medimmune.com,,Lanju Zhang,,MedImmune,One MedImmune Way,3013985366,3013988366,zhangla@medimmune.com,Response adaptive randomization in non-inferiority trials,1,Lanju,,Zhang,MedImmune LLC,Harry,,Yang,MedImmune LLC,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Response adaptive randomization has been studied extensively butonly focused on superiority trials. The main drive for responseadaptive randomization is ethical consideration, which is ascompelling in non-inferiority clinical trials as in superiorityclinical trials. If the experimental drug is not worse than theactive control, one should not randomize equal number of patientsto the experimental drug arm. The talk focuses on how to deriveoptimal allocation proportions for non-inferiority trials. Powercalculation method of Farrington and Manning for non-inferioritytrials is used in the derivation. The optimal allocationproportion is a solution to an equation, and a closed form is notavailable. Simulation is used to study the properties of theproposed allocation proportion.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Clinical trials,lux@math.ucalgary.ca,,Xuewen Lu,,University of Calgary,2500 University Drive,(403)220-6620,,lux@math.ucalgary.ca,Efficient Estimation in the partly linear additive hazards regression model with current status data,1,Xuewen,,Lu,University of Calgary,Peter,X.-K.,Song,University of Michigan,John,D.,Kalbfleisch,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We study efficient estimation in the partly linear additive hazardsregression model with current status data. We consider the use of polynomial splines to approximate the cumulative baseline hazardfunction with monotone constraints and nonparametric regressionfunctions without constraints. Both regression coefficients and nuisance parameters are estimated simultaneously using the method ofmaximum likelihood estimation. The estimator of the finite-dimensionalvector of regression parameters is shown to be asymptotically normal and achieves the semiparametric information bound.  Rates of convergence for the estimators of the nonparametric components are investigated.  To implement estimation,  we treat the model as a generalized linear model and apply the iterative weighted least squares method. We conduct simulation studies to examine the finite sample performance of the estimators and computational challenges in the proposed methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,High dimensional data,xuegu@mdanerson.org,,Xuemin Gu,,M. D. Anderson Cancer Center,1400 Pressler Street,713-792-1624,,xuegu@mdanerson.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,psimpson@mcw.edu,,pippa margaret  simpson,professor,MCW,CRI - room C3440,4147633558,,psimpson@mcw.edu,The effect of rainfall on visits to Pediatric Emergency rooms for Diarrhea.,1,Shun,H,Li,MCW,Pippa,M,Simpson,MCW,Stephen,,StanHope,MCW,Ke,,Yan,MCW,Marc,,Gorelick,MCW,Bevan,E,Huang,CSIRO,Raymond,G,Hoffmann,MCW,,,,,,,,,,,,,"Modeling the effect of one time series on another where there are innovations and both cyclical and acyclical trends requires an extensive modeling building process to examine the temporal relationships. In a study of the association between gastroenteritis and rainfall in children residing in the Lake Michigan watershed, surface water and well water are regularly contaminated with chemicals and other daily wastes. Rainwater carries additional wastes from the surface into the drinking water. Ancillary measurements include checking for cryptosporidium, giardia and coliform bacteria, but not viral particles. Cracks in the pipelines increase the chance of contamination due to infiltration during rainfall. An example of an innovation was the occurrence of an intense storm where the runoff exceeded the capacity of the normal water filtration plants. Autoregressive integrated moving average (ARIMA) models with innovations were tested and evaluated for their validity, accuracy and reliability. Since the incubation time could be variable, models were considered with and without seasonal considerations and with and without lag differences. The model building procedure involved iterative cycles consisting of three stages: (1) building the ARIMA model, (2) model estimation, and (3) diagnostic checking. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Time series,Epidemiologic methods,zhiguo@umich.edu,,Zhiguo Li,,"Institute for Social Research, University of Michi","439 West Hall, 1085 South University",734-763-2052,,zhiguo@umich.edu,Sample Size Calculation for Two-Stage Randomization Designs with Censored Data,1,Zhiguo,,Li,"Institue for Social Research, University of Michigan",Susan,,Murphy,"Institue for Social Research, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Some clinical trials are implemented via a two-stage approach. In the first stage, an initial treatment is given to induce remission, and in the second stage, a follow-up therapy is given in the intent of prolonging survival. A question for this kind of trials is how to calculate the sample size needed to guarantee a certain power in comparing different treatment policies. Assuming we want to compare two treatment policies, the sample size can be calculated based on several different test statistics. They include a weighted version of the Kaplan-Meier statistic, a weighted version of the Aalen-Nelson statistic, a weighted version of the empirical estimate of probabilities, and finally also a weighted version of the log-rank test statistic. Weights are necesary here because different subjects are consistent with a policy with different probabilities. Sample size formula are obtained under different assumptions. Under some assumptions, less information is elicited from the investigator, but the sample size can be more conservative than that obtained when more information is elicited from the investigator. Simulation studies are carried out to compare all the sample size formula in different scenarios.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Power analysis/sample size,monir.hossain@uth.tmc.edu,,Md Monir Hossain,Assistant Professor,University of Texas Health Science Center at Houst,"6410 Fannin Stree, Suite 1100.22",713 500 7964,713 500 0766,monir.hossain@uth.tmc.edu,Space-time Dirichlet process mixture models for small area disease risk estimation,1,Md,M,Hossain,"Biostatistics/Epidemiology/Research Design (BERD) CoreCenter for Clinical and Translational SciencesThe University of Texas Health Science Center at Houston",Andrew,B,Lawson,Medical University of South Carolina,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a space-time Dirichlet process mixture model. The dependencies for spatial and temporal effects are introduced by using space-time dependent kernel stick-breaking processes. We compared this model with the space-time standard random effect model by checking each models ability in terms of cluster detection of various shapes and sizes. This comparison was made for real and simulated data. For real data, we used twelve years of Georgia throat cancer mortality data. For simulated data, we used Ohio Geographies and twenty-one years of expected lung cancer cases.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Bayesian methods,wye@umich.edu,,Wen Ye,Research Assistant Professor,University of Michigan,Department of Biostatistics,7346159051,,wye@umich.edu,Use of Secondary Data Analysis and Instantaneous States in a Discrete-State Model of Diabetic Heart Disease,3,Jacob,,Barhak,University of Michigan,Deanna,JM,Isaman,,Wen,,Ye,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"With the increasing burden of chronic diseases on the health caresystem, Markov-type models are becoming popular to guide diseasemanagement. Typically, these models use secondary data analysis toestimate transitions in the model; however there are frequentdiscrepancies between the theoretical model and the design of thestudies being used. This paper demonstrates a likelihood approach tocorrectly model the design of clinical studies when 1) the theoreticalmodel may include an instantaneous state of distinct interest, and 2)the secondary data can not be used to estimate a single parameter inthe theoretical model (e.g., a study may ignore intermediary stages ofdisease). Our approach not only accommodates the two conditions above,but allows using more than one study to estimate model parameters andallows model refinement. We name our approach the Lemonade Method inthe spirit of when study data give you lemons, make lemonade. Thismethod is applied to a model of heart disease in diabetes.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,TRUE,Other,Categorical data,disease modelingzhou@stat.ucla.edu,,Qing Zhou,Assistant Professor,"Department of Statistics, UCLA","8125 Math Sciences Bldg, Box 951554",(310)794-7563,,zhou@stat.ucla.edu,Learning gene regulatory network profile across multiple experimental conditions,1,Qing,,Zhou,"Department of Statistics, UCLA",Michael,J,Mason,"Department of Statistics, UCLA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Gene regulatory network changes dynamically across different cellular conditions. When transcription factor binding data (ChIP-chip/seq) and gene expression data are generated in multiple related experimental conditions or developmental stages, inferring a set of related regulatory networks is possible. We propose a flexible statistical model for this inference problem. Under each experimental condition, we assume that the binding of a transcription factor (TF) to a gene is determined by the expression of the TF under the same condition and the enrichment of its binding sites in the regulatory region of the gene. Multiple measures can be incorporated into a unified framework. Advanced learning methods based on ensemble learning or L1 norm penalty are employed to identify regulators in a set of networks. This approach is applied to the reprogramming of mouse embryonic stem cells from fibroblasts.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Computational methods,xingao@umich.edu,,Xin(Cindy) Gao,Graduate Student Research Assistant,"Department of Biostatistics, University of Michiga","Department of Biostatistics, School of Public Health",7343555968,,xingao@umich.edu,A Markov Compliance Class and Outcome Model for Causal Analysis in the Longitudinal Setting,1,Xin,,Gao,"Department of Biostatistics, School of Public Health, University of Michigan",Michael,R,Elliott,"Department of Biostatistics, School of Public Health, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a Markov compliance class and outcome model for analyzinglongitudinal randomized studies when non-compliance is present. Theproposed model considers the problem in the potential outcomeframework, and provides causal estimates on the effect of thetreatment within principal strata, which are a function of thesubjects adherence to various possible randomization assignments.Previous research in this area (Lin, Ten Have, and Elliott 2008)considered the effect of subjects joint compliance behavior on thejoint distribution of the longitudinal outcomes, but not the effect ofoutcomes at time t-1 on the compliance behaviors at time t, which isoften of great interest to investigators. The proposed Markovcompliance class and outcome model provides estimates both on theeffect of the adherence on the following outcome, and on the effect ofthe outcome on the following adherence. The model requires assumptionsto be made about the unobservable correlation among a subjectspotential outcomes. We conduct a sensitivity analysis by varying thecorrelation. We analyze the longitudinal Suicide CBT Study using theproposed method and estimate the parameters and causal effects usingboth expectation-maximization (EM) and Markov chain Monte Carlo (MCMC)methodology.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Missing data,wguan@umich.edu,,Weihua Guan,,University of Michigan,"Department of Biostatistics, School of Public Health",7347635754,,wguan@umich.edu,Estimation of the contribution of rare disease-predisposing variants to complex diseases,1,Weihua,,Guan,"Department of Biostatistics and Center for Statistical Genetics, University of Michigan",Laura,J,Scott,"Department of Biostatistics and Center for Statistical Genetics, University of Michigan",Michael,,Boehnke,"Department of Biostatistics and Center for Statistical Genetics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For many complex diseases, common risk variants identified throughgenome-wide association studies explain only a small proportion ofdisease risk. Rapid advances in next-generation sequencingtechnologies will allow a more complete survey of genetic variants inthe genomic regions of interest, and an opportunity to identify raredisease-predisposing variants. It is often assumed that a candidategene contains a number of such variants to cause noticeable effect onthe disease. One test for the association of the rare variants withdisease status is to compare the total number of variants within agene or a region between cases and controls. When affected familymembers are available, an additional approach is to look forco-transmission of disease with the possible linked rare alleles.Through analytically calculations and computer simulations, we haveestimated the plausible numbers of linked disease-predisposingvariants given the observed signal using a family-based test, assumingall these variants have low allele frequencies but moderate to highdisease penetrance. We compared the power of the family-based test toa case-control approach. Our results will provide help to understandthe importance of the common diseases, rare variants assumption inpractice and its impact on the analyses of re-sequencing data. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,dopark@unr.edu,,Do-Hwan Park,Assistant Professor,University of Nevada-Reno,1664 N. Virginia St,765-586-2018,,dopark@unr.edu,Semiparametric Regression Analysis of Longitudinal Data with Informative Observation Times,2,Jianguo (Tony),,Sun,University of Missouri-Columbia,Do-Hwan Park,,Park,University of Maryland-Baltimore County,Liuquan,,Sun,Chinese Academy of Sciences ,Xingqiu,,Zhao,Hong Kong Polytechnic University,,,,,,,,,,,,,,,,,,,,,,,,,"Statistical analysis of longitudinal data is an important topic facedin a number of applied fields including epidemiology, public health and medicine. In general, the information contained in longitudinal data can be divided into two parts. One is the set of observation times that can be regarded as realizations of an observation process and the other is the set of actually observed values of the response variable of interest that can be seen as realizations of a longitudinal or response process.  For their analysis, a number of methods have been proposed and most of them assume that the two processes are independent.  This greatly simplifies the analysis since one can rely on conditional inference procedures given the observation times. However, the assumption may not be true in some applications. We will considersituations where the assumption does not hold and propose a semiparametric regression model that allows the dependence between the observation and response processes.  Inference procedures are proposed  based on the estimating equation approach and the asymptotic properties of the method are established. The results of simulation studies will be reported and the method is applied to a bladder cancer study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Joint models for longitudinal and survival data,ning.wang@ttu.edu,,Ning Wang,,Math and Statistics Department of Texas Tech Unive,701 N.Ithaca Ave. Apt.3620,8064452055,,ning.wang@ttu.edu,,9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,gatsonis@stat.brown.edu,,Constantine Gatsonis,Professor,"Center for Statistical Sciences, Brown University",Box G-S121-7,4018639183,,gatsonis@stat.brown.edu,Access to data from publicly funded studies: opportunities and unintended consequences,1,Constantine,,Gatsonis,"Center for Statistical SciencesBrown University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recently enacted policies for making data from publicly funded studiesaccessible to other investigators are already having their impact.Data, such as clinical information, imaging studies, and biospecimenanalysis results, are beginning to be made available to investigatorsoutside the original study teams as well as to manufacturers of drugsand devices.  The new policies may well signify a sea change inbiomedical research. On the one hand, they create new possibilitiesfor augmenting the value of studies. On the other, they create a hostof scientific, regulatory, and technical problems. These problemsinclude the risks inherent in third party analyses of data fromcomplex clinical studies, the need to protect the privacy ofparticipants, the lack of a regulatory framework for these datareleases, and the lack of attention to resource implications.  Even ifparticular problems can be addressed, the emphasis on quick and broadaccess to the data by outside investigators is likely to affect theincentives for investigators, with possibly unintended consequences. In this presentation, we will discuss the above issues using theexperience of a collaborative group as an example. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Biopharmaceutical research,lmcclure@uab.edu,,Leslie McClure,PhD,University of Alabama at Birmingham,RPHB 327F,2059345924,,lmcclure@uab.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,eleannes@cisunix.unh.edu,,Eleanne Solorzano,Associate Professor,University of New Hampshire,15 Millstream Drive,603-862-3362,,eleannes@cisunix.unh.edu,Detecting Natural Selection Across Dependent Populations,1,Eleanne,,Solorzano,University of New Hampshire,Hongyu,,Zhao,Yale University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The study of human genes is critical to the understanding ofmanifestations of diseases. Determining which genes may be undernatural selection is an important step in discovering which genes maybe associated with disease. Natural selection is a process that results in the survival andreproductive success of individuals or groups best adjusted to theirenvironment, leading to the perpetuation of genetic qualities bestsuited to that particular environment.  Sabetis (2002) ExtendedHaplotype Homozygosity Test is usually used to detect recent positiveselection of a particular gene across human populations. The standardway to make inferences across the populations is to assumeindependence using either a Bonferroni or FDR approach.  However,human populations are correlated due to the fact that all humansoriginate from one common African ancestor.  Therefore, to reducebias, it is necessary to account for this correlation amongpopulations.  A new statistical method using haplotypes is developedfor detecting natural selection across populations which accounts forsuch correlations.  This test is shown to have higher statisticalpower than the existing methods to make inferences across populationsand also controls the Type I error.  The test is illustrated with anexample using the lactase gene across 42 populations. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Multivariate methods,Statistical genetics,changwon@email.unc.edu,,Changwon Lim,,University of North Carolina at Chapel Hill,"Department of Statistics and Operations Research, UNC",919-413-6974,,changwon@email.unc.edu,Robust Statistical Theory and Methodology for Nonlinear Models with Application to Toxicology,1,Changwon,,Lim,"Department of Statistics and Operations Research, University of North Carolina, Chapel Hill, NC;Biostatistics Branch, NIEHS, NIH, RTP, NC",Pranab,K,Sen,"Department of Statistics and Operations Research, University of North Carolina, Chapel Hill, NC;Department of Biostatistics, University of North Carolina, Chapel Hill, NC",Shyamal,D,Peddada,"Biostatistics Branch, NIEHS, NIH, RTP, NC",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Often toxicologists are interested in investigating the dose-response relationship when animals are exposed to varying doses of a chemical. In some instances a nonlinear regression model such as the Hill model is used to describe the relationship. The standard asymptotic confidence intervals and test procedures based on the ordinary least squares methodology may not be robust to heteroscedasticity and consequently may produce inaccurate coverage probabilities and Type I error rates. On the other hand, the standard weighted least squares based methodology may be computationally intensive and may not be efficient when the variances are approximately equal across dose groups(homoscedasticity). In practice one generally does not know if the data are homoscedastic or heteroscedastic. Also neither method is robust against outliers or influential observations. Since the performance of a method depends on whether the data are homoscedastic or heteroscedastic, we introduce a simple preliminary test estimation (PTE) procedure that uses robust M-estimators. The methodology is illustrated using a data set obtained by the National Toxicology Program (NTP).",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Nonlinear models,Toxicology/dose-response,pxsong@umich.edu,,Peter Song,Professor,Department of Biostatistics,1420 Washington Heights,734 764 9328,,pxsong@umich.edu,SAS MACRO QIF: Transition of Methodology Research to Application,1,Peter,X,Song,"Department of BiostatisticsUM School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Quadratic inference function (QIF) proposed by Qu et al. (2000) is getting increasingly popular because this method is shown to have some desirable properties that the widely used method of generalized estimatingequation (GEE) lacks of.  To pass this new powerful method to thehands of practitioners, we developed a user-friendly SAS macro. In this talk, I will focus on the development of SAS MACRO QIF and its usagetips. I will also give a live demonstration of running this macro for the analysis of real world longitudinal data.  Some comparisons of the QIFto the GEE will be discussed based on the results produced by the SASsoftware package. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Estimating equations,mlava@bu.edu,,Michael LaValley,Professor,Boston University,Department of Biostatistics,617 638 5186,617 638 6484,mlava@bu.edu,Random effects meta-analysis with two-component normal mixtures,1,Michael,P,LaValley,Boston University ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Use of the random effects model has become widespread in meta-analysisto account for heterogeneity of study results. The random effectsmodel allows the variation in study results to come from two sources:1) the within study sample variance, and 2) a between study variationin the (latent) true effect. The between study variation is usuallymodeled using a normal distribution. In this talk, we evaluate the useof two-component mixtures of normal distributions for the betweenstudy variation. This allows greater heterogeneity in the studyresults and more flexibility in modeling the between study variation.In addition, this type of analysis may be useful when there is concernthat the studies included in a meta-analysis come from more than onepopulation. Simulated and real data will be used to evaluate thisapproach to random effects meta-analysis.",FALSE,FALSE,T1: Competing Risks,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Hierarchical models,Random effects,lnatarajan@ucsd.edu,,Loki Natarajan,Associate Professor,University of California San Diego,3855 Health Sciences Drive,858 822 4763,,lnatarajan@ucsd.edu,Dichotomized mismeasured predictors in regression models,1,Loki,,Natarajan,University of California at San Diego,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In epidemiologic studies, interest focuses on estimatingexposure-disease associations. In some cases, a continuous exposure(e.g., intake of dietary fat) may be dichotomized (e.g., fat intake >30% of calories) if threshold levels of the predictor are of primarypublic health interest. Exposure-disease risk estimates will be biasedwhen the exposure is mismeasured. In order to correct for thesebiases,  a validation substudy may be conducted  where the ``true'and imprecise exposures are observed on a small random subsample.Regression models then incorporate validation substudy data, to obtainless biased exposure-disease risk estimates. In this presentation, wewill focus on biases associated with dichotomization of a mismeasuredcontinuous exposure. The amount of bias in relation to mismeasurementin the imprecise continuous predictor, and choice of dichotomizationcut-point will be discussed.  Measurement error correction methodswill be developed for this scenario in the validation substudysetting, and compared to naively using the dichotomized mismeasuredpredictor in exposure-disease models. Properties of the measurementerror correction methods (i.e., bias, mean-squared error) will bepresented. The proposed methods will be appliedto data on blood pressure and dietary intake collected as part of anongoing epidemiologic study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Measurement error,Epidemiologic methods,shihylee@umich.edu,,Alex Tsodikov,,University of Michigan,1420 Washington  Heights,734-615-6416,,shihylee@umich.edu,Mortality Model for Prostate Cancer,1,Shih-Yuan,,Lee,"University of Michigan Department of Biostatistics",Alex,,Tsodikov,University of Michigan Department of Biostatistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Since the introduction of prostate cancer screening using the ProstateSpecific Antigen (PSA), more than thirty percent prostate cancermortality decline was observed. We propose a statistical model toassess and predict the effect of PSA screening on prostate cancermortality in United State. The model contains four major components.Marginal incidence model predicts age at diagnosis of prostate cancer.Stage and grade specific model predicts the probability of beingdiagnosed at a specific stage and grade at cancer incidence. Treatmentmodel describes the probability of receiving a certain treatmentcombination at the time of cancer diagnosis. Survival model calculatesthe survival time from the diagnosis to death after adjusting for leadtime. Treatment effect parameters were obtained from clinical trialsand the model was fitted using Surveillance, Epidemiology and EndResults (SEER) data. Age adjusted observed and predicted prostatecancer mortalities were compared.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Cancer applications,Demography and population studies,yac14@pitt.edu,,Ya-Hsiu Chuang,,University of Pittsburgh,"2232 Wightman Street, Apt. 203",4123279479,,yac14@pitt.edu,Bayesian Model Averaging Approach in Health Effects Studies,1,Ya-Hsiu,,Chuang,"Department of Biostatistics, Graduate School of Public Health, University of Pittsburgh",Sati,,Mazumdar,"Department of Biostatistics, Graduate School of Public Health, University of Pittsburgh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Determining the lagged effects of ambient air levels of a pollutant oncardiac distress is important in health effect studies. Standard modelselection procedures where a set of predictor variables is selectedignore the associated uncertainties and may lead to overestimation ofeffects. Bayesian model averaging approach takes account of modeluncertainty by combining information from all possible models.Zellners g-prior containing a hyperparameter g can account for modeluncertainty and has potential usefulness in this endeavor. We presentresults from a sensitivity analysis for Bayesian model averaging withdifferent calibrated hyperparameter g, viz., Akaike InformationCriterion prior, Bayes Information Criterion prior, and LocalEmpirical Bayes estimate. Data from Allegheny County Air PollutionStudy and the simulated data sets are used.Word count: 121",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Bayesian methods,sjmay@u.washington.edu,,Susanne May,,"Univ of Washington, Seattle","Dept of Biostatistics, Box 357232",206-616-6342,,sjmay@u.washington.edu,Pooled Nucleic Acid Testing to Identify Antiretroviral Treatment Failure during HIV Infection,1,Susanne,,May,University of Washington,Anthony,,Gamst,,Richard,,Haubrich,,Constance,,Benson,,Davey,M,Smith,,,,,,,,,,,,,,,,,,,,,,"Background: Pooling strategies have been used to reduce the costs ofpolymerase chain reaction based screening for acute HIV infection inpopulations where the prevalence of acute infection is low (<1%). Onlylimited research has been done for conditions where the prevalence ofscreening positivity is higher (>1%). Methods and Results: We present data on a variety of poolingstrategies that incorporate the use of PCR-based quantitative measuresto monitor for virologic failure among HIV-infected patients receivingantiretroviral therapy. For a prevalence of virologic failure between1% and 25%, we demonstrate relative efficiency and accuracy of variousstrategies. These results could be used to choose the best strategybased on the requirements of individual laboratory and clinicalsettings, such as required turnaround time of results, andavailability of resources. Conclusions: Virologic monitoring during antiretroviral therapy is notcurrently being performed in many resource constrained settingslargely because of costs. The presented pooling strategies may be usedto make such monitoring feasible and to optimally limit thedevelopment and transmission of HIV drug resistance in resourceconstrained settings. They may also be used to design efficientpooling strategies for other settings where screening involvesquantitative measures.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Applied data analysis,Computational methods,agenevera@gmail.com,,Genevera Allen,,Stanford University,390 Serra Mall,910-545-8187,,agenevera@gmail.com,Transposable Regularized Covariance Models with an Application to Missing Data Imputation,1,Genevera,I,Allen,Stanford University,Robert,,Tibshirani,Stanford University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Missing data is a common concern in high-dimensional problems such asmicroarrays and user-ratings data.  Recent authors have suggested thatthese examples of matrix data are transposable, meaning that the rowsand columns are not independent, and hence it is unclear which areobservations and features.To model transposable data, wepresent a modification of the matrix-variate normal, themean-restricted matrix-variate normal, in which the rows and columns each have a separate mean vector andcovariance matrix.  We extend regularized covariance models, which placean additive penalty on the inverse covariance matrix, to thisdistribution, by placing separate penalties on the covariances of therows and columns.  These so called transposable regularized covariance models allowfor maximum likelihood estimation of the mean and non-singularcovariance matrices.  Using these models, we formulate EM-typealgorithms for missing data imputationin both the multivariate and transposable frameworks.  Exploiting the structure of our transposable models, we presenttechniques enabling use of our models with high-dimensional data andthen give anefficient approximation for imputation.Simulations and results on microarray data and the Netflix data show that these imputationtechniques often outperform existing methods and offer a greater degree offlexibility.  ",TRUE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Missing data,High dimensional data,anb61@pitt.edu,,Andriy Bandos,Research Assistant Professor,"Department of Biostatistics, Univeristy of Pittsbu",A431 Crabtree Hall,412 383 57 38,,anb61@pitt.edu,COMPARISON OF TWO BINARY DIAGNOSTIC TESTS: CIRCUMVENTING AN ROC STUDY,1,Andriy,I,Bandos,"Department of BiostatisticsGraduate School of Public HealthUniversity of Pittsburgh",Howard,E,Rockette,"Department of BiostatisticsGraduate School of Public HealthUniversity of Pittsburgh",David,,Gur,"Department of RadiologySchool of MedicineUniversity of Pittsburgh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Accuracy of a binary diagnostic test is conventionally characterized by Sensitivity and Specificity.  A binary test that is better in both operating characteristics is naturally superior. However, when one of the two binary systems is better only in Sensitivity the comparison is not as straightforward. One approach to compare such binary tests is based on expected utilities. This method often requires knowledge of a difficult-to-deduce utility function. Another approach is a conventional ROC analysis. This technique is associated with an additional burden of conducting an ROC study which in some cases may cast doubts on validity and usefulness of the resulting ROC curves. Exploiting an often-satisfied convexity property of ROC curves we describe the conditions under which two diagnostic tests can be compared circumventing utility functions or an ROC study. Under the first set of conditions one binary system augmented by a suitably biased random guess is superior regardless of the utility structure. These conditions also imply that the two latent ROC curves are different. The satisfaction of the second set of conditions further strengthens the conclusions by comparing the area under the latent ROC curves.  We describe statistical tests for each set of conditions and present simulation results.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,ROC analysis,Diagnostic and screening tests,sharon@hcp.med.harvard.edu,,Sharon-Lise Normand,Professor of Health Care Policy (Biostatistics),Harvard Medical School,Dept. of Health Care Policy,617-432-3260,617-432-2563,sharon@hcp.med.harvard.edu,Statistical Strategies for PostMarket Surveillance of Medical Devices,1,Sharon-Lise,T,Normand,"Department of Health Care Policy180 Longwood AvenueHarvard Medical SchoolBoston, MA 02115",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A key problem faced by policy makers is how to assess evidence aboutthe effectiveness and safety of new therapies after they have beenapproved.  Because the evidence for approval is often based on theresults of small controlled clinical trials, (1) the patientpopulation and the provider population in the real world can differdramatically from the trial populations and (2) adverse events aredifficult to detect. Approaches to inference on the basis ofobservational data, focusing on the role of the treatment assignmentmechanism, are discussed. Methods are illustrated to examine thesafety and effectiveness of drug-eluting coronary stenting compared tobare metal stenting. Issues related to comparison groups andsensitivity to unmeasured confounders are discussed and  demonstrated.",FALSE,FALSE,,FALSE,FALSE,FALSE,I am president elect for 2009 and am not sure which meetings I need to attend.,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Health services research,"Biologics, pharmaceuticals, medical devices",rhonda.vandyke@cchmc.org,,Rhonda VanDyke,Faculty Biostatisican,Cincinnati Children's Hospital,3333 Burnet Ave,513-803-0563,513-636-7509,rhonda.vandyke@cchmc.org,Classification of Self-Modeling Regressions with Unknown Shape Functions,1,Rhonda,D,VanDyke,"Cincinnati Children's Hospital and Department of Pediatrics, University of Cincinnati",Kert,,Viele,"Department of Statistics, University of Kentucky",Robin,L,Cooper,"Department of Biology, University of Kentucky",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Studies can present distinct, yet related functions. In certain settings, self-modeling regressions can be used to provide overall and smoothed individual estimates by combining information across functions. A set of self-modeling functions is defined by the entire set of functions being related through affine transformations of the x- and y-axes to a common function g(t). We expand this definition to include the possibility that a set of functions contains two underlying sets of self-modeling functions, the first related through a common shape g1(t) and the second related through a separate shape function g2(t). We propose a method to take data consisting of a set of functions, estimate the two underlying shape functions, and to classify each function as belonging to either the first or second group of self-modeling functions. We estimate the underlying shape functions through Bayesian Adaptive Regression Splines (BARS). We illustrate the methodology through Synaptic Transmission data, where the functions measure electrical current across time and the two self-modeling groups of functions are hypothesized to result from different vesicles within synapses releasing transmitter in qualitatively different manners. The method is further assessed through simulations.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Applied data analysis,lian_liu@merck.com,,Lian Liu,,"Merck & Co., Inc.",351 N Sumneytown Pike,267-305-1270,,lian_liu@merck.com,A comparative simulation study between ETRANK® methodology and constraint longitudinal data analysis (cLDA),1,Lian,,Liu,"Merck & Co., Inc.",Richard,,Entsuah,"Merck & Co., Inc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ETRANK® method uses a unique nonparametric (randomization) technique to address treatment related withdrawals (informative censoring) in longitudinal clinical trials. ETRANK® uses data-dependent scoring schemes and a unified test statistic which incorporates all available data, and adjusts for withdrawal patterns, proportion of withdrawals, and level of response prior to withdrawal. An empirical significance level for each scoring system under both the Fulldata and Endpoint methods for a specified parameter configuration is computed using this methodology. A simulation study under  clinical trial missing data mechanisms is performed to compare the power and type I error of the ETRANK® methodology with the constraint longitudinal data analysis (cLDA) method which uses restriction of same baseline mean across treatment groups. ",FALSE,FALSE,,FALSE,FALSE,TRUE,"last day. (If possible, please don't schedule my session on last day, thanks!)",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Nonparametric methods,Clinical trials,jqfan@princeton.edu,,Jianqing Fan,Professor,Princeton University,Department of Operations Res and Fin Eng.,609-258-7924,609-258-8551,jqfan@princeton.edu,Ultrahigh dimensional variable selection:  beyond the linear model,1,Jianqing,,Fan,Princeton University,Richard,,Samworth,University of Cambridge,Yichao,,Wu,North Carolina State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Variable selection in ultrahigh-dimensional space characterizesmany contemporary problems in scientific discovery and decisionmaking.  A frequently-used technique is the independent screeningsuch as the correlation ranking (Fan and Lv, 2008) or feature selectionusing a two-sample $t$-test in high-dimensional classification.Within the context of the linear model, Fan and Lv (2008)showed that this simple correlation ranking possesses a sureindependence screening property under certain conditions and thatits revision, called iteratively sure independent screening (ISIS),is needed when the features are marginally unrelated but jointlyrelated to the response variable. In this paper, we extend ISIS,without explicit definition of residuals, to a generalpseudo-likelihood framework, which includes generalized linearmodels as a special case. Even in the least-squares setting, the newmethod improves ISIS by allowing variable deletion in the iterativeprocess.  Our technique allows us to select important features inultrahigh-dimensional classification where the popularly used two-sample$t$-method fails.  A new technique is introduced to reduce the falsediscovery rate in the feature screening stage.  Several simulatedand a real data example are presented to illustrate the methodology.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Machine learning,Data mining/massive data sets,patrick.brown@utoronto.ca,,Patrick Brown,Scientist,Cancer Care Ontario,620 University Avenue,416 971 9800 x 3439,,patrick.brown@utoronto.ca,Longitudinal Spatial Point Processes for Residential Histories,1,Patrick,E,Brown,Cancer Care Ontario,Peter,,Henrys,Lancaster University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"An individual's location at time of diagnosis can be usefulinformation for an epidemiological study, as clustering of caselocations together relative to the population's distribution couldindicate an environmental spatially-structured risk factor.  However,individuals could have changed residences between exposure anddiagnosis, or exposure could be cumulative over years spent indifferent residences.  In these cases, it is clustering of residentialhistories rather than of single locations which should be examined.This talk describes a longitudinal spatial point process for modellingresidential histories.  Statistical properties of this process arederived  and measure of clustering based on the K function ispresented.  A simulation study shows the ability of this methodologyto detect different types of spatial dependence, and the method is areal dataset of residential histories related to lung cancer cases inthe city of Winnipeg.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Longitudinal data,menggang.yu@gmail.com,,Menggang Yu,Assistant Professor,Indiana University,"Indiana University, School of Medicine",317-278-5471,,menggang.yu@gmail.com,Individualized Prediction in Prostate Cancer Studies Using a Joint Longitudinal-Survival-Cure Model,1,Menggang,,Yu,"Indiana University, School of Medicine",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For monitoring patients treated for prostate cancer, Prostate SpecificAntigen (PSA) is measured periodically after they receive treatment.Increases in PSA are suggestive of recurrence of the cancer and areused in making decisions about possible new treatments. The data fromstudies of such patients typically consist of longitudinal PSAmeasurements, censored event times and baseline covariates. Methodsfor the combined analysis of both longitudinal and survival data havebeen developed in recent years, with the main emphasis being onmodeling and estimation. We analyze a training data set from prostatecancer study in which the patients are treated with radiation therapyusing a joint model that has been extended by adding a mixturestructure to the survival model component of the model. Here we focuson utilizing the model to make individualized prediction of diseaseprogression for censored and alive patients. Results are illustratedon a validation data set.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Joint models for longitudinal and survival data,Cancer applications,wenyaw.chan@uth.tmc.edu,,Wenyaw Chan,Professor,"Division of Biostatistics, University of Texas -Ho","1200 Herman Pressler, suite W942",1-713-500-9321,1-713-500--9530,wenyaw.chan@uth.tmc.edu,Analysis of Variance on a Categorized Continuous Variable,1,Wenyaw,,Chan,"Division of Biostatistics, School of Public Health, University of Texas-Health Science Center at Houston",Lin-An,,Chen,"Institute of Statistics, National Chiao Tung University, Hsin Chu, Taiwan",Younghun,,Han,"Division of Epidemeology, University of Texas- M. D. Anderson Cancer Center, Houston, Texas 77030",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"It is not uncommon in epidemiological studies that observations from an independent continuous variable are categorized and the analysis of variance (ANOVA) method is applied to compare the means of the response variable on these categories. For example, among articles published in American Journal of Epidemiology from 2005 to October of 2007, 8 of them used this method.For this method, the statistical appropriateness has never been rigorously examined. We develop the model for this problem under the assumption that response variable and categorized variable follow a bivariate normal distribution. We found that the classical assumptions of normality and constant variance are violated. However, under the null hypothesis of equal means, the classical ANOVA technique is still valid. When the null hypothesis is rejected, the conditional means are monotone. The analytical results were verified by the simulation. These results may help the researchers in making inferences on relationship between the response variable and the categorized independent variable.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Epidemiologic methods,Multivariate methods,nysiainet@gmail.com,,Nysia I. George,,Food and Drug Administration,"3900 NCTR Rd, HFT-20",8705437047,,nysiainet@gmail.com,Optimal Shrinkage Variance Estimation and Outlier Detection in Microarray Data Analysis,1,Nysia,I,George,Food and Drug Administration,Naisyin,,Wang,Texas A&M University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A core goal of microarray analysis is to identify an informativesubset of differentially expressed genes under different experimentalconditions. Typically, this is done through hypothesis testing, whichrelies on test statistics that properly summarize and evaluateinformation in the sample(s). A reliable variance estimator that isapplicable to all genes is important for analysis. We often find thatgenome-scale microarray expression analysis generates large data setswith a small number of replicates for each gene. The widespreadstatistical limitations due to low replication make it necessary todevise adaptive methods for estimating gene-specific variance. Furthercomplicating variance estimation is the frequent presence of outliersin microarray data. We propose a robust modification of optimalshrinkage variance estimation. Our estimator is uninfluenced byoutliers and allows for gene-specific, rather than pooled, estimatesof variance. In order to increase power, we estimate a common varianceterm by grouping standardized data so that information shared by genespost-standardization can be more efficiently utilized. For outlierdetection we adopt a technique which is based on the false discoveryrate approach. Numerous methodologies for estimating variance arecompared via simulation and real data analysis of colon cancermicroarray data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Microarray analysis,High dimensional data,Not sure what to write here.miok.kim@cchmc.org,,Mi-Ok Kim,Assisant Professor,Cincinnati Children's Hospital Medical Center,MLC 5041,513-636-1895,513-636-7509,miok.kim@cchmc.org,Generalized t-test for censored data,1,Mi-Ok,,Kim,Cincinnati Children's Hospital Medical Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider the problem of testing that two populations are identical with respect to the distribution of a continuous variable subject to random right censoring against the alternative that values tend to be larger in one population. In practice the alternative is usually formulated by the proportional hazards model and log-rank test is standard. In application, however, data often suggests departure from the proportional hazards model, in which case log-rank test loses its optimality and weighted versions of log-rank test have been proposed (see Harrington and Fleming 1982; Peto and Peto 1972; Prentice and Marek 1979). However, the selection of the weights is problematic as the power depends on where and how the two hazard curves differ, while such information is usually unknown. OBrien (1988) formulated this problem as heterogeneity in the difference of log-rank scores and proposed an extension of log-rank test where group membership is regressed against the log-rank scores using a quadratic model. We consider two extensions: firstly, we note that the power of the test is limited by the use of second degree polynomials and propose an extension with splines. Secondly we consider an extension that allows covariates and different censoring distributions. ",TRUE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Multivariate survival,lekyla.whitaker@fda.hhs.gov,,Ram Tiwari,"Associate Director, Office of Biostatistics",U. S. Food and Drug Administration,10903 New Hampshire Ave,301-796-1700,301-796-9888,lekyla.whitaker@fda.hhs.gov,Prediction of U.S. Mortality Counts Using Semiparametric Bayesian Techniques,1,Ram,,Tiwari,"Office of Biostatistics, Food and Drug Administration",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Accurate prediction of cancer mortality figures for the current and upcoming year are extremely essential for public health planning and evaluation. Due to delay in reporting cause-specific mortality for the US, there is a 3-year lag between the latest year for which such figures are available and the current year. Prior to 2004, the American cancer Society (ACS) used to predict cancer mortality counts by first fitting a time series model with quadratic trend and autoregressive error to the past data and then projecting this model into the future. Beginning 2004 the ACS has begun to implement a new methodology in its annual publication Cancer facts & Figures, 2004. This method, known as the state-space method (SSM), uses a quadratic trend with random time-varying coefficients to model the mortality counts. In this talk, Bayesian versions of the SSM are presented. In particular, we present two models for short-term prediction of the number of deaths that arise from common cancers in the United States. The first is a local linear model, in which the slope of the segment joining the number of deaths for two consecutive time periods is assumed to be random with a nonparametric distribution, which has a Dirichlet process prior. For slightly, longer prediction periods, we present a local quadratic model. The hierarchical and nested Dirichlet models are also considered. Bayesian models are compared with both the SSM and the previous ACS method.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Health services research,hfeng12@its.jnj.com,,Huaibao Feng,,Johnson & Johnson PRD,920 US 202 South,908 927 7666,,hfeng12@its.jnj.com,Adaptive Group Sequential Design in Clinical Trials with  Changing Patient Populations,1,Huaibao,,Feng,"Clinical Biostatistics, Johnson & Johnson, Raritan, NJ",Qing,,Liu,"Statistical Science, Johnson & Johnson, Raritan, NJ",Jun,,Shao,"Department of Statistics, University of Wisconsin-Madison,Madison, WI, 53706",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Standard group sequential test assumes that the treatment effects arehomogeneous over time. In practice, however, the assumption may beviolated. Often, this occurs when treatment effects are heterogeneousin patients with different prognostic groups, which are not evenlydistributed over the time course of the group sequential trial. Inthis talk, we consider a setting where the inclusion/exclusioncriteria for patient entry are relaxed at interim analyses. Thistriggers heterogeneous treatment effects over the enlarged patientpopulation.  In particular, we assume that the population changerelates to some baseline covariates. We propose a set of linearregression models. With these models, we make inference on the targetpopulation based on additional data from the changed populations. Theeffect of the changing patient population on the available informationis studied and the group sequential boundary values are adjustedaccordingly in order to control overall type I error. We propose tocarry out the group sequential procedure with these revised boundaryvalues. Simulation results show that the type I error probability ofthis procedure is close to the desired level when the patientpopulations are changing across stages. Results on the power of ourproposed group sequential design are also presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Clinical trials,samurphy@umich.edu,,Susan Murphy,Prof. of Statistics,University of Michigan,"439 West Hall, 1085 S. Univ.",734-647-3684,734-647-3684,samurphy@umich.edu,"STAR*D, Dynamic Treatment Regimes and Missing Data",4,Dan,,Lizotte,University of Michigan,Lacey,,Gunter,University of Michigan,Eric,,Laber,University of Michigan,Susan,,Murphy,,,,,,,,,,,,,,,,,,,,,,,,,,"We illustrate the combined use of Q-Learning and multiple imputationfor use in constructing dynamic treatment regimes using the  clinicaltrial, STAR*D (Sequenced Treatment Alternatives to RelieveDepression).  In this trial each individual was rerandomized to newtreatments each time the individual failed to respond to treatment. Q-Learning is a generalization of regression for use in constructingdynamic; this method is not a likelihood based method. Missing dataoccurs in a variety of ways in the STAR*D data set including missingdata due to study dropout and missing data due to missed clinicalvisits and missing data due to missed clinical assessments.  Wediscuss open questions that arise in applying Q-Learning to multiplyimputed data sets. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Applied data analysis,Missing data,ananda@math.asu.edu,,Anandamayee Majumdar,Assistant Professor,Arizona State University,"Tempe, AZ 85287-1804",480 965 3488,,ananda@math.asu.edu,Bayesian Modeling for Nonstationary Multivariate Spatial Processes,1,Anandamayee,,Majumdar,Arizona State University,Debashis,,Paul,University of California at Davis,Dianne,,Bautista,Ohio State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a flexible class of nonstationary stochastic models formultivariate spatial data. The method is based on convolutions ofspatially varying covariance kernels and produces mathematicallyvalid covariance structures. This method generalizes the convolutionapproach suggested by Majumdar and Gelfand (2007) to extendmultivariate spatial covariance functions to the nonstationary case.A Bayesian method for estimation of the parameters in the covariancemodel based on a Gibbs sampler is proposed, and applied to simulateddata. Model comparison is performed with the coregionalization modelof Wackernagel (2003) which uses a stationary bivariate model. Basedon posterior prediction results, the performance of our model isseen to be considerably better.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Hierarchical models,mwu@hsph.harvard.edu,,Michael C. Wu,,Harvard University,655 Huntington Ave.,410-917-6621,,mwu@hsph.harvard.edu,Variable selection in the kernel machine framework,1,Michael,C,Wu,Harvard University,Tianxi,,Cai,Harvard University,Xihong,,Lin,Harvard University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The complexity of high-throughput biomedical data requires the use of flexible methods that can account for nonlinearity and complex interactions in building the predictive models.  Kernel machine methods, e.g. support vector machines, meet these criteria and are frequently applied to build predictive models from genomics data.  However, KM methods are still subject to considerable noise and decreased prediction accuracy when few predictors are related to the outcome.  Variable selection is necessary.  We propose a statistical framework for integrating regularization and variable selection with kernel machines that still maintains the flexible dual formulation.  Connections with existing variable selection procedure are examined. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Variable subset selection/model selection,High dimensional data,ye.li@utoronto.ca,,Ye Li,,University of Toronto,"679, 155 College Street",4168977827,,ye.li@utoronto.ca,"Spatio-Temporal Modelling for Lupus Incidence in Toronto, Canada since 1965",1,Ye,,Li,University of Toronto,Patrick,E,Brown,Cancer Care Ontario,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The data provided by the Toronto Western Hospital Lupus clinic provide a fairly complete capture of residence locations of lupus cases in Toronto since 1965. These data span several census periods, with each census using different geographical boundaries for census tracts. A fine grid (175m*175m) was created over entire Greater Toronto Area, expected count were calculated for each grid cell by obtaining the expected count for each Age, Sex and Year group of the census data. Using that as an offset, a Gaussian Markov Random Field is fit on the grids assuming risk surface does not change over time. The posterior sample of risk surfaces is used to construct an estimated risk surface and detect areas likely to be 'clusters' of excess risk. Reporting bias is examined by including distance from the clinic as a covariate, allowing for the clinic changing locations in the 1980's. Future work planned involves allowing the risk surface to change over time, and using Markov Random Fields to approximate continuous spatial surfaces. The results of this research will be used to identify environmental risk factors for the disease.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Applied data analysis,Judith.Quinlan@cytel.com,,Judith Quinlan,VP; Adaptive Clinical Trials,Cytel Inc,675 Massachusetts Avenue,+1 857-928-7889,,Judith.Quinlan@cytel.com,Improved dose ranging through adaptive dose allocation,1,Judith,A,Quinlan,Cytel Inc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Dose ranging studies are internal decision making studies, and are an area of development where agencies have actively encouraged the use of adaptive designs. In this particular case study, we will see how given a fixed sample size, adaptive allocation out performs the traditional approach, and efficiently maximizes patient allocation to doses of interest. Focus will also be given to the role and benefits of simulations. Both in designing the clinical trial, but also importantly as an example of how simulations were effectively used to minimize drug supply requirements for this adaptive trial. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Statistical education,aghosh@bios.unc.edu,,Arpita Ghosh,,University of North Carolina at Chapel Hill,141 Brookberry Circle,301-247-9053,,aghosh@bios.unc.edu,Risk Effect Estimation for Multiple Phenotypes and Gene-environment Interaction: a Conditional Likelihood Approach,1,Arpita,,Ghosh,"Department of Biostatistics,The University of North Carolina at Chapel Hill",Fei,,Zou,"Department of Biostatistics andCarolina Center for Genome Sciences andCenter for Environmental Bioinformatics,The University of North Carolina at Chapel Hill",Fred,A.,Wright,"Department of Biostatistics andCarolina Center for Genome Sciences andCenter for Environmental Bioinformatics,The University of North Carolina at Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The use of genome-wide testing thresholds in association scans isknown to inflate estimates of genetic risk among significant SNPs (thewinners curse). We have recently reported an approximateconditional likelihood approach to correct for this bias, using theestimated risk effect and its standard error as reported by standardstatistical software. A similar problem arises when risk estimation isperformed for secondary effects, such as secondary phenotypes orgene-environment interactions, when the secondary analysis isrestricted to SNPs that are significant for the primary phenotype.Such secondary bias can be substantial, and we describe an extensionof our conditional likelihood approach to the multivariate settingwhere multiple effect coefficients are simultaneously estimated. Theresults have considerable importance for the proper analysis ofsecondary effects, and in the design of follow-up studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Epidemiologic methods,xuegu@mdanderson.org,,Xuemin Gu,,MD Anderson Cancer Center,1400 Pressler Street,713-792-1624,713-563-4242,xuegu@mdanderson.org,Bayesian Adaptive Randomization Designs versus Frequentist Designs for Targeted Agent Development,1,Xuemin,,Gu,MD Anderson Cancer Center,Suyu,,Liu,MD Anderson Cancer Center,Jack J.,,Lee,MD Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"With the advent of searching for predictive markers in guidingtargeted agent development, many new designs have been proposedrecently to increase study efficiency. These include the efficienttargeted design,1 adaptive signature design,2 marker by treatmentinteraction design,3 and biomarker adaptive threshold design,4 etc. Incontrast to the frequentist selection or equal randomization designs,we had reported a novel Bayesian adaptive randomization design toallow treating more patients with effective treatments, stoppingineffective treatments early, and increasing efficiency whilecontrolling type I and type II errors.5  Similar Bayesian designs wereproposed in this study.  The new designs incorporate rational learningfrom the interim data to guide the study conduct in terms ofrandomization. By comparing with previously published designs, theproposed design can be efficient and ethical and is also more flexiblein the study conduct. The statistical properties for various designsare evaluated via simulation studies.",FALSE,FALSE,T1: Competing Risks,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Clinical trials,scott@berryconsultants.com,,Scott Berry,Statistical Scientist,Berry Consultants,3145 Chaco Canyon Drive,979-690-2027,,scott@berryconsultants.com,Bayesian Adaptive Designs in Medical Device Trials,1,Scott,M,Berry,Berry Consultants,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Adaptive designs--especially Bayesian adaptive designs--have been ahot topic.  In this talk I focus on presenting actual designs andresults of Bayesian adaptive designs.  These features include adaptivesample size selection, dropping of treatment arms, and early successanalyses.  These features will be demonstrated through actual trialexamples.  The focus of the talk will be on the applications.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Bayesian methods,h.tang@mayo.edu,,Hui Tang,,Mayo Clinic,200 First St SW,5075384033,,h.tang@mayo.edu,Statistical metrics for quality assessment of high density tiling array data,1,Hui,,Tang,Mayo Clinic,Terence,,Speed,University of California at Berkeley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"High density tiling arrays are designed to blanket an entire genomic region of interest using tiled oligonucleotides at very high resolution and are widely used in ChIP-chip, DNA methylation, comparative genomic hybridization and transcriptome mapping studies. Experiments are usually conducted in multiple stages, including chromatin sonication, amplification of DNA fragment mixtures, labeling and hybridizing them onto tiling arrays, in which unwanted variations maybe introduced. As high density tiling arrays become more popular and are adopted by many research labs, it is pressing to develop quality control tools as what people did in DNA expression microarrays. To this aim, we propose a set of statistical quality metrics analogous to those in expression microarray studies with the application in high density tiling array data. We also developed a procedure to estimate the significance of the proposed metrics by using randomization test. Our method is applied to multiple real data sets, including three independent ChIP-chip experiments and one transcriptom mapping study. It showed sensitivity in capturing tiling arrays with high noise levels and filled in a gap in the literature for this topic.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Applied data analysis,Deborah.Glueck@uchsc.edu,,Deborah Glueck,Assistant Professor,Colorado School of Public Health,Department of Biostatistics,303-315-7227,,Deborah.Glueck@uchsc.edu,A two-stage adaptive design can increase power for multiple comparisons,1,Deborah,H,Glueck,"Department of Biostatistics, Colorado School of Public Health, University of Colorado Denver",Anis,,Karimpour-Fard,"Center for Computataional Pharmacology, University of Colorado School of Medicine, University of Colorado Denver",Keith,E,Muller,"Division of BiostatisticsDepartment of Epidemiology and Health Policy ResearchUniversity of Florida",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Testing the association between multiple polymorphisms and a disease state involves multiple hypothesis testing.  All of the many methods to avoid inflating the Type I error rate do so at the price of a decline in power.  For multiple comparison procedures like the Benjamini-Hochberg procedure, the power decreases as a function of the number of hypotheses tested.We propose a two-stage adaptive design to increase power for multiple comparisons.  For the two-stage process, 1) estimate the number of null hypotheses in the experiment and choose a truncation point based on this estimate.  2) Conduct a multiple comparison procedure for all hypotheses whose p-values are less than the truncation point.    The two-stage adaptive design typically increases power while still providing control of the type I error rate. The increase in power occurs because fewer hypotheses are considered. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Multiple testing,david.andrae@austin.ppdi.com,,David Andrae,"Manager, Biostatistics","PPD, Inc",7551 Metro Center Drive,512 747 5327,512 747 9327,david.andrae@austin.ppdi.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,jzhy@umich.edu,,Jin,,Zheng,3953 Wind Drift Dr. E,(745)7640721,,jzhy@umich.edu,Locate Complex Disease Loci by Investigating Gene and Environment Interaction for Genome-Wide Association Studies,1,Jin,,Zheng,University of Michigan,Goncalo,R,Abecasis,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Most of complex diseases result from the interaction of gene andenvironmental factors. But we still do not know the best way to locatecomplex disease susceptibility loci by exploiting the gene environmentinteraction, especially at the genome-wide associate studiesframework, in which hundreds of thousands markers are scanned.The traditional model is the logistic model, with covariates ofgenotypic and exposure status, and the interaction term of them,testing how gene and environment affect disease interactively. Anotherway is to detect the gene environment interaction in case-only data.Kraft et al (2007) proposed a two-degrees-of-freedom test for bothinteraction effect and genotypic effect. Recently, Murcary et aldevelop a two-step method. They discover the marginal correlation ofgene and environment first and then use the traditional model at thesecond step, and claimed it is more powerful compared to thetraditional method. In this paper, we would compare performance of several methods forsimulated data under genome-wide association structure. And we proposea one-step method, which considers the correlation of gene andenvironment at both case and control groups. We speculate this wouldgain power at some situations. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,lixx0525@umn.edu,,Pei Li,,University of Minnesota,1262 Fifield Ave,6122294230,,lixx0525@umn.edu,Bayesian Non-Parametric Approaches for Detecting Abrupt Changes in Disease Maps.,1,Pei,,Li,University of Minnesota,Sudipto,,Banerjee,University of Minnesota,Timothy,E,Hanson,University of Minnesota,Alexander,M,McBean,University of Minnesota,,,,,,,,,,,,,,,,,,,,,,,,,"In many applications involving geographically referenced datain public health, investigators want to understand the underlyingmechanisms causing disparities in the outcome variables. Statisticalmethods correctly accounting for uncertainty at various levels to elicit'boundaries' or 'zones' of rapid change help epidemiologistsand policy-makers better understand the factors driving these disparities. Such boundaries or zones often occur due to lurking spatial variables representing local disparities, such as those in income or access to health care. This talk will discuss Bayesian non-parametric approaches using areally dependent stick-breaking priors to achieve fullymodel-based inference on regions to reveal clusters or boundaries thatsuggest hidden risk factors. In particular, we concentrate on modelsthat embed univariate and multivariate Markov random fields within anon-parametric framework. We illustrate with simulated data as well asPneumonia and Influenza hospitalization data from the SEER-Medicareprogram in Minnesota.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Bayesian methods,qin.rui@mayo.edu,,Rui Qin,Research Associate,Mayo Clinic,Department of Health Sciences Research,507-538-3837,507-266-2477,qin.rui@mayo.edu,Comparison of three adaptive dose-finding models for combination therapy in phase I clinical trials,1,Rui,,Qin,Mayo Clinic,Yufen,,Zhang,University of Minnesota,Sumithra,J,Mandrekar,Mayo Clinic,Wei,,Zhang,Boehringer Ingelheim,Daniel,J,Sargent,Mayo Clinic,,,,,,,,,,,,,,,,,,,,,"Combination therapies become more popular in current medical researchin pursuit of potential synergy in efficacy. Even though previousknowledge about each individual component may suggest proper dosagesfor combination therapy, a phase I clinical trial is still requiredfor determining of the optimal dose combination to be further testedfor efficacy. We have proposed three models incorporating bothtoxicity and efficacy within a Bayesian adaptive design in thedose-finding for combination therapy. Their performances underclinical relevant scenarios are evaluated by simulations. We concludethat all three models work reasonably well except for the extremescenarios with significant non-monotone dose-efficacy curves. Furtherresearch are needed to improve the models to accommodate extremescenarios.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Adaptive design/adaptive randomization,xiaor@umich.edu,,rui xiao,,University of Michigan,"Department of Biostatistics, School of Public Health, University of Michigan",714-866-9775,,xiaor@umich.edu,A mixture model for the analysis of allelic expression imbalance,1,rui,,xiao,University of Michigan,Michael,,Boehnke,University of Michigan,Laura,,Scott,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genetic polymorphisms that regulate gene expression account for majorphenotypic diversity in human, and may also predispose to complexdiseases and determine variability in quantitative traits.Polymorphisms affecting gene expression in cis will causedifferentiated expression levels depending on which of the two allelesof the SNP of interest are present. Such an allelic expressionimbalance (AEI) can be detected in individuals heterozygous for atranscribed SNP. The use of AEI is complementary to testing forSNP-gene expression association and has the advantage of testing bothalleles within the same environment in each individual. To detect theassociation between the SNP of interest and the AEI, we propose amixture model and corresponding expectation-maximum (EM) algorithmthat take into account the linkage disequilibrium (LD) structurebetween the SNP of interest and the transcribed SNP.  In the typicalcase of small sample size, we describe an approach to estimate themixing proportion by using LD information from the HapMap to increasepower. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,ning.wang@ttu.edu,,Ning Wang,,"Math and Statistics Department, Texas Tech Univers",701 N. Ithaca Ave. Apt. 3620,8064452055,,ning.wang@ttu.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,yining.du@ttu.edu,,Yining Du,,"Math and Statistics Department, Texas Tech Univers",701 N. Ithaca Ave. Apt.820,8065437858,,yining.du@ttu.edu,SIMULATION OF THE OPTIMAL TIMING OF A MULTIPLE REGIME,1,Yining,,Du,"Math and Statistics Department, Texas Tech University",Ning,,Wang,"Math and Statistics Department, Texas Tech University",Clyde,,Martin,"Math and Statistics Department, Texas Tech University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A characteristic of treating a terminal disease with multiple drugtherapies is the timing of the drug treatments. We develop a verysimple simulation model for such treatments. Mathematically this workis based on the theory of switching systems. We assume that somemeasure of quality, x, is being measured and that it is scaled so that0 represents deathand 1 represents remission. For each treatment we assume a discretelinear model. Then for two treatments we have the combined model whichswitchs between two linear systems. Dayawansa and Martin gavenecessary and sufficient conditions for the stability of switchingsystems and in their proof constructed the worst case switchingregime. This worst case regime is the best case for the problem here.We would like to switch the treatment in an effort to either drive thesystem to 1 or to delay as long as possible the inevitable approach to0. We show that the best possible switching times occur when thesystem is driven by one treatment as far from zero as possible. Thisis in contrast to the usual practice. This result is a major deviationfrom the result of Dayawansa and Martin in that the model isstochastic. Dayawansa and Martin worked in continuous time and withdeterministic models. We are able to give optimal strategies forswitching both in the deterministic case and the stochastic case.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Cancer applications,ann.chen@moffitt.org,,Ann Chen,Assistant Member,Moffitt Cancer Center,12902 Magnolia Drive,(813) 745-6890,,Ann.Chen@moffitt.org,A nonparametric approach to detect nonlinear correlation in gene expression,1,Yian,Ann,Chen,"Department of Biostatistics, Moffitt Cancer Center, USF",Jonas,S.,Almeida,"Department of Bioinformatics and Computational Biology, The University of Texas, M.D. Anderson Cancer Center",Adam,J.,Richards,"Department of Biostatistics, Bioinformatics, and Epidemiology, Medical University of South Carolina",Peter,,Müller,"Department of Biostatistics, The University of Texas, M.D. Anderson Cancer Center",Raymond,J.,Carroll,"Department of Statistics, Texas A&M University",Baerbel,,Rohrer,"Department of Neurosciences, Medical University of South Carolina",,,,,,,,,,,,,,,,,"We propose a distribution-free approach to detect nonlinear relationships by reporting local correlation. The effect of our proposed method is analogous to piece-wise linear approximation although the method does not utilize any linear dependency. The proposed metric, maximum local correlation, was applied to both simulated cases and expression microarray data comparing the rd mouse, which exhibits photoreceptor degeneration with age-matched control animals. The rd mouse is an animal model (with a mutation for the gene Pde6b) for photoreceptor degeneration. Using simulated data, we show that maximum local correlation detects nonlinear association, which could not be detected using other correlation measures. In the microarray study, our proposed method detects nonlinear association between the expression levels of different genes, which could not be detected using the conventional linear methods.  ",FALSE,FALSE,T5: Analysis of Censored Cost or Health Outcomes Data,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Nonlinear models,Genomics,davisjwa@health.missouri.edu,,J. Wade Davis,Assistant Professor,University of Missouri,Office of Medical Research,573-882-0770,,davisjwa@health.missouri.edu,Statistical Inference for Pooled Samples in Next Generation Sequencing,1,Justin,W,Davis,University of Missouri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the last few years, there have been major breakthroughs in DNAsequencing technology.  These 'next-generation' sequencers have madesequencing more accessible to researchers due to reduced costs andhigher throughput.  Even though costs are lower, researchers poolsamples from different individuals to further reduce costs and to takeadvantage of the large number of reads produced in a singleexperiment.  However, such pooling prevents meaningful varianceestimates and eliminates the possibility of statistical inference atthe appropriate unit of analysis.  This talk focuses on problemscaused by pooling and presents a possible solutions to extract moremeaningful information from pooled data arising from next-generationshort-read sequencing technologies. Simulations demonstrate theperformance of the proposed method, while application of the method toactual data from a run on a 454 Life Sciences sequencer closes out thepresentation.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,T1: Methods for Reproducible Research,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Microarray analysis,Data mining/massive data sets,DNA sequencingrenajsun@umich.edu,,Jie (Rena) Sun,,University of Michigan,Department of Biostatistics SPH II,734-846-4331,,renajsun@umich.edu,A Risk-Adjusted O-E CUSUM with V-mask in a continuous time setting,1,Jie (Rena),,Sun,University of Michigan,John,D,Kalbfleisch,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Some recent research has examined the CUSUM control chart in a medical setting. Motivated by a risk-adjusted one-sided CUSUM procedure with a continuous time setting, we introduce a risk-adjusted O-E (Observed-Expected) CUSUM to simultaneously monitor 'worse than expected' and 'better than expected' by using a V-shaped mask (V-mask) as the decision criterion. This approach has several advantages over the one-sided CUSUM. Simulation studies were conducted to test the performance of the proposed method, which was compared to the one-sided CUSUM method. Control limits (or threshold value for signaling) were obtained for facilities with different sizes by controlling the false alarm rate over a period of given length. A case study was carried out for 67 liver transplantation programs, using the proposed O-E CUSUM with a V-mask, the one-sided CUSUM, and the current-used model for the Program-Specific Reports (PSR) in the Scientific Registry of Transplant Recipients (SRTR). The presentation concludes with a brief discussion on the unique features of the proposed procedure.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Survival analysis,Statistics in orgran transplant applicationsafelmi@mail.med.upenn.edu,,Angelo Elmi,Ph. D student,University of Pennsylvania,420 S. 45th St.,(516)660-7242,,afelmi@mail.med.upenn.edu,Modelling Labor Curves in Women Attempting a Vaginal Birth After a Cesarean Using a B-splines Based Semiparametric Nonlinear Mixed Effects Model,1,Angelo,,Elmi,"University of Pennsylvania, Dept. of Biostatistics",Sarah,,Ratcliffe,"University of Pennsylvania, Dept. of Biostatistics",Sam,,Parry,"University of Pennsylvania, Dept. of Obstetrics and Gynecology",Wensheng,,Guo,"University of Pennsylvania, Dept. of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,"New methodology is developed for the SemiparametricNonlinear Mixed Effects Model and is applied for the analysis andcomparison of labor curves from women attempting a vaginal birthafter cesarean (VBAC) delivery. Existing approaches estimate the shapefunction with smoothing splines and use a backfittingapproach that iterates between two mixed effects models andconsequently two separate likelihoods. Such an algorithm will not beguaranteed to converge; and because it incorrectly assesses thevariability in the parameter estimates, it will not give validinferences. We develop a new model replacing the smoothing spline withB-splines. This approach requires only one mixed effects model, andsince all parameters are estimated from the same likelihood, aniterative algorithm will be guaranteed to converge andlikelihood-based inferences will be valid. We apply this model to makecomparisons between the average labors of women who do and do notexperience uterine rupture and to find the earliest time point whereclinicians could distinguish between the average labor curves indifferent groups which enables them to make the potentiallylife-saving decision of whether or not to perform a cesarean delivery.Since the dimension of integration is reduced, we are able to obtainaccurate representations of the log-likelihood based on AdaptiveGaussian Quadrature.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Functional data analysis,Nonlinear models,inyoungk@vt.edu,,Inyoung Kim,Assistant Professor,Virginia Tech,Department of Statistics,540-231-5366,,inyoungk@vt.edu,Sparsity priors for protein-protein interaction predictions,1,Inyoung,,Kim,"Department of Statistics, Virginia Tech.",Yin,,Liu,"Dept. of Neurobilogy and AnatomyUniversity of Texas Medical School at Houston",Hongyu,,Zhao,"Department of Epidemiology and Public HealthYale University ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Protein-protein interactions play important roles in most fundamentalcellular processes including cell cycle, metabolism, and cellproliferation. Therefore, the development of effective statisticalapproaches to predicting protein interactions based on recentlyavailable large scale experimental data is a very important problem.However, due to the number of protein-protein interaction to beobserved is very small, the number of parameters to be estimated isvery large. Therefore the data is very sparse due to a few number ofprotein-protein interaction to be observed. In this paper, weincorporate a point-mass mixture prior in the analysis through aBayesian method. The prediction results between with and without thisprior are compared using the large-scale protein-protein interactiondata obtained from high throughput yeast two-hybrid experiments. Theresult demonstrates the advantages of the Bayesian approach with asparsity prior based on point-mass mixture prior.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Bayesian methods,miwing@email.unc.edu,,Mihee Lee,,UNC,318 Hanes Hall CB#3260,919-843-9058,,miwing@email.unc.edu,Sieve Type Deconvolution Estimation of Mixture Distributions with Boundary Effects,1,Mihee,,Lee,UNC-CH,Peter,,Hall,The University of Melbourne,Haipeng,,Shen,UNC-CH,Christina,,Burch,UNC-CH,Jon,,Tolle,UNC-CH,J. S.,,Marron,UNC-CH,,,,,,,,,,,,,,,,,"Density estimation in measurement error models has been widelystudied. However, most existing methods consider only the case wherethe target distribution is continuous, hence they cannot be applieddirectly to many practical problems, such as the estimation ofmutation effects distribution in evolutionary biology, which is amixture of a discrete atom and a continuous component.  In thispaper, we propose two sieve type estimators for distributions thatare mixtures of a finite number of discrete atoms and continuousdistributions under the framework of measurement error models. Onemajor contribution of our paper is correct handling of knownboundary effects. Compared to classical Fourier deconvolution, ourestimators reduce the boundary problem at known non-smoothboundaries. In addition, the use of penalization improves thesmoothness of the resulting estimator and reduces the estimationvariance. We establish some asymptotic properties of the proposedestimators, and illustrate their performance via a simulation studyand a real application in evolutionary biology.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,Measurement error,enders.felicity@mayo.edu,,Felicity B. Enders,Assistant Professor,Mayo Clinic,"200 First St, SW",507-538-4970,507-284-9542,enders.felicity@mayo.edu,Assessing Biostatistical Literacy and Statistical Thinking,1,Felicity,B,Enders,Mayo Clinic Division of Biomedical Statistics and Informatics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"One of the tenets of statistical education is that outcome assessmentshould be based on an independent instrument rather thancourse-specific outcomes.  DelMas, Garfield, Ooms, and Chance (2006)have developed the Comprehensive Assessment of Outcomes in a firstStatistics course (CAOS) and a smaller version called the StatisticsThinking and Reasoning Test (START; Garfield, DelMas, Chance, andOoms; 2007),  but no such assessment has been available forbiostatistics courses for nonstatisticians.  Windish et al (2007)developed an instrument to assess physicians' comprehension ofbiostatistical results typically represented in the medical literature(the Windish Quiz).  In this study, the START and the Windish Quizwere used to jointly assess biostatistical literacy and statisticalthinking among physicians completing an introductory course inbiostatistics.  The two assessments were independently associated withfinal exam scores (START p=0.004; Windish p<0.001); together, theypredicted 46% of exam variability.  This project suggests thatbiostatistical literacy varies somewhat from statistical literacy andthat a different outcome instrument is needed for biostatistics courses.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biostatistics Education,Statistical education,lprice1@tuftsmedicalcenter.org,,Lori Lyn Price,Biostatistician,Tufts Medical Center,Tufts Medical Center,617-636-2398,617-636-5560,lprice1@tuftsmedicalcenter.org,xxx,1,Lori,Lyn,Price,Tufts Medical Center,John,L,Griffith,Tufts Medical Center,Emalee,,Flaherty,,Robert,,Sege,Boston Medical Center,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,xoyo@unc.edu,,Xuanyao He,,UNC at Chapel Hill,"318 Hanes Hall, CB#3260, UNC at Chapel Hill",919-962-5707,,xoyo@unc.edu,Asymptotic Comparison of Predictive Densities for Dependent Observations,1,Xuanyao,,He,"Department of Statistics and Operations Research, UNC at Chapel Hill",Richard,,Smith,"Department of Statistics and Operations Research, UNC at Chapel Hill",Zhengyuan,,Zhu,"Department of Statistics and Operations Research, UNC at Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper studies Bayesian predictive densities based on differentpriors and frequentist plug-in type predictive densities when thepredicted variables are dependent on the observations. AverageKullback-Leibler divergence to the true predictive density is used tomeasure the performance of different inference procedures. The notion ofsecond-order KL dominance is introduced, and an explicit condition fora prior to be second-order KL dominant is given using an asymptoticexpansion. As an example, we show theoretically that for mixed effectsmodels, the Bayesian predictive density with a prior from a particularimproper prior family dominates the performance of REML plug-indensity, while the Jeffreys prior is not always superior to the REMLapproach.Simulation studies are included which show good agreement with theasymptotic results for moderate sample sizes.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Spatial/temporal modeling,yukf@mail.nih.gov,,Kai Fun Yu,,NICHD,"6100  Executive Bld, Room 7B07J",301-496-6813,301-402-2084,yukf@mail.nih.gov,An efficient rank-based test for the generalized nonparametric Behrens-Fisher problem,1,Kai,F,Yu,Eunice Kennedy Shriver  National Institute of Child Health and Human Development,Qizhai,,Li,National Cancer Institute,Aiyi,,Liu,Eunice Kennedy Shriver  National Institute of Child Health and Human Development,Kai,,Yu,National Cancer Institute,,,,,,,,,,,,,,,,,,,,,,,,,"For a generalized Behrens-Fisher hypothesisproblem for comparison of multiple outcomes  commonly encountered inbiomedical research,Huang et al. (2005,  Biometrics, 61, 532-539)improved O'Brien's (1984,  Biometrics, 40}, 1079-1087)rank-sum tests with replacement of the ad hoc variance by theasymptotic variance of thetest statistics. The improved tests control the Type error at thedesired level and gain powerwhen the differences in each individual outcome variable fall into thesame direction. However, theymay lose power when the differences are in different directions ( e.g.some are positive, and some are negative).We propose a more efficient test statistic, taking the maximum ofindividual rank-sum statistics, that controls the type I error andmaintains satisfactory power regardless of the directions ofdifferences.  Data from two studies are used to illustrate theapplication of the proposed test.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Nonparametric methods,Multivariate methods,mwheeler@bios.unc.edu,,Matthew Wheeler,,The University of North Carolina at Chapel Hill Bi,CB #7420,919-419-1792,,mwheeler@bios.unc.edu,Using finite multivariate mixtures to model adverse birth outcomes,1,Matthew,W,Wheeler,The University of North Carolina at Chapel Hill Department of Biostatistics,Amy,,Herring,The University of North Carolina at Chapel Hill Department of Biostatistics,Eric,,Kalendra,North Carolina State University Department of Statistics,Montse,,Fuentes,North Carolina State University Department of Statistics,Brian,,Reich,North Carolina State University Department of Statistics,,,,,,,,,,,,,,,,,,,,,"Gestational age and birth weight are continuous pregnancy outcomesthat are frequently investigated in the literature.  As the observeddistribution of both outcomes is a complex multivariate distributioninvestigators frequently dichotomize both responses into adverseresponse categories. These dichotomized variables are then analyzedindependently. Such analyses obviously ignore possible correlationbetween the pregnancy outcomes, and create cut points that may loseimportant features of data.  In attempt to better characterize risk ofboth pre-term delivery and low birth weight we study both of theseoutcomes jointly using finite mixture models.  Here a bivariate normaldistribution is used for each bin, and, given a bin, the mean age andbirth weights are modeled through regression using a vector of riskfactors.  Further, as the probability of group membership may also bedependent of these and other risk factors, we model the probability offalling in a specific bin through logistic regression.  Thus riskfactors are used to describe the risk of an adverse outcome, andexplain the changes in the mean gestational age and birth weight giventhe assignment to a specific bin in order to better characterize theobserved distribution of birth outcomes. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Epidemiologic methods,Bayesian methods,kspence@mail.ucf.edu,,Kathryn Spence Pridemore,,University of Central Florida,215 Oglethorpe Place,407-947-0754,,kspence@mail.ucf.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,dongfeng.wu@louisville.edu,,Dongfeng Wu,Associate Professor,University of Louisville,"555 S. Floyd Street, Suite 4026",502-852-1888,502-852-3294,dongfeng.wu@louisville.edu,Bayesian Inference for True-Benefit and Over-diagnosis in Periodic Cancer Screening,1,Dongfeng,,Wu,"Dept of Bioinformatics and Biostatistics, School of Public Health, University of Louisville",Gary,L.,Rosner,"Dept of Biostatistics and Applied Math, Univ. of Texas, MD Anderson Cancer Center",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We developed a probability model for evaluating true benefit and over-diagnosis in periodic cancer screening. Peoples who take part in the screening program are categorized into 4 mutually exclusive groups: Pure-Waste, No-Benefit, True-Benefit, and Over-Diagnosis. For each case, the probability was derived. Simulation studies using the HIP (Health Insurance Plan for Greater New Yorker) study's data provide estimates for these probabilities. Our model can provide policy makers with important information regarding the percentage of true early detection and the percentage of over diagnosis. Though the study focuses on breast cancer screening, it is also applicable to other kinds of chronic disease.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Diagnostic and screening tests,Bayesian methods,pangdu@vt.edu,,Pang Du,,Virginia Tech,406-A Hutcheson Hall,(540) 231-7613,(540) 231-3863,pangdu@vt.edu,Nonparametric modeling of semi-continuous data with application to medical cost data analysis,1,Pang,,Du,Virginia Tech,Lei,,Liu,University of Virginia,Anna,,Liu,University of Massachusetts at Amherst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Semi-continuous data arise in longitudinal studies when the repeatedmeasures contain a large portion of zero values, as well as rightskewness and heteroscedasticity for non-zero positive values, e.g.,monthly medical costs, daily drinking records, or number of dayshospitalized within a year.  We propose a nonparametric random effectstwo-part model to analyze this type of data.  One part of the model isfor the odds of being positive, the other is for the positivemeasurements, and the two parts are joined by the correlated randomeffects.  Nonparametric smooth estimates of the pattern functions inboth parts are obtained through the minimization of a penalized jointlikelihood.  The model is then applied to the analysis of longitudinalmonthly medical costs of chronic heart failure patients from theclinical data repository (CDR) at the University of Virginia.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Nonparametric methods,wingjeff@umich.edu,,Jeffrey J Wing,,Department of Biostatistics University of Michigan,1420 Washington Heights,(734) 936-9848,,wingjeff@umich.edu,Performance of the Hochberg multiple testing procedure in cluster randomized designs,1,Jeffrey,J,Wing,"Department of Biostatistics, University of Michigan",Brisa,N,Sánchez,"Department of Biostatistics, University of Michigan",Cathie,,Spino,"Department of Biostatistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Hochberg step-up procedure (1988) is a simple improvement to theBonferroni multiple testing procedure which increases the power,preserves the familywise error rate, and maintains the ease ofapplication.  While its use is quite common in applications during theanalysis stage, there is little guidance in the clinical trial designliterature on how to incorporate the procedure into sample sizecalculations.  We performed simulation studies to examine differencesin Type I error rates and power under various clinical trial designswhen the Hochberg procedure is used in the analysis, assessing bothsimple randomized and cluster randomized designs.  We varied thenumber of outcome variables (i.e., comparisons) and their correlationsand compared results with that of the traditional Bonferroni approachas a benchmark.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Power analysis/sample size,siddique@northwestern.edu,,Juned Siddique,,Northwestern University,Department of Preventive Medicine,312-908-9241,,siddique@northwestern.edu,A Missing Data Approach for Adjusting Diagnoses of Post-Traumatic Stress Disorder that are Subject to Rater Bias,1,Juned,,Siddique,Northwestern University,Bonnie,L,Green,Georgetown University,Robert,D,Gibbons,University of Illinois at Chicago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Compared to other rating systems that are based on counts or frequencies of readily observerable behaviors, assessments of Post-Traumatic Stress Disorder (PTSD) allow more scope for disagreement among raters due to the fact that PTSD assessments require raters to make subjective judgments about the meaning and representations of target behaviors. We describe a multiple imputation approach using a Bayesian censored ordinal probit model to adjust diagnoses of PTSD that are subject to rater bias. Adjusted diagnoses can be used as the basis for revised estimates of PTSD prevalence or as a covariate in subsequent analyses. We apply our methods to data from a depression study where nurse practitioners were twice as likely to diagnose participants with PTSD as compared to diagnoses made by clinical psychologists.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Latent variables,Health services research,meagan_clement@rhoworld.com,,Meagan E. Clement,Senior Biostatistician,"Rho,Inc.",6330 Quadrangle Drive,919-408-8000,,meagan_clement@rhoworld.com,Approximation of the Geisser-Greenhouse Sphericity Estimator and its Application to Analyzing Diffusion Tensor Imaging Data,1,Meagan,E.,Clement,"Rho, inc.",David,,Couper,"UNC - Chapel Hill, Dept. of Biostatistics",Keith,E.,Muller,Univ. of Florida - Division of Biostatistics,Hongtu,,Zhu,"UNC - Chapel Hill, Dept. of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,"Recent protocol innovation with magnetic resonance imaging has resulted in diffusion tensor imaging (DTI). The approach holds tremendous promise for improving our understanding of neural pathways, especially in the brain. The DTI protocol highlights the distribution of water molecules, in three dimensions. In a medium with free water motion, the diffusion of water molecules is expected to be isotropic, the same in all directions. With water embedded in nonhomogeneous tissue, motion is expected to be anisotropic, not the same in all directions, and might show preferred directions of mobility. DTI fully characterizes diffusion anisotropy locally in space, thus providing rich detail about tissue microstructure. However, little has been done to define metrics or describe credible statistical methods for analyzing DTI data. Our research addresses these research issues.     First, we show that fractional anisotropy values for given regions of interest are functions of the Geisser-Greenhouse (GG) sphericity estimator. Next, we demonstrate that the GG sphericity estimator can be approximated by a squared beta distribution. Finally, noise is added to show these approximations also work for simulated diffusion tensors. Thus, using these approximate distributions, one can then avoid the 'curse of dimensionality'.",FALSE,FALSE,,FALSE,FALSE,TRUE,SC5 Half-day course,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Imaging,Applied data analysis,brandelwine@hotmail.com,,Brandy Ringham,,"University of Colorado, Denver",2420 Sherri Mar St.,303-565-0803,,brandelwine@hotmail.com,Estimates of observed sensitivity and specificity must be corrected when reporting the results of the second test in a screening trial conducted in series,1,Brandy,M,Ringham,"University of Colorado, Denver",Deborah,H,Glueck,"University of Colorado, Denver",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recommendations for cancer screening are based, in part, on trials that use a series design to compare screening modalities.  In a series design, because the decision to conduct the second test depends on the results of the first test, the estimates of sensitivity and specificity for the second test are conditional.  Conditional sensitivity and specificity estimates may differ from unconditional estimates, i.e., those that would have been observed if the second test were used alone. 	All estimates of diagnostic accuracy may be biased if the true state of disease is not observed.  In a common design for screening trials, diagnosis is made solely by use of a reference test.  Reference tests are given only to participants who have abnormal screening test results or who experience signs and symptoms of disease during the follow-up period.  The remaining participants are assumed to be disease free.  The study investigator calculates estimates of diagnostic accuracy using the observed cases of disease.  These estimates may not mirror the true results. 		Using parametric assumptions to derive formulae, we show that the chance of occult disease, and the correlation between the screening tests affect the difference between the unconditional true and the conditional observed sensitivity and specificity.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Diagnostic and screening tests,Cancer applications,kmesser@ucsd.edu,,Karen Messer,Associate  Professor,Moores UCSD Cancer Center,3855 Health Sciences Dr MC 0901,858 822 4334,,kmesser@ucsd.edu,Dose-establishment designs for phase I/II cancer immunotherapy trials,1,Karen,,Messer,"Division of Biostatistics and  Moores UCSD Cancer Center,University of California San Diego",Loki,,Natarajan,"Division of Biostatistics and  Moores UCSD Cancer Center,University of California San Diego",Edward,,Ball,"Department of Medicine and  Moores UCSD Cancer Center,University of California San Diego",Thomas,,Lane,"Department of Medicine and  Moores UCSD Cancer Center,University of California San Diego",,,,,,,,,,,,,,,,,,,,,,,,,"A standard Phase I oncology trial is designed to find the maximumtolerated dose in a setting where serious drug-related toxicity isexpected. However, agents investigated for cancer immunotherapy mayhope to show efficacy without increasing the rate of adverse events.Our Phase I/II dose-establishment design is suitable when thetherapeutic dose is expected to be well below the maximum tolerateddose. This design incorporates a likelihood ratio test of a safetyhypothesis in Phase I, using a standard 3+3 group sequentialenrollment scheme. Phase I serves as an interim safety analysis beforeproceeding to Phase II. The Phase I/II data are combined for an upperconfidence limit on the toxicity rate at the therapeutic dose. We givean example in a Phase I/II trial of immunotherapy in leukemia. Thenumber of patients enrolled in Phases I and II depends on thesequential toxicity outcomes of the trial, and both Phases contributeto estimating the toxicity rate and efficacy outcome at the targetdose. We show how to compute the power, expected sample size, andexpected number of dose limiting toxicities, as well as the MLE andexact small sample confidence intervals for the toxicity rate at thetherapeutic dose. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Cancer applications,Clinical trials,joyee123in@gmail.com,,Joyee Ghosh,,University of North Carolina at Chapel Hill,"1521 E Franklin St, Apt B-210",919-323-2284,,joyee123in@gmail.com,Bayesian Variable Selection for Latent Class Models,1,Joyee,,Ghosh,University of North Carolina at Chapel Hill,Amy,H.,Herring,University of North Carolina at Chapel Hill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Weight gain during pregnancy is believed to have an effect on various maternal and child health outcomes like child birth weight, maternal postpartum weight retention, gestational diabetes etc. Pregnancy weight gain is considered to be a modifiable risk factor and hence this is an important and active area of research. One of our goals is to investigate whether maternal characteristics such as age, gender, diet etc. are predictive of weight gain adequacy, defined based on Institute of Medicine criteria as inadequate, adequate or excessive. We use a Bayesian multinomial logistic model with Bayesian variable selection to assess the importance of predictors of weight gain. Our approach can be extended to latent class models with class probabilities depending on subject-specific predictors of risk, while allowing uncertainty regarding which predictors to include.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Hierarchical models,meihuawu@umich.edu,,Meihua Wu,Graduate Student,"University of Michigan, Biostatistics Dept","1420 Washington Heights, 4th Fl",7347632451,,meihuawu@umich.edu,Application of the Kalman filter algorithm to estimate a functional mixed model,1,Meihua,,Wu,"University of Michigan, Biostatistics Department",Brisa,N,Sánchez,"University of Michigan, Biostatistics Department",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Applications of functional data analysis (FDA) methods in epidemiology remain limited in large population-level studies.  One such FDA method is functional mixed models (FMMs).  In functional mixed models, the dependent variable for each individual in the study is a function, which is sampled at a finite set of time points.  The average across individuals is a function, which may depend on covariates.  Covariate effects are also represented by functions.  In small studies, FMMs can be easily estimated using standard mixed model software.  However, when using this software, the computation time grows proportional to the cube of the number of observations.  Thus, in large studies (in terms of the number of individuals) mixed model software fails.  The Kalman filter algorithm has been proposed as a computationally efficient approach to estimate FMMs, and has been used effectively in moderate size datasets where the set of sample times is the same for all individuals.  We employ the Kalman filter to estimate a FMM for a large dataset where the sample times differ across individuals.  We discuss some challenges of this application, and propose some solutions to make the estimation feasible. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,TRUE,Functional data analysis,Applied data analysis,lih5@mail.nih.gov,,Huilin Li,Dr.,NIH,"6120 Executive BLVD, # 8047",3014352510,,lih5@mail.nih.gov,Using Cases from Genome-Wide Association Studies to Strengthen Inference on the Association between Single Nucleotide Polymorphisms and a Secondary Phenotype,1,Huilin,,Li,"National Cancer Institute, DCEG, BB",Mitchell,H,Gail,"National Cancer Institute, DCEG, BB",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Whole genome scan case-control association studies offer theopportunity to study the association of genotypes with a secondaryphenotype in addition to case-control status.  Because controls mayrepresent a random sample from the population, they can be used forthis purpose.  However, the cases can provide additional usefulinformation.  We examine under what conditions cases can be usedwithout introducing bias in estimating the association between agenotype and a secondary phenotype.  We consider a two-stage model toincorporate the bias from using cases, and propose an Empirical Bayesmethod to combine the case and control data to estimate theassociation with reduced mean square error, based on a balance betweenbias and variance. Both simulated and real data examples suggest theadvantage of the newly proposed Empirical Bayes estimator.  We alsoshow how to extend the method to accommodate multiple case types andone control group.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Epidemiologic methods,yijia@psu.edu,,Yijia Feng,,"Department of Statistics, PSU",326 Thomas Building,814-321-2225,,yijia@psu.edu,Stopping Boundaries of Flexible Sample Size Design with Flexible Trial Monitoring - A Unifed Approach,3,Yi,,He,Sanofi-aventis,Zhenming,,Shun,Sanofi-aventis,Yijia,,Feng,The Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the group sequential (GS) approach with a fixed sample size design,the type I error is controlled by the additivity of exit spendingvalues. However, in a flexible sample size design where the samplesize will be re-calculated using the interim data, the overall type Ierror rate can be inflated. Therefore, the pre-defined the GS stoppingboundaries have to be adjusted to maintain the type I error level ateach interim analysis and the overall level. The modified ±- spendingfunction adjusted for sample size re-estimation (SSR) is proposed tomaintain the type I error level. We use unified approach andmathematically quantify the type I error with and without sample sizeadjustment constraints. As a result, stopping boundaries can beobtained by inversely solving the exact type I error functions. Thisunified approach, using Brownian motion theory, can be applied tonormal, survival, and binary endpoints. Extensive simulations show thestopping boundaries can control the type I error at each analysis andthe overall level.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Clinical trials,nclose@empiristat.com,,Nicole C.Close,President and Principal Biostatistician,"EmpiriStat, Inc.",13694 Sam Hill Drive,866-935-7828,866-276-7828,nclose@empiristat.com,Prioritizing Areas of Therapeutic Interest to Target Product Profiles: Lessons and Examples in Infectious Diseases and Vaccine Development,1,Nicole,C,Close,"EmpiriStat, Inc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A review from the statistician's role and viewpoint will be given on the statistical input provided and needed when reviewing strategies to prioritize areas of therapuetic interest for an organization.  Also, a large amount of time is spent with Regulatory Professionals in developing target product profiles and often times statisticians are not fully engaged or involved in this process.  Tips will be given on how to engage your statistician in these areas as well as how the statistician must engage the organizational team.  Examples are drawn from infestious disease areas and vaccine development.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Consulting,"Biologics, pharmaceuticals, medical devices",paciorek@hsph.harvard.edu,,Chris Paciorek,Assistant Professor,Harvard School of Public Health,655 Huntington Avenue,617-432-4912,617-432-5619,paciorek@hsph.harvard.edu,Bias and Spatial Scale in Models with Spatial Confounding,1,Christopher,J,Paciorek,Harvard School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Increasingly, regression models are being used when residuals arespatially correlated. The spatial residual may be induced by anunmeasured confounder. In this setting, the hope is that models thataccount for the spatial structure will reduce or eliminate the biasfrom confounding. I show that regression models with spatial randomeffects and closely-related models such as kriging and penalizedsplines are biased. I provide analytic and simulation results showinghow the bias depends on the spatial scales of the covariate of interest and the residual, with bias reducedsubstantially only when the scale of the covariate is small and thatof the residual larger than the covariate. The use of fixed effects tocapture residual spatial structure effectively reduces bias but with a bias-variance tradeoff. I illustrate the results with anexample of the effects of black carbon traffic pollution onbirthweight. Time provided, I will also discuss the effects ofresidual spatial correlation on the precision of the exposure estimator. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Epidemiologic methods,Spatial/temporal modeling,invited sessionqzheng@srph.tamhsc.edu,,Qi Zheng,,Texas A&M Health Science Center,1266 TAMU,979-845-5425,,qzheng@srph.tamhsc.edu,Comparing Bayesian and frequentist approaches to estimating mutation rates,1,Qi,,Zheng,"School of Rural Public Health, Texas A&M Health Science Center",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The concept of mutation lies at the heart of modern biology; measuring mutation rates is a major task in genetics research. The fluctuation experiment, pioneered by Luria and Delbruck in 1943, has been the method of choice for estimating microbial mutation rates. Recent years witnessed vigorous development of new statistical methods for estimating mutation rates in the context of the fluctuation experiment. These methods are strictly based on the likelihood principle. However, a Bayesian approach holds promise for some  problems that are not tractable by a frequentist approach. This presentation is based on a pilotstudy. It first presents Bayesian solutions to some problems that have already been solved by a frequentist approach, it then suggests Bayesian solutions to some open problems to which no solutions exist.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Statistical genetics,blankenshipd43@hotmail.com,,Derek Blankenship,Direector of Biostatistics,Baylor Health care System,7223 La Sobrina Drive,214-625-3604,,blankenshipd43@hotmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,ori@math.utep.edu,,Ori Rosen,Dr.,University of Texas at El Paso,Department of Mathematical Sciences,915 747-6843,,ori@math.utep.edu,A Bayesian Regression Model for Multivariate Functional Data,1,Ori,,Rosen,University of Texas at El Paso,Wesley,K,Thompson,University of California San Diego,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this paper we present a model for the analysis of multivariatefunctional data with unequally spaced observation times that maydiffer amongsubjects. Our method is formulated as a Bayesian mixed-effects modelin whichthe fixed part corresponds to the mean functions, and the random partcorresponds to individual deviations from these mean functions. Covariatescan be incorporated into both the fixed and the random effects. The randomerror term of the model is assumed to follow a multivariateOrnstein-Uhlenbeckprocess. For each of the response variables, both the mean and thesubject-specific deviations are estimated via low-rank cubic splines usingradial basis functions. Inference is performed via Markov chain MonteCarlo  methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Functional data analysis,Bayesian methods,dicook@iastate.edu,,Dianne Cook,Professor,"Department of Statistics, ISU",Snedecor Hall,1-515-294-5533,,dicook@iastate.edu,Incorporating interactive graphics into metabolomics data pre-processing,1,Dianne,,Cook,"Department of Statistics, Iowa State University",Michael,,Lawrence,,Suh-yeon,,Choi,,Heike,,Hofmann,,Eve,,Wurtele,,,,,,,,,,,,,,,,,,,,,,"In metabolomics experiments the purpose is to determine quantities ofmetabolites present in a sample and compare between samples. Methodsthat were developed for detecting a small number of metabolites arenow being used to detect hundreds of metabolites. Many of themetabolites are also unidentifiable because they do not exist in anylibrary of compounds.To determine the quantities of metabolites the GC-MS raw data needs tobe processed to extract the peaks, for each mass-to charge ratio(m/z), and group the peaks that occur in close temporal locations toproduce a metabolite profile. The raw data suffers from backgroundnoise, and temporal shifts from one sample to another. Thuspre-processing of the data is messy. A plus is that there are alwaysreplicates for each treatment and these can be used stabilize theestimation of the metabolite profiles.In our work we have incorporated interactive graphics to improve thequality of the pre-processing. Here examples of ways we can check thepre-processing: (1) Compare replicates and probe places where there isdisagreement in the samples from the same treatment; (2) Investigate diagnostic statistics measuring the peak fitting, to checkthe particularly problematic fits;(3) Examine and adjust thegrouping of peaks into metabolite profiles.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Metabolomics,Applied data analysis,Data pre-processingsylensing@uams.edu,,Shelly Lensing,Statistician,University of Arkansas for Medical Sciences,Department of Biostatistics,501-686-8203,501-526-6729,sylensing@uams.edu,Determination of sample size for demonstrating efficacy of radiation countermeasures,2,Ralph,L,Kodell,University of Arkansas for Medical Sciences,Shelly,Y,Lensing,University of Arkansas for Medical Sciences,Reid,D,Landes,University of Arkansas for Medical Sciences,K. Sree,,Kumar,Armed Forces Radiobiology Research Institute,Martin,,Hauer-Jensen,University of Arkansas for Medical Sciences,,,,,,,,,,,,,,,,,,,,,"In response to the ever increasing threat of radiological and nuclear terrorism, active development of non-toxic drugs and other countermeasures to protect against or mitigate adverse radiological health effects is ongoing. Although the classical LD50 study used for decades in preclinical toxicity testing has been largely replaced by experiments with fewer animals, the need to evaluate the radioprotective efficacy of new drugs necessitates the conduct of traditional LD50 comparative studies (FDA, 2002). However, no readily available method exists for determining the number of animals needed to establish efficacy in such studies. We derived a sample-size formula for comparative potency testing of response modifiers in total body irradiation experiments incorporating elements of FDA's requirements for robust efficacy data where human studies are not ethical or feasible. Monte Carlo simulation demonstrated the formula's performance for Student's t, Wald, and Likelihood Ratio tests in logistic and probit models. Results showed clear potential for justifying use of substantially fewer animals than customarily studied. This work may thus initiate a dialogue among researchers using animals for radioprotection survival studies, institutional animal care and use committees, and drug regulatory bodies to arrive at a legitimate number of animals needed for statistically robust results.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,TRUE,T2 and T3 (options above don't match tutorials),contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Power analysis/sample size,Defense and national security applications,rmcnally@celgene.com,,Richard McNally,,Celgene Corporation,"9900 W. 109th St., Suite 300",913-266-0510,,rmcnally@celgene.com,A Phase 2 Clinical Trials with Adaptation on 2 Factors,1,Richard,J,McNally,Celgene Corporation,David,,McKenzie,Celgene Corporation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A clinical trial is proposed with the dual objectives of finding both an effective and tolerable dose of a treatment and the optimal treatment schedule (e.g., 10, 15, or 20 days of treatment in a 28-day cycle) that will be used in a confirmatory study. Due to the large number of design points, with each point being a particular dose/schedule combination, an adaptive design is proposed. To find the best dose/schedule, techniques are discussed that combine elements from Bayesian dose-ranging studies and response surface methodology. We develop schemes for allocating patients to each design point at each study stage. We also develop rules for stopping the study, either for efficacy or futility, and selecting the best dose/schedule combination. Simulations are presented to show the operating characteristics of each design under both the universal null hypothesis (i.e., a flat response surface) and various alternatives. The study can be run independently or implemented as the 'learning' phase of a seamless phase 2/3 trial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Clinical trials,fhahne@fhcrc.org,,Florian Hahne,PhD,Fred Hutchinson Cancer Research Center,M2-B876,(+1) 206 6673148,(+1) 206 6673148,fhahne@fhcrc.org,Bioconductor tools for high-throughput flow-cytometry data analysis,1,Florian,M,Hahne,Fred Hutchinson Cancer Research Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Automation technologies developed during the last several years have enabled the use of flow cytometry high content screening (FC-HCS) to generate large, complex datasets in both basic and clinical research applications. A serious bottleneck in the interpretation of existing studies and the application of FC-HCS to even larger, more complex problems is that data management and data analysis methods have not advanced sufficiently far from the methods developed for applications of flow cytometry (FCM) to small-scale, tube-based studies. Some of the consequences of this lag are difficulties in maintaining the integrity and documentation of extremely large datasets, assessing measurement quality, developing validated assays, controlling the accuracy of gating techniques, automating complex gating strategies, and aggregating statistical results across large study sets for further analysis. In this presentation, we introduce a range of computational tools developed in Bioconductor that enable the automated analysis of large flow cytometry data sets, from the initial quality assessment to the statistical comparison of the individual samples.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Applied data analysis,Flow Cytometrycuiy@msu.edu,,Yuehua Cui,Assistant Professor,Michigan State University,A432 Wells Hall,517-432-7098,,cuiy@msu.edu,Nucleotide mapping complex disease and the limiting distribution of the likelihood ratio test,1,Yuehua,,Cui,"Department of Statistics and Probability, Michigan State University",Dong-Yun,,Kim,"Department of Statistics, Virginia Tech",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Detecting the pattern and distribution of disease variants across thegenome is essential in understanding the etiology of complex diseasein human. Statistical methods based on single nucleotide polymorphism(SNP) or haplotype analysis have been developed. Different from thesemethods, we propose a nucleotide-based mapping approach which canestimate and test the effect of a risk haplotype on a disease risk.The model is developed under the mixture model framework whichpresents challenges in assessing statistical significance when usingthe traditional likelihood ratio test (LRT). The widely usedpermutation tests can be applied to asses the statisticalsignificance, but is computationally intensive. Here we study thelimiting distribution of the LRT under the proposed framework and showthat the distribution of the LRT asymptotically follows a chi-squaredistribution under the null of no association. Simulation studies showthat the asymptotic chi-square distribution performs well in finitesamples with disease trait following an exponential family distribution.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Infectious disease models,Genetic association studyblu@cph.osu.edu,,Bo Lu,Assistant Professor,The Ohio State University,"B110 Starling-Loving Hall, CPH",6142933906,,blu@cph.osu.edu,Efficiency of Study Designs in Diagnostic Randomized Clinical Trials,1,Bo,,Lu,The Ohio State University,Constantine,,Gatsonis,Brown University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"From the patients' management perspective, a good diagnostic testshould contribute to both reflecting the true disease status andimproving clinical outcomes. Two study designs for the randomizedclinical trial--the two-arm design and the paired design--arecompared in the evaluation of diagnostic tests with patient outcomesas the primary endpoint. In the conventional two-arm design,patients are randomized to one of the diagnostic tests. In thepaired design, patients undergo both tests and randomization occursin the patients with discordant test results. Treatment will beapplied based on test results. The follow-up clinical outcomes willbe measured to determine the prognostic value of the tests. Thepaired design is shown to be more efficient than the two-arm designwhen the operating characteristics of the tests are given. Theefficiency gain depends on the discordant rate of test results.Estimation of important quantities under the paired design isderived and simulation studies are also conducted to verify thetheoretical results. The method is illustrated with an example of designing a randomized study on preoperative staging of bladder cancer. ",FALSE,FALSE,T5: Analysis of Censored Cost or Health Outcomes Data,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Diagnostic and screening tests,Experimental design,pavan@ime.usp.br,,Julia Maria Pavan Soler,Professor,UNiversity of Sao Paulo,Rua Leonardo Mota 100 Apt 32,55-11-30916129,55-11-3091-6130,pavan@ime.usp.br,Use of the Item Response Theory to Evaluate Gene Expression in Congenic Rat Strains,1,Julia,P,Soler,"University of Sao Paulo, Brazil",Carlos,E,Neves,"University of Sao Paulo, Brazil",Suely,R,Giolo,"Federal University of Parana, Brazil",Dalton,F,Andrade,"Federal University of Santa Catarina, Brazil",Mariza,,de Andrade,"Division of Biostatistics, Mayo Clinic, MN, USA",Ayumi,A,Miyakawa,"Heart Institute, University of Sao Paulo, Brazil",Jose,E,Krieger,"Heart Institute, University of Sao Paulo, Brazil",,,,,,,,,,,,,"In the new field of genetical genomics, researches has focused attention in exploring the quantitative genetic of gene expression and also in relating transcriptional variation to variation in clinical traits. In this work, we explore the potential of this field on the analysis of a microarrays data set which considered measurements of gene expression of 35,129 fragments evaluated in samples from kidney tissue extracted from 5 rat strains (Spontaneously Hypertensive Rat (SHR) and four congenic rat strains) evaluated on two conditions of salt exposition (absence and presence of NaCl). The congenic rat strains were derived for 4 previously mapped blood pressure Quantitative Trait Loci (QTLs) in chromosomes 2 (two positions), 4, and 16. Thus, these animals have specific modifications in the genome that probably generate variations on the phenotypic blood pressure as well as on the gene expression pattern. We considered two strategies to analyze the microarrays data: a classical approach in terms of MAANOVA (MicroArrays Analysis of Variance), and a more robust strategy through Item Response Theory (ITR). Under the ITR formulation we fitted the Rach and Samejima models, considering genes as individuals and the rat groups as items. As a result, we classify the genes in terms of their expression probability in each rat strain.             ",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Microarray analysis,Biostatisticsgklein@stat.cmu.edu,,Gary Klein,Student,Carnegie Mellon University,"Department of Statistics, Baker Hall",8184001916,,gklein@stat.cmu.edu,Characterizing patterns of treatment utilization for youth with ADHD,1,Gary,,Klein,Carnegie Mellon University,Joel,,Greenhouse,Carnegie Mellon University,Abigail,,Schlesinger,"Western Psychiatric Institute and Clinic, University of Pittsburgh",Bradley,,Stein,"Western Psychiatric Institute and Clinic, University of PittsburghCommunity Care Behavioral Health OrganizationRAND Corporation",,,,,,,,,,,,,,,,,,,,,,,,,"The primary treatments for Attention Deficit/Hyperactivity Disorder(ADHD) in school-aged children are medication and behavioralinterventions.  Using two sources of Medicaid claims data, behavioralhealth claims and retail pharmacy claims, we characterize thelong-term utilization of these treatments in a more representativepopulation than is typically found in clinical trials data.  Methodsused include survival analysis, multi-state Markov models, andtrajectory analysis.At the outset, we discuss some of the difficulties of working withthese types of administrative data, especially for an inexperiencedresearcher and clinician trying to establish a collaborativerelationship. These include the understanding, merging, and cleaningof two very different databases as well as defining suitable outcomesand covariates which may not be easily extrapolated from the given data.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Health services research,Longitudinal data,rfeng@ms.soph.uab.edu,,Rui Feng,Assistant Professor,University of Alabama at Birmingham,RPHB 327C,205-975-9194,,rfeng@ms.soph.uab.edu,Analysis of Twin Data Using SAS,1,Rui,,Feng,University of Alabama at Birmingham,Gongfu,,Zhou,Yale University,Meizhuo,,Zhang,Yale University,Heping,,Zhang,Yale University,,,,,,,,,,,,,,,,,,,,,,,,,"Twin studies are essential for assessing disease inheritance. Data generated from twin studies are traditionally analyzed using specialized computational programs. For many researchers, especially those who are new to twin studies, understanding and using those specialized computational programs can be a daunting task. Given that SAS is the most popular software for statistical analysis, we suggest the use of SAS procedures for twin data may be a helpful alternative and demonstrate that we can obtain similar results from SAS to those produced by specialized computational programs. This numerical validation is practically useful, because a natural concern with general statistical software is whether it can deal with data that are generated from special study designs such as twin studies and whether it can test a particular hypothesis. We conclude through our extensive simulation that SAS procedures can be used easily as a very convenient alternative to specialized programs for twin data analysis. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Applied data analysis,Statistical genetics,haihong_li@vrtx.com,,Haihong Li,,Vertex Pharmaceuticals,130 Waverly St,617-444-7294,,haihong_li@vrtx.com,Comparison of variations of the Duffy-Santner confidence intervals for the one-sample proportion based on multistage designs,1,Haihong,,Li,Vertex Pharmaceuticals,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There exist several methods to calculate the exact confidence intervals for the one-sample proportion from multistage designs.  One of them, which extends Crows method from single stage to multistage design, was propose by Duffy and Santner.  We compared several variations of this method.  It turned out that the 'keeping left' method has favorable properties compared with the original DS method.  Four Fleming 3-stage plans and a Simon 2-stage plan were used for comparison.  In most cases, the 'keeping left' method resulted in CIs with smaller total lengths and expected lengths, and coverage probabilities closer to the nominal level.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Adaptive design/adaptive randomization,Exact methods,zhang@stat.ncsu.edu,,Daowen Zhang,Professor,North Carolina State University,"2501 Founder's Drive, 220 Patterson Hall",919-515-1933,,zhang@stat.ncsu.edu,Variable selection in additive mixed models for longitudinal data,1,Daowen,,Zhang,North Carolina State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In longitudinal studies with a potentially large number ofcovariates, investigators are often interested in identifyingimportant variables that are predictive of the response. Suppose we can a priori divide the covariates into two groups: one where parametriceffects are adequate and the other where nonparametric modeling isrequired. In this research, we propose a new method to simultaneouslyselect important parametric covariates and nonparametric covariates inadditive mixed models for longitudinal data. Simulation will be used toevaluate the performance of the new method and a real data analysisis used to illustrate its application.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Nonparametric methods,tang@stt.msu.edu,,Xiaoqin Tang,,Michigan State University,"B630 West Fee Hall, Michigan State University",5178985523,,tang@stt.msu.edu, Analysis of Duration Times with Unobserved Heterogeneity through Finite Mixtures,1,Xiaoqin,,Tang,"Department of Statistics and Probability,Michigan State University",Hwan,,Chung,"Department of Epidemiology,Michigan State University",Joseph,,Gardiner,"Department of Epidemiology,Michigan State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In health services research, hospital length of stay (LOS) is oftenused as a proxy for health care utilization. Sometimes LOSdistributions exhibit heterogeneity that cannot be adequately fittedthrough standard parametric models (eg, Log normal, Gamma) orexplained by observed patient variables. Similar concerns are alsopresent with time to event data. We use a latent Markov model toprovide a set of principles for systematic identification ofhomogeneous stages of hospital duration of stay or failure time anddescribe their stage-sequential process. Using methods from latentclass analysis we make inference on the number of latent classes andtheir membership probabilities. A Bayesian estimation method viaMarkov chain Monte Carlo (MCMC) is developed as an alternative totraditional maximum-likelihood (ML) based methods. We illustrate theuse of our strategy and compare it to ML in the context of asimulation study. We also demonstrate its application with survivaltimes of patients who underwent heart transplantation. The strategyworks well and can be implemented in SAS software.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Latent variables,Bayesian methods,rchagant@odu.edu,,N. Rao Chaganty,Professor,Old Dominion University,Dept. of Math and Stat,7575606002,7576833885,rchagant@odu.edu, A mixture model for the analysis of correlated binomial data,1,N. Rao,,Chaganty,Old Dominion University,Yihao,,Deng,Indiana University-Purdue University Fort Wayne,Roy,,Sabo,Virginia Commonwealth University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Analysis of correlated binary data has beenthe topic of numerous papers during the last two decades. However, not much attentionwas given to the analysis of correlated binomial data. The analysis is complicatedbecause there are many parameters in the usual multivariate models. Further the ranges of the correlation parameters, being functions of the marginal means, are subject  to unmanageable constraints.  In this talk we will discuss a mixture model, which circumvents those complications, allows a wide range of dependence and  requires only specification of the first two moments for the mixing distribution. We will discuss estimation of the parameters and an illustrative example.  ",FALSE,FALSE,,FALSE,FALSE,TRUE,"I can attend the conference only on Monday and Tuesday morning session.I would appreciate it if you could schedule my talk on Monday. Thank you.",oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Latent variables,Binary dataWenjun.Li@umassmed.edu,,Wenjun Li,Assistant Professor,Univ of Mass Medical School,"Preventive & Behavioral Medicine, Shaw SH2-230",508 856 6574,508 856 4543,Wenjun.Li@umassmed.edu,Should auxiliary variables with measurement error be used in the estimation of population mean based on survey samples?,1,Wenjun,,Li,"University of Massachusetts Medical School Division of Preventive and Behavioral Medicine Shaw Building, SH2-230 55 Lake Avenue North Worcester, MA  01655 ",Edward,J.,Stanek III,"Department of Public HealthBiostatistics & EpidemiologyUniversity of Massachusetts404 Arnold House715 N. Pleasant StreetAmherst, MA 01003-9304",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Auxiliary information is commonly used to improve the precision of estimators of population quantities in sample surveys.  However, auxiliary variables may be measured with error.  We extend the random permutation model proposed by Stanek, Singer and Lencina (2004) to obtain best linear unbiased estimators of a finite population mean in situations where auxiliary information is available but measured with error in the sample under simple random without replacement sampling (SRS).  The variance of the estimator is inflated by measurement error, in particular among scenarios where the correlation coefficient between the outcome and auxiliary covariates are relatively strong and the sampling fraction is high.  For a modest reliability of auxiliary variates (0.75) and a strong correlation, the variance inflation is nearly 2.5 fold.  When compared to simple estimator, the potential variance reduction due to adjustment diminishes quickly with lower reliability of the auxiliary variable.  When reliability is lower than 0.5, meaningful variance reduction is unlikely even when the correlation between outcome and the auxiliary variable is strong (rho=0.9), in particular when sampling fraction is high.  The results of this analysis are used to guide the choice of auxiliary variables in the estimation of community walkability in a study of Boston neighborhoods. ",FALSE,FALSE,T1: Competing Risks,FALSE,TRUE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Survey research data,Measurement error,xu_ruifeng@yahoo.com,,Ruifeng,,"Merck & Co., Inc.",UG-1C60,267-305-1352,,xu_ruifeng@yahoo.com,Measurement Error in Longitudinal Data without Validation Samples,1,Ruifeng,,Xu,"Merck & Co., Inc.",Jun,,Shao,"Department of StatisticsUniversity of Wisconsin - Madison",Mari,,Palta,"Department of Population Health SciencesUniversity of Wisconsin - Madison",Zhiguo,,Xiao,"Department of StatisticsSchool of Management, Fudan University",,,,,,,,,,,,,,,,,,,,,,,,,"Measurement error in covariates has received considerable attention in the biostatistics literature. Longitudinal data allow correction for errors in covariates in linear models, even when true replicate measurements or validation samples are not available. Wansbeek (2001) proposed a generalized method of moments (GMM) framework for analyzing longitudinal data with measurement error in a single regressor. We generalize his method to the case of uneven length of follow-up and the case where more than one covariate are measured with errors. The methods are applied to the Wisconsin Sleep Cohort Study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Measurement error,Longitudinal data,fhu@smu.edu,,Fan Hu,,Southern Methodist University,5555 Amesbury Dr,214-768-2450,,fhu@smu.edu,Sample Size Calculation for Clustered Binary Outcomes with Sign Tests,1,Fan,,Hu,"Department of Statistical Science, Southern Methodist University, Dallas, TX",William,,Schucany,"Department of Statistical Science, Southern Methodist University, Dallas, TX",Chul,,Ahn,"Department of Clinical Sciences, UT Southwestern Medical Center, Dallas, TX",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a sample size calculation for testing proportions using the sign test when binary outcomes are dependent within clusters. A sample size formula is derived using the test statistic of Datta and Satten (2008) for clustered binary outcomes accounting for the variability due to cluster size. A simulation study is conducted to evaluate the performance of the proposed sample size formula in terms of empirical type I errors and powers. This simulation study shows that empirical type I errors and powers are close to the nominal levels when the number of clusters is greater than 10. Our simulation study also shows that the number of clusters required increases as the imbalance in cluster size increases.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Clustered data methods,Clinical trials,shen_lei@lilly.com,,Lei Shen,Sr. Research Scientist,Eli Lilly and Company,Lilly Corporate Center DC-0734,(317)651-1762,(317)651-9964,shen_lei@lilly.com,Winner's Curse and Bias Correction in Genome-wide Association and Candidate Gene Studies,1,Lei,,Shen,Eli Lilly and Company,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recently there has been increasing attention to the problem induced by multiplicity in large-scale analyses known as the Winner's Curse, referring to the over-estimation of effect sizes of the top candidate biomarkers inferred by the analysis, which can be regarded as a form of selection bias.  In this talk, we focus on genome-wide association and candidate gene studies, in which a large number of potential genetic biomarkers are routinely screened for their associations with clinical endpoints.  Typically additional investment, often in the form of follow-up studies, is made for the top candidate biomarkers.  We study the severity of the bias and its implications to a clinical program of drug development, such as resource allocation, business decisions and planning of confirmatory studies, in some typical settings.  We then look closely at potential questions of interest and determine their relevance to pharmaceutical research.  Finally we investigate some methods to correct for the selection bias and compare their performance.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Multiple testing,ssmehta@purdue.edu,,Shraddha Mehta,,Purdue University - Department of Statistics,2252 US Hwy 52 West Apt F-1,612-819-7615,,ssmehta@purdue.edu,Bayesian Hazard Rate Estimation and Sufficient Dimension Reduction,1,Shraddha,S,Mehta,"Department of Statistics, Purdue University, West Lafayette, USA",Surya,T,Tokdar,"Department of Statistics, Carnegie Mellon University, Pittsburgh, USA",Jayanta,K,Ghosh,"Department of Statistics, Purdue University, West Lafayette, USA and Division of Theoretical Statistics and Mathematics, Indian Statistical Institute, Kolkata, India",Bruce,A,Craig,"Department of Statistics, Purdue University, West Lafayette, USA",,,,,,,,,,,,,,,,,,,,,,,,,"Logistic Gaussian process priors have been used in nonparametric Bayesian density estimation and sufficient dimension reduction in a density regression setting. In this talk, I will describe the extension of this approach to survival modeling with right-censored data. The method simultaneously estimates the covariate subspace as well as the hazard rate given this subspace.  As a result, this method overcomes the dimensionality problem and the inability to estimate non-linear covariate effects when using the traditional proportional hazards model. In addition, sufficient dimension reduction approach provides the most parsimonious representation of the covariate space in relation to the hazard. Currently, the hazard rate is estimated using a partial likelihood approach. Simulation studies are used to compare this method, random survival forests, and Coxs proportional hazards model using variable importance measures and estimates of treatment effect. Future work will utilize the full-likelihood instead of partial likelihood in estimating the hazard rate.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Bayesian methods,xyqiao@email.unc.edu,,Xingye Qiao,,UNC-Chapel Hill,Hanes Hall CB# 3260,919 923 3723,,xyqiao@email.unc.edu,A new approach to high dimensional variable selection,1,Xingye,,Qiao,"Department of Statistics and Operations Research, UNC-Chapel Hill",Yufeng,,Liu,"Department of Statistics and Operations Research, UNC-Chapel Hill",J.S.,,Marron,"Department of Statistics and Operations Research, UNC-Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Popular methods of variable selection, such as SAM, focus on each variable individually. However, some heuristic data examples show that improvement is available by testing on multiple variables simultaneously. We propose an innovative approach to variable selection based on testing significance of variables in a moving window with varying centers and sizes, i.e. a multi-scale approach.  Re-ordering of the variables according to the summarized score of each variable is taken to improve the ranking of the variables. A new visualization gives a convenient summary of the test results.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Recent Advances on Feature Selection and Its Applications (Invited Session)T3: Genetic and Microarray Data AnalysisMonday 1:45-3:30 pm (Tutorials)",presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Microarray analysis,Variable subset selection/model selection,Multiple testing (third appro.)fthomas@utmem.edu,,Fridtjof Thomas,Dr,University Of Tennessee Health Science Center,"66 N Pauline, Suite 633",901-448-6461,,fthomas@utmem.edu,A Bayesian Change-point Algorithm for Detecting Copy Number Alteration,1,Fridtjof,,Thomas,University of Tennessee Health Science Center,Stanley,,Pounds,St. Jude Children's Research Hospital,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recent technical developments have made it possible to collecthigh-resolution genomics data using single nucleotide polymorphism(SNP) arrays. These arrays can be used in a paired data context tocompare cancer tissue to normal samples in an effort to identifyregions of genomic amplification or deletion. Such regions potentiallycontain oncogenes or tumor suppressor genes and are therefore ofparticular interest.However, using SNP array signals to identifying regions of copy numberalteration is a challenging task due to the properties of the derivedmeasurements. We apply a Bayesian change-point algorithm topre-normalized signals from SNP microarrays obtained from a set ofleukemia samples in an effort to infer regions of copy numberalteration and compare this approach to other approaches currently inuse for this purpose.The Bayesian change-point algorithm detects multiple change-pointswhere a change can be in the mean of the subsequent measurements, intheir variance, in their autocorrelation structure, or in acombination of two or all of these aspects.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Bayesian methods,baileyk@mayo.edu,,Kent R. Bailey,Professor of Biostatistics,Mayo Clinic,200 SW 1st Street,507-284-5581,507-284-9542,baileyk@mayo.edu,Models with multiple event types and their predictions,1,Kent,R,Bailey,Division of Biomedical Statistics and iInformatics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Longitudinal follow-up studies often record and analyze more than one event type, for example, myocardial infarction, stroke, and death, leading to multiple right-censored observations per individual.  Methods have been proposed to treat these events simultaneously in a multivariate mode, for example, Wei Lin and Weissfeld (1989 JASA)  We consider a simple procedure utilizing the proportional hazards framework, in which each event type is treated as a separate observation, leading to kN observations, where k is the number of event types, and N the number of observations.  Stratification on event type allows fitting a rich variety of models with various degrees of linkage between the predictive models for the different events.  We then develop a method whereby the resulting marginal models' predictions can be combined, by strategic use of the probit transformation, and construction of a multiply right-censored multinormal likelihood.  We apply this to a large Percutaneous Coronary Intervention Database to illustrate the features of this approach.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Multivariate survival,simac@mskcc.org,,Cami Sima,,Memorial Sloan-Kettering Cancer Center,"307 E 63rd St, 3rd Floor",6467358107,,simac@mskcc.org,Optimal Cutpoint Estimation with Censored Data,2,Mithat,,Gonen,Memorial Sloan-Kettering Cancer Center,Cami,,Sima,Memorial Sloan-Kettering Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider the problem of selecting an optimal cutpoint for acontinuous marker when the outcome of interest is subject to rightcensoring. Maximal chi square methods and receiver operatingcharacteristic (ROC) curves-based methods are commonly used when theoutcome is binary. In this article we show that selecting the cutpointthat maximizes the concordance, a metric similar to the area under anROC curve, isequivalent to maximizing the Youden index, a popular criterion whenthe ROC curve is used to choose a threshold. We use this as a basisfor proposing maximal concordance as a metric to use with censoredendpoints. Through simulations we evaluate the performance of twoconcordance estimates and three chi-square statistics under variousassumptions. Maximizing the partial likelihood ratio test statistichas the best performance in our simulations.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,ROC analysis,Survival analysis,gzou@robarts.ca,,G Y Zou,,University of Western Ontario,Dept of Epidemiology & Biostatistics,519-661-2111X86298,,gzou@robarts.ca,An alternative approach to meta-analysis with confidence,1,G Y,,Zou,University of Western Ontario,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Research results in primary studies are now usually reported in the format of effect point estimates and associated confidence intervals. The purpose of this presentation is to discuss a general meta-analytic confidence interval procedure that relies only on the individual confidence intervals reported in primary studies. It is shown that the lower margin of error for the overall effect size is given by the mean root sum squares of individual lower margins, while the upper margin of error is given by the mean root sum squares of individual upper margins. Simulation results for several common effect measures demonstrate that this simple procedure performs well, even when the effect sizes are heterogeneous, the number of studies is small, and the effect size estimates have skewed distributions. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Applied data analysis,Mohamed.Alosh@fda.hhs.gov,,Mohamed,,FDA,10903 New Hampshire Ave,301-796-0844,,Mohamed.Alosh@fda.hhs.gov,A comparison of imputation methods in a longitudinal clinical trial count data,1,Mohamed,,Alosh,"Division of Biometrics III, OB, OTS, FDA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Despite substantial efforts to follow each patient, missing data remains a common problem in longitudinal clinical trials. In this presentation we compare methods for handling missing data in a clinical trial with repeated evaluations for actinic keratosis lesion counts. One approach uses a predictive mean response from an extended integer valued autoregressive INAR(1) model. A second approach is based on the inverse probability weighting for handling missing data.  These two methods are contrasted with last observed value carried forward (LOCF) and the complete case analyses in a simulation study. Missing data were simulated using an approach consistent with the missing data patterns found in the original data, where missingness is expected to be related to some covariates in the model.Key Words: missing data, model dependent imputation, INAR(1) model",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Missing data,xlu2@phhp.ufl.edu,,Xiaomin Lu,Assistant Professor,University of Florida,Department of Epidemiology &  Biostatistics,3522737920,,xlu2@phhp.ufl.edu,Semiparametric Estimation of Treatment Effect with Time-Lagged Response in the Presence of Informative Censoring,1,Xiaomin,,Lu,University of Florida,Anastasios,,Tsiatis,North Carolina State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In many randomized clinical trials, the primary response variable, forexample, the survival time, is not observed directly after thepatients enroll in the study but rather observed after some period oftime (lag time). It is often the case that such a response variable ismissing for some patients due to censoring that occurs when the studyends before the patient's response is observed or when the patientsdrop out of the study. It is often assumed that censoring occurs atrandom which is referred to as noninformative censoring; however, inmany cases such an assumption may not be reasonable. If the missingdata are not analyzed properly, the estimator or test for thetreatment effect may be biased. In this paper, we use semiparametrictheory to derive a class of consistent and asymptotically normalestimators for the treatment effect parameter which are applicablewhen the response variable is right censored. The baseline auxiliarycovariates and post-treatment auxiliary covariates, which may betime-dependent, are also considered in our semiparametric model. Theseauxiliary covariates are used to derive estimators that both accountfor informative censoring and are more efficient then the estimatorswhich do not consider the auxiliary covariates.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Joint models for longitudinal and survival data,ligengxi@stt.msu.edu,,Gengxin Li,,Bio-Statistics (QTL mapping),808A Cherry lane,517-599-3360,,ligengxi@stt.msu.edu,Mapping imprinted quantitative trait loci underlying endosperm trait in flowering plant: a variance component approach,1,Gengxin,,Li,Bio-Statistics: Mapping iQTL,Yuehua,,Cui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genomic imprinting has been thought to play an important role in the development in flowing plant. Empirical studies have shown that some economically important endosperm traits are genetically controlled by imprinted genes. However, the exact number and location of the imprinted genes are largely unknown due to the lack of efficient statistical mapping methods. Methods developed for diploid population can not be directly applied due to the unique triploid inheritance structure of the endosperm genome. Here we propose a statistical variance component framework by utilizing the nature of sex-specific alleles shared identical-by-decent among sibpairs in experimental crosses to map imprinted quantitative trait loci (iQTL) underlying endosperm traits. We propose a new variance component partition method based on the nature of the triploid inheritance pattern and develop an efficient restricted maximum likelihood estimation method in a genome-wide interval scan for estimating and testing the effects of iQTL. Cytoplasmic maternal effect which is believed to have primary influences on yield and grain quality is also considered when testing for genomic imprinting. Extension to multiple QTL analysis is proposed. Both simulation study and real data analysis indicate good performance and powerfulness of the developed approach.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Applied data analysis,ekaizar@stat.osu.edu,,Eloise Kaizar,Assistant Professor,Ohio State University,Department of Statistics,614-247-2585,,ekaizar@stat.osu.edu,Generalizing Data from Randomized Trials,1,Eloise,,Kaizar,Ohio State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Randomized controlled trials are often thought of as the gold standardof evidence in medicine because they offer strong internal validity.However, subject recruitment may introduce selection bias that limitstrials' external validity.  We describe and illustrate an approachthat relies on the often smaller selection bias seen in observationaldata to make what we call generalizability judgments for trials. Further, we examine a simple framework for combining randomized andobservational data to model and adjust for the selection bias of arandomized study.  We consider the approximations this model requiresfor the structure of the data under a simple linear model, and exploreits feasibility for real applications.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Causal inference,gorman@srph.tamhsc.edu,,Dennis M. Gorman,,Texas A&M Health Science  Center,"School of Rural Public Health, TAMHSC",979-458-8059,,gorman@srph.tamhsc.edu,"Geospatial Models of Alcohol, Drugs and Violence",1,Dennis,M,Gorman,"Department of Epidemiology & Biostatistics, School of Rural Public Health, Texas A&M Health Science Center ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Ecologic studies have shown a relationship between alcohol outlet densities and violence and between the location of illicit drug markets and violence.  Most of the former studies have been conducted by researchers in the fields of alcohol studies and public health and have employed some geographic space (such as a census bock group or track) as the unit of analysis.  The major theoretical models used by researchers in this area include social disorganization theory, social capital theory, and collective efficacy theory.  The second group of studies has mainly been conducted by criminologists and typically uses point data pertaining to specific places (or clusters of places) as the unit of analysis.  The major theoretical models used by researchers in this area include routine activities theory and hot-spot theory. Both sets of studies present theoretical, methodological and statistical challenges.  These are reviewed in this introductory presentation so as to provide a context for the analysis of spatial models of alcohol and drug availability and violence.  Particular emphasis is placed on examining the extent to which the theories that are used in this research are truly spatial and ecologic.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Epidemiologic methods,cdi@jhsph.edu,,Chongzhi Di,,Johns Hopkins University,"615 N. Wolfe St, E3034",443-827-2486,,cdi@jhsph.edu,Penalized Likelihood Ratio Test When Some Parameters are Present Only Under the Alternative,1,Chongzhi,,Di,"Department of Biostatistics, Johns Hopkins Hopkins University",Kung-Yee,,Liang,"Department of Biostatistics, Johns Hopkins Hopkins University",Ciprian,M,Crainiceanu,"Department of Biostatistics, Johns Hopkins Hopkins University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider hypothesis testing problems where some parameters are present only under the alternative hypothesis. Examples include testing the existence of a change point, testing the number of components in finite mixture models, and testing linkage in genetic epidemiology. The likelihood ratio test (LRT) statistic does not follow a conventional chi-square distribution in these problems, due to nonidentifiability. In this paper, we extend the modified/penalized likelihood ratio test, proposed by Chen et al. (2001) in finite mixture models, to a wider class of problems and obtain its asymptotic null distribution and power. Simulation studies suggest that asymptotic results are accurate in small and moderate samples and are insensitive to the magnitude and the functional form of the penalty. We apply the proposed method to a data set testing the null hypothesis of linearity versus a nonlinear alternative.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Generalized linear models,statistical theory and inferencemklein1@umbc.edu,,Martin D Klein,,"University of Maryland, Baltimore County",12140A Heneson Garth,410-963-0192,,mklein1@umbc.edu,A statistical perspective of DNA-protein cross-links (DPX) data,1,Martin,,Klein,"University of Maryland, Baltimore County",Bimal,,Sinha,"University of Maryland, Baltimore County",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"It is generally believed that the DNA-protein cross-links (DPX) in the nasal mucosa of species is formed by formaldehyde inhalation.  DPX data available from the nasal mucosa of rats and rhesus monkeys have been used as a measure of tissue dose in cancer risk assessments for formaldehyde. Using an appropriate modeling of the available DPX data observed on rats and rhesus monkeys, prediction of human nasal mucosa DPX resulting from formaldehyde inhalation can be done based on a meaningful extrapolation using species-specific covariates. In this talk several statistical aspects of such an existing model (Hubal et al. (1997), Conolly et al. (2000)) are further explored and improved inference about the model parameters is drawn.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Nonlinear models,Environmental and ecological applications,aixiang.jiang@vanderbilt.edu,,Aixiang Jiang,,Vanderbilt University,571 Preston Research Building (6848),615-936-7360,,aixiang.jiang@vanderbilt.edu,Rank-based similarity metric with tolerance,1,Aixiang,,Jiang,Vanderbilt University,Yu,,Shyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many similarity metrics are available to measure the degree of similarity of two variables. Multivariate analyses such as dimension reduction, clustering, factor analysis, and classification all rely on similarity metrics or relative measurement distances.  This implies that similarity metrics are crucial in high-dimensional data analysis. Among available similarity metrics, the rank-based Spearman correlation coefficient is the most robust, with the ability to overcome outlier or leverage effect. However, ranks themselves might artificially expand the real difference when two numbers are actually very close. To keep the robustness of the Spearman correlation coefficient, but make some correction to its drawback and also provide researchers with a method for calculating loose correlation coefficient with customized tolerance, we created a new similarity metric based on ranks with a tolerance. Both simulated and real data are used to illustrate our new metric.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Applied data analysis,Nonparametric methods,rhoffmann@mcw.edu,,Ray Hoffmann,Professor,Medical College of Wisconsin,Quatitative Health Sciences,414-955-7634,414-955-6331,rhoffmann@mcw.edu,Dirichlet Process Models for Changes in fMRI Visual Field,1,Raymond,G,Hoffmann,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",Pippa,,Simpson,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",Shun-Hwa,,Li,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",Ke,,Yan,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",Edgar,A,DeYoe,"Department of RadiologyMedical College of Wisconsin",Daniel,B,Rowe,"Department of BiophysicsMedical College of Wisconsin",,,,,,,,,,,,,,,,,"The Visual Field Map (VFM) is a circular region that maps the visualcortex to a virtual retina.  The relationship between the dynamicimage presented to the eye and the virtual retina can be used toidentify changes in the visual system.  These changes could be aresult of surgery near the components of the visual system or resultfrom progression of a  chronic  disease.  The visual field map is anonisotropic, nonhomogeneous set of points that represent theactivation of voxels in the visual cortex assessed in an fMRI scanner.A wedge shaped mask (18 to 90 degrees of arc) of the image is used tosimulate the effect of surgical damage.   A Bayesian non-parametricmixture model, a Dependent Dirichlet Process, uses a Dirichlet prioron a space of 2D density functions G to model the intensity of thestochastic process that generates the points in the VFM, an inversegamma on the precision of the distribution and a gamma prior on themixing parameter for the number of the distributions needed to modelthe DDP.  The posterior probability of the DDP model on the diskquantifies the probable location of the wedge-shaped mask compared toa reference scan.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Imaging,Spatial/temporal modeling,seongho.song@uc.edu,,Seongho Song,Assistant Professor,"Department of Mathematical Sciences, University of",839 Old Chem ML:210025,513-556-4047,,seongho.song@uc.edu,Hierarchical Bayesian Analysis of Genetic Diversity in Geographically Structured Populations,1,Seongho,,Song,"Department of Mathematical SciencesUniversity of Cincinnati",Dipak,K.,Dey,"Department of StatisticsUniversity of Connecticut",Kent,E.,Holsinger,"Department of Ecology & Evolutionary BiologyUniversity of Connecticut",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Populations may become differentiated from one another as a resultof genetic drift. The amounts and patterns of differentiation atneutral loci are determined by local population sizes, migrationrates among populations, and mutation rates. We proposed ahierarchical Bayesian model for inference of Wright's$F$-statistics in a hierarchy in which we estimate theamong-region correlation in allele frequencies by substitutingreplication across loci for replication across time based on theexact first two moments of a stochastic model for hierarchicallystructured populations subject to migration, mutation, and drift.As an application, microsatellite human data will be discussed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Hierarchical models,yongtao.guan@yale.edu,,Yongtao Guan,,Yale University,60 College St,203-785-6125,,yongtao.guan@yale.edu,On Consistent Nonparametric Intensity Estimation for Inhomogeneous Spatial Point Processes,1,Yongtao,,Guan,"Division of Biostatistics, Yale University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A common nonparametric approach to estimate the intensity function of aninhomogeneous spatial point process is through kernel smoothing. Whenconducting the smoothing, one typically uses events only in a localset around the point of interest. The resulting estimator, however, isoften inconsistent since the number of events in a fixed set is oforder one for spatial point processes. In this paper, we propose a newcovariate-based kernel smoothing method to estimate the intensityfunction. Our method defines the distance between any two points asthe difference between their associated covariate values.Consequently, we determine the kernel weight for a given event of theprocess as a function of its new distance to the point of interest.Under some suitable conditions on the covariates and the spatial pointprocess, we prove that our new estimator is consistent for the trueintensity. To handle the situation with high-dimensional covariates,we also extend sliced inverse regression, which is a usefuldimension-reduction tool in standard regression analysis, to spatialpoint processes. Simulations and an application to a real data exampleare used to demonstrate the usefulness of the proposed method.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Nonparametric methods,kogle@uwyo.edu,,Kiona Ogle,Assistant Professor,University of Wyoming,"Botany, Dept. 3165",307-766-3219,,kogle@uwyo.edu,Data-model integration for understanding belowground ecosystems,1,Kiona,,Ogle,University of Wyoming,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Developing a mechanistic understanding of belowground ecosystemdynamics (e.g., soil carbon storage and fluxes) is critical tounderstanding whole-ecosystem behavior (e.g., net ecosystem carbonexchange). In particular, soils are important players in the globalcarbon cycle, and developing a quantitative understanding of thebelowground system is critical to forecasting climate change impacts.Significant advances have been made in belowground ecosystem ecology,but several challenges remain. One important problem is the ability torigorously partition the effects of different belowground processesand to identify how they vary across space and time. Modernstatistical and computational tools, combined with field experimentsand ecological process modeling, provide a rigorous approach forreconstructing belowground processes. The approach employs ahierarchical Bayesian (HB) framework that simultaneously analyzesdiverse data sources within the context of process-based models.Process parameters and latent variables are partially constrained byinformation from published studies (priors) and by the underlyingstructure of the ecological process model. An example is presentedthat couples diverse laboratory and field data with soil carbon fluxmodels to partition sources (e.g., plant- vs. microbial-derived, oldvs. new, shallow vs. deep carbon) of soil carbon efflux in a desertecosystem.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Hierarchical models,a.staicu@bristol.ac.uk,,Ana-Maria Staicu,,"Department of Mathematics, University of Bristol","University Walk, Bristol, UK",+44117 9289811,,a.staicu@bristol.ac.uk,Generalized Multilevel Functional Regression,2,Ciprian,M,Crainiceanu,"Johns Hopkins University, USA",Ana-Maria,,Staicu,"University of Bristol, UK",ChongZhi,,Di,"Johns Hopkins University, USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We introduce Generalized Multilevel Functional Linear Models (GMFLM), a novel statistical framework motivated by and applied to the Sleep Heart Health Study (SHHS), the largest community cohort study of sleep. The primary goal of SHHS is to study the association between sleep disrupted breathing (SDB) and adverse health effects. An exposure of primary interest is the sleep electroencephalogram (EEG), which was observed for thousands of individuals at two visits, roughly 5 years apart. This unique study design led to the development of models where the outcome, e.g. hypertension, is in an exponential family and the exposure, e.g. sleep EEG, is multilevel functional data. We show that GMFLMs are, in fact, generalized multilevel mixed effect models. Two consequences of this result are that: 1) the mixed effects inferential machinery can be used for GMFLM and 2) functional regression models can be extended naturally to include, for example, additional covariates, random effects and nonparametric components. We propose and compare two inferential methods based on the parsimonious decomposition of the functional space.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Functional data analysis,Hierarchical models,anfangen@umich.edu,,Peter Larson,Graduate Student,University of Michigan,400 Richlyn Dr,5172701418,,anfangen@umich.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,yonzhang@umich.edu,,Yong Zhang,,"University of Michigan, SPH",Dept. of Biostatistics,7349364018,,yonzhang@umich.edu,Estimation in Hierarchical Models with Incomplete Data,1,Yong,,Zhang,"Univ. of Michigan, Dept. of Biostatistics",Trivellore,E.,Raghunathan,"Univ. of Michigan, Dept. of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Hierarchical models are often used when data are observed at differentlevels and the interaction effects on the outcomes between variablesmeasured at different levels are of interest. Missing data cancomplicate analysis using hierarchical models and can occur at alllevels, in both outcomes and covariates. Ignoring the subjects withmissing data usually leads to biased estimates, yet less attention hasbeen paid to the analysis based on hierarchical models with incompletedata. We use a combination of the EM algorithm and Multiple imputationto develop approximate maximum likelihood estimates of the parametersin hierarchical models, assuming missing at random (MAR) and ignorablemissing mechanism (Rubin, 1976; Little and Rubin, 2002). In this paperwe consider a binary response with missing values as well ascontinuous and binary covariates with missing values at each level.Simulation study is used to demonstrate that our proposed method hasdesirable repeated sampling properties. The method is also applied toa survey data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Hierarchical models,yhuang5@emory.edu,,Yijian Huang,Associate Professor,Emory University,1518 Clifton Rd NE,404-727-2951,,yhuang5@emory.edu,Quantile Regression with Censored Data,1,Yijian,,Huang,Emory University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Quantile regression has developed into a primary statistical methodologyto investigate functional relationship between a response and covariates.This technique has a long history in econometric applications. More recently,it has also been advocated for the analysis of survival data to assessevolving covariate effects. However, with censored data, existing methodstypically require strong assumptions on the censoring mechanism or entailcomplication-plagued algorithms. In this talk, I will discuss several recentadvances on this problem and motivate a new estimation procedure basedon a set of differential equations. A fairly efficient algorithm has beendeveloped for the computation. The proposed estimator is uniformly consistentand converges weakly to a Gaussian process. Inference procedures aresuggested. Simulations show good numerical and statistical performance. Theproposal is illustrated in the application to a clinical study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Estimating equations,yucheng95@gmail.com,,Yu Cheng,Assistant Professor,Department of Statistics,"CL 2734, University of Pittsburgh",4126249033,,yucheng95@gmail.com,Modelling Cumulative Incidences of Dementia and Dementia-free Death Using a Novel Three-parameter Logistic Function,1,Yu,,Cheng,"Department of Statistics,University of Pittsburgh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Parametric modelling of univariate cumulative incidence functions andlogistic models have been studied extensively. However, to the best ofour knowledge, there is no study using logistic models to characterizecumulative incidence functions.  We hence propose a novelparametric model which is an extension of a three-parameter logisticfunction.  The modified model can accommodate various shapes ofcumulative incidence functions and be easily implemented usingstandard statistical software.   The simulation studies demonstratethe good performance of the proposed model when it is correctlyspecified and the robustness of the model when the underlyingcumulative incidence function does not follow the three-parameterlogistic function.  The practical utility of the modifiedthree-parameter logistic model is illustrated using the data from theCache County Study of dementia. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Multivariate survival,b.emma.huang@gmail.com,,Emma Huang,Dr.,CSIRO,306 Carmody Rd,61732142953,61732142900,b.emma.huang@gmail.com,Linkage Map Construction in Integrated Crosses,1,Emma,,Huang,CSIRO Mathematical and Information Sciences,Andrew,,George,CSIRO Mathematical and Information Sciences,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The integrated cross is an exciting new experimental design whichenables genomic regions housing genes of commercial significance to bedetected with far greater precision than previously possible. By usingfour or eight parents rather than a traditional biparental design andbreeding generations through to fixation, these crosses represent anabundance of genetic diversity with the potential for high mappingresolution. CSIRO is currently conducting the world's first integratedcross in wheat. This promises to be instrumental in unlocking thegenetic secrets of agronomic, disease and quality traits of one of theworld's most important domesticated crops. While there are similarities between this project and theCollaborative Cross in mice, a key difference is in the lack ofphysical maps or genome sequence for wheat. Thus a crucial element forthe success of the project is the production of highly accurate DNAmarker maps. We have developed statistical methods and computationaltools to address this challenge. We use two- and three-point haplotypeprobabilities to group and order loci within linkage groups. Thesealgorithms and software have been tested through extensive simulationsand will be applied to 4-way and 8-way cross data as it becomesavailable. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Forestry/agriculture applications,cnwang@umich.edu,,Chia-Ning Wang,PhD student,"Dept. of Biostatistics, University of Michigan, An","1073 Barton Drive, Apt 202",7347096219,,cnwang@umich.edu,A hot-deck multiple imputation procedure for gaps in longitudinal event histories.,1,Chia-Ning,,Wang,"Department of Biostatistics, University of Michigan, Ann Arbor",Roderick,,Little,"Department of Biostatistics, University of Michigan, Ann Arbor",Bin,,Nan,"Department of Biostatistics, University of Michigan, Ann Arbor",Sioban,,Harlow,"Department of Epidemiology, University of Michigan, Ann Arbor",,,,,,,,,,,,,,,,,,,,,,,,,"In many longitudinal cohort studies or clinical trials, subjects areassessed for the transition to an intermediate state and to a finalevent, such as an occurrence of a disease-related non-fatal event anddeath, respectively. In our specific application, the intermediatestate is a measure of menopausal transition based on information onmenstrual cycles, and the final event is the final menstrual period(FMP). The distribution of the occurrence time for the intermediatestate and the distribution of the duration from the intermediate stateto the final event are two primary research interests of the dataanalysis. However, a difficulty arises when some subjects have gaps intheir event history. These gaps can create problems in determiningthe time of transition to the intermediate state, and simpleapproaches such as ignoring gap times or dropping cases with missinggaps have obvious limitations. A better approach is to impute themissing information for the gaps. In this study, predictive meanmatching is used to multiply impute by matching gaps to completelyrecorded histories, conditional on longitudinal characteristics, andthe time of FMP (which may be censored). This procedure is applied toan important data set for assessing various measures of menopausaltransition and FMP.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Markers and surrogate markers,leiqian@ucla.edu,,Lei Qian,,"Biostatistics, UCLA","3110 Sawtelle Blvd, #308",310-689-8656,,leiqian@ucla.edu,Bayesian Mixtures for Modeling the Correlation of Longitudinal Data,1,Lei,,Qian,"Department of Biostatistics, University of California at Los Angeles",Robert,,Weiss,"Department of Biostatistics, University of California at Los Angeles",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In longitudinal data analysis, parametric covariance models relyon strong assumptions, while the unstructured covariance model hastoo many parameters and can not be fit to high dimensionalunbalanced data. We propose two rich families of Bayesiansemi-parametric stationary correlation mixture models. Oneapproach is a convex combination of simple structure correlationmatrices, the second approach models correlations as a convexmonotone B-spline function. We compare our models to each otherand to standard models using DIC, cross-validation and logmarginal likelihood. Simulations show that our model works welland DIC, cross-validation and log marginal likelihood choosesimilar models. We illustrate our methods with an unbalanceddataset of CD4 cell counts.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Bayesian methods,rgd27@mizzou.edu,,Ruixin Guo,,University of Missouri,"306 Hitt St, APT 8F",(573) 882-2172,,rgd27@mizzou.edu,Bayes Factor Consistency in Linear Models,1,Ruixin,,Guo,University of Missouri,Paul,L,Speckman,University of Missouri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Liang et al. (2008) considered Bayesian model selection problem in the linear regression and showed consistency of the Bayes factor by using mixtures of g-priors, where they adapted the idea of Zellner and Siow (1980) to put flat priors on the common parameters and Zellners g prior on the parameters only in the more complex model. In this case, the prior on g must be proper. Marin and Robert (2007) suggested to put Zellners g-prior in each model so that an improper prior on g can be used. Later they suggested to use Jefferys prior for g. In this paper, we adapt their idea to put the g-prior in each model, and we show consistency of the Bayes factor associated with the reference prior for g, which is improper, when model dimensions are fixed. We also discuss the consistency and inconsistency problems for the Bayes factor associated with this improper prior when comparing any model with fixed dimension and a model whose p can grow with n. We obtain consistency and inconsistency depending on the limiting behavior of p/n.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Variable subset selection/model selection,esegawa@uic.edu,,Eisuke Segawa,,University of Illinois at Chicago,1747 W. Roosevelt Rd.  Suite 558,312-413-0478,,esegawa@uic.edu,Three-level mixed effects location scale model,1,Eisuke,,Segawa,Univeristy of Illinois at Chicago,Donald,,Hedeker,Univeristy of Illinois at Chicago,Robin,J.,Mermelstein,University of Illinois at Chicago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Hedeker et. al. (2008) provided an example of a two-level mixed-effects location scale model. In that application, each subject was observed repeatedly up to 40 times; several times within a day over several days. The mixed-effects location scale model allows the variance of random intercept (scale) to vary stochastically over subjects as well as the intercept (location).  However, this two-level model does not distinguish within-day variation from between-day variation: they both comprise the within-subjects variance term. In order to separate the within-day and between-day variation (both are within-subjects), the two-level mixed-effects location scale model is extended to three-levels, treating observations nested within days nested within subjects.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Hierarchical models,Longitudinal data,abel.tilahuneshete@uhasselt.be,,Abel Tilahun,,University Hasselt,Strobanders plien 1,+32484-955-605,,abel.tilahuneshete@uhasselt.be,Selection and Evaluation of Biomarkers using Information Theoretic Approach,1,Abel,,Tilahun,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Universiteit Hasselt",Dan,,Lin,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Universiteit Hasselt",Suzy,,Van Sanden,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Universiteit Hasselt",Ziv,,Shkedy,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Universiteit Hasselt",Ariel,,Alonso,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Universiteit Hasselt",Geert,,Molenberghs,"Interuniversity Institute for Biostatistics and statistical Bioinformatics, Universiteit Hasselt",,,,,,,,,,,,,,,,,"The selection and evaluation of biomarkers plays a vitalrole in the discovery and development of new drugs. This calls fornew and efficient statistical methods that can be used in differentscenarios for the selection and evaluation purposes. While jointmodels have been proposed for several settings involving acombination of normally and non-normally distributed outcomes, theyare cumbersome in the sense of computationally complex and ofproducing validation measures that are, unlike in the Gaussian case,not of an R-squared type (Van Sanden et al. 2007). A way to put theseproblems to rest is by employing information theory, already appliedin the continuous case (Alonso and Molenberghs 2007). In this paper,the information-theoretic approach is applied to the selection andevaluation of genomic markers. Its use is illustrated using casestudies and its performance, relative to existing methods, is alsoassessed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Biomarkers/surrogate markers,Markers and surrogate markers,JGARDINER@EPI.MSU.EDU,,JOSEPH C GARDINER,"DIRECTOR, DIVISION OF BIOSTATISTICS",MICHIGAN STATE UNIVERSITY,B629 WEST FEE,517-353-8623,517-432-1130,JGARDINER@EPI.MSU.EDU,Stochastic Models in Cost-Effectiveness Analysis,1,Joseph,C,Gardiner,MICHIGAN STATE UNIVERSITY,Zhehui,,Luo,RTI International,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Cost-effectiveness analysis (CEA) is a collection of techniques for structuring comparisons between competing interventions. It can inform decision-making by providing means for optimizing health benefits from a specified budget, or finding the lowest cost strategy for a specified health benefit. Markov processes are useful in modeling the dynamics of patient health outcomes as they unfold over time. States of the process represent health conditions or health states. We use a continuous-time finite-state Markov process to incorporate patient costs as they are incurred during sojourn in health states and in transition from one health state to another. By combining these expenditure streams, the net present value is the discounted expected total cost over a specified time period. Other metrics widely used in CEA such as net health benefit, net health cost and the cost-effectiveness ratio, and measures of health benefit such as life expectancy and quality-adjusted life years are defined as functions of expected values.  We outline approaches to estimation of these summary statistics from health outcome and cost data that might be incompletely ascertained in some patients. Regression models are used to incorporate patient-specific demographic and clinical characteristics and their impact on the metrics used in CEA can be assessed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Health policy applications,huebner.marianne@mayo.edu,,Marianne Huebner,Ph.D.,Mayo Clinic,Division of Biomedical Statistics and Informatics,(507) 293 0290,,huebner.marianne@mayo.edu,A multi-step approach to genetic association for asthma characteristics in the Isle of Wight Birth Cohort,1,Marianne,,Huebner,Mayo Clinic,Hasan,,Arshad,"University of Southampton, UK",Eric,,Schauberger,Michigan State University,Karen,,Friderici,Michigan State University,Marsha,,Wills-Karp,Cinncinati Childrens Hospital,Wilfried,,Karmaus,University of South Carolina,Susan,,Ewart,Michigan State University,,,,,,,,,,,,,"A subset of children in the Isle of Wight, UK, 1989-90 birth cohort(n=272) were classified as having asthma based on physician diagnosis,positive reaction allergens by skin prick test, and bronchialhyperresponsiveness. SNPs were ranked by comparing hybridizationintensities in a pooled genome wide association study in theseregions. For the identified genomic region SNPs were genotyped inindividual samples to capture the variation across the region. A multistage survival analysis model was developed to determine thesignificance of SNPs for asthma characteristics and asthma developmentat ages 4, 10 and 18 years.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Survival analysis,dung-tsa.chen@moffitt.org,,Dung-Tsa Chen,PhD,Moffitt Cancer Center & Research Institute,12902 Magnolia Drive,813-745-1327,,dung-tsa.chen@moffitt.org,Evaluation of a classifier performance at various cutoffs of gene selection in microarray data with time-to-event endpoint,1,Dung-Tsa,,Chen,"Moffitt Cancer Center & Research InstituteDepartment of Oncologic SciencesUniversity of South Florida",Ying-Lin,,Hsu,"Department of Applied Mathematics at National Chung Hsing University, Taichung, Taiwan",Tzu-Hsin,,Liu,"Department of Applied Mathematics at National Chung Hsing University, Taichung, Taiwan",James,J,Chen,"National Center for Toxicological Research, Food and Drug Administration",Timothy,,Yeatman,"Moffitt Cancer Center & Research InstituteDepartment of Oncologic SciencesUniversity of South Florida",,,,,,,,,,,,,,,,,,,,,"Statistical methods have been widely used to analyze microarray data with time-to-event endpoint by reducing gene expression data from a high-dimensional space to a manageable low-dimensional space for the follow-up survival analysis. Basically, the methods start with exploring an optimal cutoff to subset genes. The selected cutoff will lead to determine the number of genes (and which genes). With the cutoff, we can employ survival analysis methods to build a classifier for prediction purpose. Often we do not have the luxury to have a test dataset ready for validation. To evaluate how good the classifier is, random split scheme has been used to divide the whole data into training and test sets and repeat the process many times for evaluation.In this study, we examine performance of a classifier at various cutoffs by utilizing the random split scheme. We compare performance of a classifier in the test set at a pre-specified optimal cutoff versus other cutoffs. We also examine correlation of performance of the classifier at the training set versus at the test set. Several random split schemes are also compared for their impact on performance. A set of real data and simulation data are used for illustration.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Microarray analysis,High dimensional data,high-dimensionjiankang@umich.edu,,Jian Kang,,University of Michigan,2011 Medford Rd. Apt H157,7346781267,,jiankang@umich.edu,Meta-Analysis of fMRI Data via a Bayesian Cox Cluster Process,1,Jian,,Kang,"University of Michigan, Department of Biostatistics",Timothy,D,Johnson,"University of Michigan, Department of Biostatistics",Thomas,E,Nichols,"GlaxoSmithKline; University of Oxford, FMRIB; University of Michigan, Department of Biostatistics",Tor,D,Wager,"Columbia University, Department of Psychology",,,,,,,,,,,,,,,,,,,,,,,,,"Most functional magnetic imaging studies (fMRI) are small in size due to cost and difficulty in recruiting special patient populations. Since the same psychological paradigms are used in many studies, there is growing interest in meta-analyses of these data.  Typical data available for fMRI meta-analyses consists of all activation foci from several experiments. To date the most widely used method is the Activation Likelihood Estimation (ALE) method, either on its own or as part of a larger multi-stage method. The ALE method has known shortcomings, in particular only producing null-hypothesis inferences and providing no interpretable fitted model. In contrast, our model, a Bayesian spatial Cox cluster process model, provides an explicit fitted model and interpretable parameters. In particular our model provides information, via posterior intensity functions, about the most likely locations of activation centers at a population level and the inter-experiment spread of activated foci about these population centers. In our model, the observed activated foci are the offspring of a latent realization (population centers) of a parent process.  A priori, the offspring arise from a Cox cluster process while the parent process is a homogeneous Poisson process. We demonstrate our method on an emotion activation meta-analysis of 169 studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Imaging,Bayesian methods,simone@stat.duke.edu,,Simone Gray,,Duke University, 214 Old Chemistry Building,919-613-8722,,simone@stat.duke.edu,"Spatial modeling of air pollution exposure, measurement error and adverse birth outcomes.",1,Simone,,Gray,Department of Statistical Science,Alan,,Gelfand,Department of Statistical Science,Marie Lynn,,Miranda,Nicholas School of the Environment,Sharon,,Edwards,Nicholas School of the Environment,,,,,,,,,,,,,,,,,,,,,,,,,"When estimating personal exposure it is customary to use measurementsfrom the closest monitoring station as a simple proxy for personalexposure. Evidently, measurement error is introduced as the estimatedexposure is not equal to the monitored exposure. This talk presentswork that attempts to better understand the relationship betweenmaternal exposure to air pollution and pregnancy outcomes, whileaccounting for the associated measurement error. Induced by a processmodel specification for exposure reflecting sparsity of monitoringsites, we construct a spatial model that allows uncertainty inexposure to increase as the distance between maternal residence andthe location of the closest monitor increases.  We illustrate with airquality data from the EPA . We extend this method to account formissing data values in the monitors that are inactive by design andmeasured every three or every six days.  We assume that errorincreases as the time from the nearest recorded measurement increasesand incorporate that error term into a temporal component of themodel.  The statistical analyses are implemented using hierarchicalmodeling within a Bayesian perspective.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Spatial/temporal modeling,calvinw@ces.clemson.edu,,Calvin L. Williams,Associate Professor,Clemson University,Box 340975,864-656-5214,,calvinw@ces.clemson.edu,Biomedical applications of Convolutions of Mixed Distributions,2,Calvin,L,Williams,Clemson University,Charity,N,Watson,Clemson University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The concept of a convolution is statistical theory can be applied tomany situations where the distributions being convolved are mixeddensities and mass functions.This theory has been will established inthe insurance industry (Lanzenauer and Lundberg(1974), and Brown(1977)) . The idea of estimating the parameter(s) from convolution was discussed with some intrepidation  by Sclove and Van Ryzin(1969), and  Gong and Samaniego, (1981). Sprott (1984) tackled the issue of estimating parameters of convolutions. Moral (et. al, 2001) shows some interesting applications of mixed truncated exponentials whose convolutions show some interesting features. Here, we review these issues along with some interesting biostatical applications and give some additional examples based on our own insight.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Biostatistics Education,Hierarchical models,jpreisse@bios.unc.edu,,John S. Preisser,,University of North Carolina,Department of Biostatistics,(919) 966-7265,,jpreisse@bios.unc.edu,Software Development for GEE and ORTH,1,John,S,Preisser,University of North Carolina,Bahjat,F,Qaqish,University of North Carolina,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Although they received some early attention, the generalizedestimating equations (GEE) observation- and cluster-deletiondiagnostics of Preisser and Qaqish (1996) had yet seen broad use inpractice up to their incorporation in the GENMOD procedure of SASversion 9.2 released in 2008.  We consider our experience in the earlylack of promotion of this methodology and in the late development ofaccompanying software. We apply lessons learned to the recentdissemination of computational tools (R and SAS) for a new statisticalmethodology, orthogonalized residuals (ORTH), that includes thealternating logistic regressions procedure. We discuss incentives,disincentives, and rewards for pursuing software development in anacademic setting.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Tutorial - T3, Genetic and Microarray Data Analysis",invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Computational methods,Clustered data methods,christopher.swearingen@alumni.musc.edu,,Christopher Swearingen,"M.S., Ph.D. Candidate",Medical University of South Carolina,135 Cannon Place,843-876-1100,,christopher.swearingen@alumni.musc.edu,Further Development of Semi-parametric Methods in Bayesian Beta Regression,1,Christopher,J.,Swearingen,"Department of Biostatistics, Bioinformatics and EpidemiologyMedical University of South Carolina, Charleston SC USA",Dipankar,,Bandyopadhyay,"Department of Biostatistics, Bioinformatics and EpidemiologyMedical University of South Carolina, Charleston SC USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Beta Regression is a generalized linear model that estimates both location and precision parameters of a dependent variable that assumes a Beta distribution.  This model is extremely flexible, allowing independent covariate prediction of change in location, such as response between treatment groups, as well as changes in precision, such as heteroskedasticity of response between groups.  To date, only one Bayesian approach to Beta Regression has been published [1] and illustrated the usage of penalized splines to model a non-linear location covariate.  However, this work assumed constant precision in its modeling and did not consider how a non-linear covariate may impact precision. Our examination of penalized spline models examining both constant and non-constant precision assumptions is presented, based in part upon a previous analysis [2] of 285 ischemic stroke lesion volumes which detected a non-linear association between a covariate and median infarct volume.[1] Branscum, A., Johnson, W. & Thurmond, M. Bayesian beta regression: Applicationsto household expenditure data and genetic distance between foot-and-mouth diseaseviruses. Australian and New Zealand Journal of Statistics 49, 287301 (2007).[2] Nicholas, J. Swearingen, C.J. et al. The effect of statin pre-treatment on infarct volume in ischemic stroke. Neuroepidemiology. 31, 4856 (2008).",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Generalized linear models,Bayesian methods,genton@stat.tamu.edu,,Marc G. Genton,Prof.,Texas A&M University,Department of Statistics,979 845 3141,,genton@stat.tamu.edu,Testing and Modeling the Cross-Covariance Functions of Multivariate Random Fields,1,Marc,G,Genton,Texas A&M University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There is an increasing wealth of multivariate spatial and multivariatespatio-temporal data appearing. First, we propose a methodology to evaluate the appropriateness of several types of common assumptions on cross-covariance functions in the spatiotemporal context. The methodology is based on the asymptotic joint normality of sample space-time cross-covariance estimators. Specifically, we address the assumptions of symmetry, separability and linear models of coregionalization. Second, we address the problem of constructing valid parametric cross-covariance functions. We propose a simple methodology for developing flexible, interpretable, and computationaly feasible classes of cross-covariance functions in closed form. We discuss estimation of these models and perform a small simulation study to demonstrate our approach. We illustrate our methodology on a trivariate pollution dataset from California.This talk is based on joint works with Tatiyana Apanasovich, Bo Li, and Michael Sherman. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Time series,gailm@mail.nih.gov,,Mitchell H. Gail,Senior Investigator,National Cancer Institute,6120 Executive Blvd,301-496-4156,301-402-0081,gailm@mail.nih.gov,Value of SNPs in Models that Predict Breast Cancer Risk,1,Mitchell,H,Gail,"Division of Cancer Epidemiology and Genetics, National Cancer Institute",Ruth,M,Pfeiffer,"Division of Cancer Epidemiology and Genetics, National Cancer Institute",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Breast Cancer Risk Assessment Tool (BCRAT) uses age at menarche,age at first live birth, number of previous biopsies and familyhistory to project breast cancer risk.  Recently, seven singlenucleotide polymorphisms have been validated as associated with breastcancer risk.  We compare a model that adds these seven SNPs,BCRATplus7, with BCRAT.  We discuss the added value of BCRATplus7 interms of discriminatory accuracy and in terms of reduced expectedlosses for: deciding whether to take tamoxifen to prevent breastcancer; deciding whether to have a mammogram; and allocating scarceresources for mammography.  In all these applications, adding theseven SNPs improves the performance of BCRAT very little.   ",FALSE,FALSE,T1: Competing Risks,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Epidemiologic methods,zhanghy@vt.edu,,Huaiye ZHANG,PhD student,Dept. of Statistics of Virginia Tech,"H14, 1009 University City Blvd",540-808-7513,,zhanghy@vt.edu,The Model Selection of Zero-inflated Mixture Poisson Regression,1,Huaiye,,ZHANG,PhD student in the Dept. of Statistics  in Virginia Tech,Inyoung,,Kim,Assistant Professor in the Dept. of Statistics in Virginia Tech,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Poisson regression provides a standard framework for the analysis of the counting data. However in many application, count data has many zeros and also has the mixture distributions. The zero-inflated mixture Poisson regression can be handled for the data. However, it is not obvious to choose a zero-inflated Poisson model without any statistical evidence and also difficult to select the number of mixing components in mixture distributions. Hence, in this paper, we propose a score test for zero-inflated mixture Poisson regression and give a procedure of component selections based on several criterions. And then the mixture model can be estimated using Expectation-Maximization algorithm and be made inference using bootstrapping approach. We demonstrate the advantage of our approaches using the example which motivated this work.Keywords: Bootstrap; Expectation Maximization algorithm; Mixing Component; Zero-Inflated Poisson;",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Variable subset selection/model selection,Applied data analysis,vanmete@musc.edu,,Emily Van Meter,,Medical University of South Carolina,1244 Aruba Circle,864 915-1608,,vanmete@musc.edu,Proportional odds model for design of dose finding clinical trials with ordinal toxicity grading,1,Emily,M,Van Meter,"Medical University of South Carolina, Department of Biostatistics, Bioinformatics, and Epidemiology",Elizabeth,,Garrett-Mayer,"Medical University of South Carolina, Department of Biostatistics, Bioinformatics, and Epidemiology",Dipankar,,Bandyopadhyay,"Medical University of South Carolina, Department of Biostatistics, Bioinformatics, and Epidemiology",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Currently many phase I clinical trial designs including the continual reassessment method (CRM) dichotomize toxicity based on pre-specified dose-limiting criteria. Since phase I trials use small sample sizes, much information is lost by not accounting for different toxicity grades. Our proposed design incorporates ordinal toxicity endpoints as specified by Common Toxicity Criteria (CTCAE v3.0) and includes toxicity grades 1 and 2 not currently considered in standard designs. We extend the CRM to include ordinal toxicity outcomes using the proportional odds model and compare our results with the traditional CRM. Simulation studies and further sensitivity analysis of the new design comparing various weighting schemes, sample sizes, and cohort sizes show that the proposed proportional odds CRM does as well or better than the standard CRM and estimates the maximum tolerated dose (MTD) with more precision. Our findings suggest that it is beneficial to incorporate ordinal toxicities into phase I trial designs and future studies will compare this proposed design to other phase I trial designs.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Cancer applications,estanwy1@math.umbc.edu,,Elizabeth A. Stanwyck,Graduate Student,"University of Maryland, Baltimore County",Math/Psych Building room 237,443-858-4475,,estanwy1@math.umbc.edu,Statistical Analysis of The Effects of Air Pollution on Children's Health,1,Elizabeth,A.,Stanwyck,University of Maryland Baltimore County,Bimal,,Sinha,University of Maryland Baltimore County,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Effects of air pollution on human health, especially on the health ofchildren, have been a concern for many years and serious attempts havebeen made to effectively measure such effects. While one end of theequation, namely human health effects, can be either observed ormeasured without much error, the other end of the equation, specificpollutants and their amounts at the (micro) individual level, is veryhard to determine. Fortunately, some interesting and usefulstatistical models can be used to describe such uncertainties, andmore importantly, to meaningfully link the two sides. In thispresentation a general overview of the problem and some solutions fromboth frequentist and Bayesian points of view will be presented. Thenan adaptation of these procedures to deal with real data available inthe United States will be discussed. Also, separate results of a studyinvolving a few Chinese cities  based on an entirely differentapproach - will be given and a comparison of the two approaches usinga simulation study will be mentioned. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Epidemiologic methods,Environmental and ecological applications,slooney@mcg.edu,,Stephen Looney,Professor,Medical College of Georgia,"1120 15th St., AE-3020",706-721-4846,706-721-6294,slooney@mcg.edu,A New Method for Estimating the Odds Ratio from Incomplete Matched Data,2,Kelly,,Miller,Medical College of Georgia,Stephen,,Looney,Medical College of Georgia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Matched case-control studies are commonly used to evaluate theassociation between the exposure to a risk factor and a disease. Theodds ratio is typically used to quantify this association.Difficulties in estimating the true odds ratio arise, however, whenthe exposure status is unknown for one individual in a pair. In thecase where the exposure status is known for both individuals in allpairs, the true odds ratio is estimated as the ratio of the counts inthe discordant cells of the observed two-by-two table. In the casewhere all data are independent, the odds ratio is estimated using thecross-product ratio from the observed table. In this paper we suggesta method for estimating the odds ratio when the sample consists of acombination of paired and unpaired observations. This method uses aweighted average of the two odds ratio calculations described above.We compare our method to existing methods via simulation.",FALSE,FALSE,,FALSE,FALSE,TRUE,"T3: Genetic and Microarray Data AnalysisMonday 1:45-3:30 pm(This is not listed in the drop-down box for Tutorials given above.)",oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Applied data analysis,Categorical data,whou@biostat.ufl.edu,,Wei Hou,Assistant Professor,University of Florida,"1329 SW 16th Street, Room 5127",(352)265-0111 ext 86565,(352) 265-8047,whou@biostat.ufl.edu,Haplotype Analysis of Quantitative Traits in Outcrossing Plant Populations,1,Wei,,Hou,"Department of Epidemiology and Health Policy Research, University of Florida, Gainesville, FL 32608",Rongling,,Wu,"Department of Statistics, University of Florida, Gainesville, FL 32611, and Department of Public Health Sciences, Department of Statistics, Pennsylvania State University, Hershey, PA 17033",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The genetic architecture of quantitative traits can be understood atthe haplotype level. For natural outcrossing plant populations, themethods for discovering and modeling haplotypes associated withquantitative traits have not been sufficiently developed. We present astatistical model for haplotype analysis by incorporating genetic andbiological characteristics of outcrossing plants. The model allows thesimultaneous estimation of outcrossing rate, recombination fraction,haplotype frequencies, linkage disequilibria, and haplotype effects.The model is formulated with a mixture-based maximum likelihood andimplemented with the EM algorithm. A Monte Carlo simulation wasperformed to test the statistical properties of the model, suggestingthat the model is robust for parameter estimation. A series ofhypothesis tests were formulated to study the pattern and amount ofpopulation genetic variation and the genetic architecture of complextraits for outcrossing plants.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Computational methods,fthomas4@utmem.edu,,Fridtjof Thomas,Dr,University of Tennessee Health Science Center,"66 N. Pauline, Suite 633",901-448-6461,,fthomas4@utmem.edu,A Bayesian Change-point Algorithm for Detecting Copy Number Alteration,1,Fridtjof,,Thomas,University of Tennessee Health Science Center,Stanley,,Pounds,St. Jude Children's Research Hospital,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recent technical developments have made it possible to collecthigh-resolution genomics data using single nucleotide polymorphism(SNP) arrays. These arrays can be used in a paired data context tocompare cancer tissue to normal samples in an effort to identifyregions of genomic amplification or deletion. Such regions potentiallycontain oncogenes or tumor suppressor genes and are therefore ofparticular interest.However, using SNP array signals to identifying regions of copy numberalteration is a challenging task due to the properties of the derivedmeasurements. We apply a Bayesian change-point algorithm topre-normalized signals from SNP microarrays obtained from a set ofleukemia samples in an effort to infer regions of copy numberalteration and compare this approach to other approaches currently inuse for this purpose.The Bayesian change-point algorithm detects multiple change-pointswhere a change can be in the mean of the subsequent measurements, intheir variance, in their autocorrelation structure, or in acombination of two or all of these aspects.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Cancer applications,mac1166@uncw.edu,,Margaret Cohen,,University of North Carlolina at Wilmington,Department of Mathematics and Statistics,7817185562,,mac1166@uncw.edu,Estimating the Maximum Growth Rate of Harmful Algal Blooms Using a Combined Model Method,1,Margaret,A,Cohen,Graduate student in Mathematics at the University of North Carolina at Wilmington,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper proposes a creative new method of estimating the maximum growth of harmful algal blooms.  Traditionally marine scientists have calculated the maximum growth rate using a linear method which can be influenced by the choice of endpoints.  A more objective statistical method of estimating growth in a sigmoidal curve using the rate at the point of inflection was presented, but the estimates were viewed as too large.  In response, we proposed a hybrid approach that was the combination of the Logistic, Weibull, and Gompertz models.  This paper illustrates this combined method along with examples of harmful algal blooms.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Nonlinear models,Environmental and ecological applications,pkolm@christianacare.org,,Paul Kolm,Director of Biostatistics,Christiana Care Health System,131 Continental Drive,302-623-0671,302-623-0669,pkolm@christianacare.org,A Comparison of Missing Data Methods for Quality of Life Measures in a Clinical Trial with Long-term Follow-up,1,Paul,,Kolm,Christiana Care Health System,Wei,,Zhang,Christiana Care Health System,John,A,Spertus,Mid America Heart Institute,David,J,Maron,Vanderbilt University,William,E,Boden,Buffalo General Hospital,William,S,Weintraub,Christiana Care Health System,,,,,,,,,,,,,,,,,"The primary outcomes of clinical trials are typically well-defined events such as survival, non-fatal cardiovascular events (e.g., stroke, myocardial infarction) and recurrence of cancer.  Many trials also include secondary measures assessing patient quality of life and health status.  The latter types of outcomes are assessed at different times over the length of the trial or an additional follow-up period.  Inevitably, missing patient values occur either intermittently or through dropout.  Generally, more missing values occur with increasing length of follow-up and present a challenge for longitudinal analyses.  Analytic methods for handling missing data have advanced in recent years with respect to statistical sophistication.  In this study, we compare results of applying several methods for missing data, including last value carried forward, missing at random, pattern mixture and propensity scores, to a quality of life measure obtained from patients in a large cardiovascular clinical trial spanning up to 7 years of follow-up.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Longitudinal data,imrunnin2win@yahoo.com,,Joel O'Hair,,"Department of Statistical Science, Southern Method",8441 Southwestern Blvd #6119,214-300-9391,,imrunnin2win@yahoo.com,Extraction of the Hemodynamic Response Function and Parameter Estimation for the Two Gamma Difference Model,1,Joel,C,O'Hair,"Graduate Research Assistant, Department of Statistical Science, Southern Methodist University",Richard,,Gunst,"Professor, Department of Statistical Science, Southern Methodist University",William,,Schucany,"Professor, Department of Statistical Science, Southern Methodist University",Wayne,,Woodward,"Professor, Department of Statistical Science, Southern Methodist University",,,,,,,,,,,,,,,,,,,,,,,,,"Stimulation of certain brain regions induces changes in the concentration of oxygen in those areas of the brain.  The nature of this hemodynamic response is an essential component of understanding brain function.  The purpose of this work is to find the best method of ascertaining the characteristics of the hemodynamic response from an fMRI time series.  The structure of the hemodynamic response, referred to as the hemodynamic response function (HRF), can be estimated without assuming any mathematical form or by estimating the parameters of an appropriate model.  Four nonparametric methods of extracting the HRF are compared using simulation methods.  These four methods  extraction by deconvolution, Wiener filter extraction, LS-F extraction, and LS-T extraction  produce a nonparametric estimate of the HRF.  There are five methods of parametric estimation of the HRF considered in this article.  These include the Convolved HRF fit to the original fMRI time series, and a parametric model fit to the time series resulting from each of the four nonparametric extraction methods.  These methods are also compared using simulation.  It is shown that the LS-T extraction often provides the most desirable nonparametric estimation of the HRF, while the Convolved HRF fit usually provides the best parametric estimation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Time series,Imaging,chauhan@ipfw.edu,,Chauhan.C.K.,Associate Professor,Indiana ourdue University,2101 E.Coliseum Blvd,260-481-6227,,chauhan@ipfw.edu,The Use of Extreme Order Statistics to Estimate Standard Deviation,1,Chand,K,Chauhan,Indiana Purdue University Fort Wayne Indiana,Yvonne,M,Zubovic,Indiana Purdue University Fort Wayne Indiana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For an unknown standard deviation, Ã, we investigate an estimate Sp, which has a probability p, of being as high as or higher than Ã, where the value of p may be selected by an experimenter. To avoid underestimating Ã, one may pick p as high as or higher than 95%. In many situations only the highest value, Yn and the lowest value, Y1, of a sample may be available. Therefore in this paper we propose a linear combination, aYn - bY1, as an estimate. For symmetric distributions such as normal, it is common to divide a sample range by 4 to obtain an estimate of Ã. However this may not result in a estimate with a high value of p. The values of a and b depend on many factors such as the parent distribution, the sample size used to find the extreme order statistics, the desired value of p, and other properties of the estimate. The authors discuss the choice of the linear combination, aYn - bY1,  under different conditions. Analytical and simulated results of the performance of the proposed estimator are provided. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Applied data analysis,estimationwolfb@musc.edu,,Bethany J. Wolf,,Medical University of South Carolina,1641 Fairway Place Lane,843-509-4992,,wolfb@musc.edu,An evaluation of logic forest for identification of disease biomarkers,1,Bethany,J,Wolf,"Medical University of South Carolina, Department of Biostatistics, Bioinformatics, and Epidemiology",Elizabeth,H,Slate,"Medical University of South Carolina, Department of Biostatistics, Bioinformatics, and Epidemiology",Elizabeth,G,Hill,"Medical University of South Carolina, Department of Biostatistics, Bioinformatics, and Epidemiology",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Adequate screening tools allowing physicians to diagnose diseases inasymptomatic individuals or identify individuals at elevated risk ofdeveloping disease have potential to reduce overall disease relatedmortality.  Multiple studies cite the need for noninvasive tests thatare both sensitive and specific.  Diagnostic tests based on multiplebiomarkers may lead to enhanced sensitivity and specificity. Statistical methodologies that can model complex biologic interactionsand that are easily interpretable allow for translation of biomarkerresearch into diagnostic tools.  Logic regression, a relatively newmultivariable regression method that predicts binary outcomes usinglogical combinations of binary predictors, has the capability to modelthe complex interactions in biologic systems in easily interpretablemodels.  However the performance of logic regression degrades in noisydata.  We implement an extension of logic regression methodology to anensemble of logic trees, which we call Logic Forest.  We conduct asimulation study to compare the ability of logic regression and logicforest to identify interactions among variables that are predictive ofdisease status.  Our findings indicate Logic Forest is superior tologic regression for identifying important predictors, particularly innoisy data.  Logic Forest provides a new statistical tool capable ofidentifying predictors and predictor interactions associated with disease.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biomarkers/surrogate markers,Diagnostic and screening tests,di.li@abbott.com,,DI LI,,Abbott Lab,"R436, AP9A-1",217-493-5894,,di.li@abbott.com,Evaluating Probability of Success for Clinical Trials with Time-to-Event Endpoints,1,DI,,LI,Abbott Laboratories,Todd,A,Busman,Abbott Laboratories,Martin,S,King,Abbott Laboratories,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Traditional clinical trial design and sample size planning is usually based on achieving a targeted statistical power to detect a specified treatment effect. Since statistical power is conditional on the specified treatment effect, trial planning based on statistical power leaves out the uncertainty of the estimate of the true underlying treatment effect. A high statistical power does not guarantee a high probability of achieving the desired outcome. The probability of success is the unconditional probability of a successful trial. In simple cases, it can be calculated as the expected power averaged over some prior distribution for the unknown true treatment effect. The primary efficacy endpoint in oncology clinical trials is often a time-to-event outcome, e.g., overall survival or time to disease progression. This presentation illustrates evaluation of the probability of success in trial planning with time-to-event endpoints through simulations in a Bayesian framework.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Biopharmaceutical research,Clinical trials,jhogan@stat.brown.edu,,Joseph W Hogan,Professor of Biostatistics,Brown University,Center for Statistical Sciences,401 863 9243,401 863 9182,jhogan@stat.brown.edu,Constructing and Calibrating Informative Priors for Nonidentified Parameters in Models Fit to Incomplete Data,1,Joseph,W,Hogan,Brown University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Consider the problem of estimating a parameterof interest from incomplete data (e.g. populationmean, odds ratio, regression coefficient).Two common inferential summaries are boundsand sensitivity analyses.  Bounds are attractivebecause they convey lack of information about theparameter (i.e. a point estimate is not available);sensitivity analyses are useful because they can beused to depict possible inferences across a rangeof plausible assumptions.A relatively underutilized approach is the useof informative priors under a Bayesian formulationof the inferential problem.  We illustrate withseveral examples how to encode untestable assumptionswith prior distributions, and how to draw sensibleposterior inferences.  Issues addressed includeelicitation and formulation of priors using expertopinion, dynamic updating of priors for longitudinaldata, and calibration of ranges for priors and sensitivityanalyses. ",FALSE,FALSE,,FALSE,FALSE,TRUE,Prefer Mon or Tues session,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Bayesian methods,vjb2@stat.duke.edu,,Veronica J Berrocal,,Duke University,Department of Statistical Science - Box 90251,206 999-7634,206 999-7634,vjb2@stat.duke.edu,Synthesizing categorical datasets to enhance inference,1,Veronica,J,Berrocal,"Duke University, Department of Statistical Science",Alan,E,Gelfand,"Duke University, Department of Statistical Science",Sourab,,Bhattacharya,"Bayesian and Interdisciplinary Research Unit, Indian Statistical Institute",Marie,L,Miranda,"Duke University, Nicholas School of the Environment",Geeta,,Swamy,"Duke University, Department of Obstetrics and Gynecology",,,,,,,,,,,,,,,,,,,,,"A common data analysis setting consists of a collection of datasets of varyingsizes that are all relevant to a particular scientific question, but whichinclude different subsets of the relevant variables, presumably with someoverlap.Here, we present a method to synthesize incomplete categorical datasets drawnfrom an incompletely cross-tabulated population, for which the marginalprobabilities are known and where at least one of the dataset is completelyobserved. We show that our method can still be applied even when the completedataset refers to a subset of the population not obtained via simple randomsampling.We also demonstrate that in the incomplete categorical datasets scenariosynthesizing datasets is expected to reduce uncertainty about the cellprobabilities in the associated multi-way contingency table as well as forderived quantitites such as relative risks and odds ratios, but the improvementis not guaranteed. This differs from the complete datasets scenario, where we areassured to tighten inference.To illustrate our method, we present simulated examples motivated by an adversebirth outcomes investigation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Categorical data,Missing data,cuwang@aecom.yu.edu,,Cuiling Wang,Assistant professor,Albert Einstein College of Medicine,1300 Morris Park Ave,718-430-2006,,cuwang@aecom.yu.edu,Power analysis for mediation effect in longitudinal studies,1,Cuiling,,Wang,Albert Einstein College of Medicine,,,,Albert Einstein College of Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Mediation effect is often of great interest in longitudinal epidemiological research. Current literature on longitudinal mediation analysis mostly focuses on time dependent mediators, while in some studies both the independent variable and the mediator are time independent. Power analysis methods for such study designs have not been adequately addressed. In this work we derive formulas based on asymptotic theory for calculation of power for the longitudinal mediation effect of a time-independent mediator on a time-independent predictor on repeated measures outcome, with and without drop out. Performance of the formulae for limited sample sizes were examined through simulation studies. The method was applied to a project in the design of Mobility Biology & Interventions Study at Einstein a longitudinal (MOBILISE) study where the mediation effect of markers of cerebral microvascular status and function on the associations of cardiovascular risk factors with decline in gait velocity.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Power analysis/sample size,Applied data analysis,scott.schwartz@stat.duke.edu,,Scott Schwartz,,"Department of Statistical Science, Duke University",2420 Perkins Road,210 296-4392,210 296-4392,scott.schwartz@stat.duke.edu,"Confounding and Bias from Intermediate Variables, and a Joint Model for Birthweight and Gestational Age Addressing Them",1,Scott,L,Schwartz,"Department of Statistical Science, Duke University",Alan,E,Gelfand,"Department of Statistical Science, Duke University",Marie,L,Miranda,"Children's Environmental Health Initiative, Nicholas School of the Environment, Duke University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A frequently studied birth outcome is birthweight (BW), evidently, strongly associated with gestational age (GA).  Customary modeling for BW is conditional on GA.  However, adjusting for intermediate variables (1) entails the loss of `causal effect' estimation and (2) may produce `spurious effect estimates' due to back-door criterion violation.  Joint modeling provides an alternative means to address the relationship between BW and GA while at the same time offering increased flexibility and interpretation in studying these outcomes. We introduce a `mixture of bivariate regressions' model for the joint distribution of BW and GA which, (1) enables us to capture the clearly nonGaussian distributional shapes observed in histograms, (2) addresses the interval censoring commonly associated with gestational age (to the nearest completed week) through a latent specification, and (3) facilitates interpretation by providing a bivariate regression structure for well-established risk factors. This approach explicitly avoids adjusting for the intermediate variable GA, the consequences of which are explained.KEYWORDS: Causal Inference, Confounding, Bias.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Bayesian methods,johair@smu.edu,,Joel O'Hair,,"Department of Statistical Science, Southern Method",8441 Southwestern Blvd #6119,214-300-9391,,johair@smu.edu,Extraction of the Hemodynamic Response Function and Parameter Estimation for the Two Gamma Difference Model,1,Joel,C,O'Hair,"Graduate Research Assistant, Department of Statistical Science, Southern Methodist University",Richard,,Gunst,"Professor, Department of Statistical Science, Southern Methodist University",William,,Schucany,"Professor, Department of Statistical Science, Southern Methodist University",Wayne,,Woodward,"Professor, Department of Statistical Science, Southern Methodist University",,,,,,,,,,,,,,,,,,,,,,,,,"Stimulation of certain brain regions induces changes in the concentration of oxygen in those areas of the brain.  The nature of this hemodynamic response is an essential component of understanding brain function.  The purpose of this work is to find the best method of ascertaining the characteristics of the hemodynamic response from an fMRI time series.  The structure of the hemodynamic response, referred to as the hemodynamic response function (HRF), can be estimated without assuming any mathematical form or by estimating the parameters of an appropriate model.  Four nonparametric methods of extracting the HRF are compared using simulation methods.  These four methods  extraction by deconvolution, Wiener filter extraction, LS-F extraction, and LS-T extraction  produce a nonparametric estimate of the HRF.  There are five methods of parametric estimation of the HRF considered in this article.  These include the Convolved HRF fit to the original fMRI time series, and a parametric model fit to the time series resulting from each of the four nonparametric extraction methods.  These methods are also compared using simulation.  It is shown that the LS-T extraction often provides the most desirable nonparametric estimation of the HRF, while the Convolved HRF fit usually provides the best parametric estimation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Time series,Imaging,liuc3@mail.nih.gov,,Liu Chunling,Postdoctoral Fellow,NICHD/NIH,"6100 Executive Blvd, Rm 7105K",3014356940,,liuc3@mail.nih.gov,A method for accelerating the quadratic lower-bound algorithm,2,Aiyi,, Liu,"Biostatistics and Bioinformatics Branch Eunice Kennedy Shriver National Institute of Child Health and Human Development National Institutes of Health  ",Chunling,,Liu,"Biostatistics and Bioinformatics Branch Eunice Kennedy Shriver National Institute of Child Health and Human Development National Institutes of Health  ",Man Lai,,Tang,"Department of Mathematics, Hong Kong Baptist University, Kowloon Tong, Hong Kong",Guo-liang,,Tian,"Department of Statistics and Actuarial Science, University of Hong Kong,  Pokfulam Road, Hong Kong",,,,,,,,,,,,,,,,,,,,,,,,,"Abstract:The quadratic lower-bound (QLB) algorithm is particularly suited to problems such as logistic, multinomial logistic, and Cox's proportional hazards models, where  EM-type algorithms can not be applied because of lacking a missing-data structure. However, to overcome slow convergence, we proposes a novel 'shrinkage parameter' approach to accelerate the QLB algorithm while maintaining its simple  implementation and stable convergence. The uniformly optimal shrinkage parameter was derived. Two real data examples applying to logistic regression and Cox proportional hazard models separately are analyzed and compared to show that the accelerated QLB algorithm is dramatically twice as fast as the traditional QLB  and to illustrate the proposed methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Computational methods,Applied data analysis,starima@mcw.edu,,Sergey Tarima,,Medical College of Wisconsin,8701 Watertown Plank Rd,414-456-5605,,starima@mcw.edu,Testing for conditional independence via multiple models: an orthopedic application for the six segment foot model,1,Sergey,,Tarima,Medical College of Wisconsin,Xue-Cheng,,Liu,Medical College of Wisconsin,Roger,,Lyon,Medical College of Wisconsin,John,,Thometz,Medical College of Wisconsin,Channing,,Tassone,Medical College of Wisconsin,,,,,,,,,,,,,,,,,,,,,"A hypothesis about conditional independence between two random variables X and Y given Z is often tested via an assumed regression model E(g(Y)|X,Z), where g(y) is monotone. After fitting a chosen model the Wald test is regularly used for testing significance of the regression coefficient at X (we assume no interaction with Z). However, this model is not the only model applicable for testing conditional independence. The other regression models, such as E(h(Y)|X,Z) or E(f(X)|Y,Z) can be used as well, h(y) and f(x) are monotone functions. We consider a simple method for testing conditional independence based on multiple models. Our method secures asymptotically the type I error. We illustrate application of this approach on orthopedic patients for testing conditional independence between kinematic parameters measured by the six-segment foot model. The six-segment foot model is designed for 3D measurements of the foot and ankle joint in the patient with spastic cerebral palsy (CP), which allows assessing the degree of foot deformities objectively. A total of 5 typically developing children and 8 children with CP, aged from 6 to 18 year-old, were enrolled in the study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Other,Variable subset selection/model selection,Multimodel inferenceliua@mail.nih.gov,,Aiyi Liu,Senior Investigator,NICHD/NIH,"6100 Executive Blvd, RM 7B05B",(301) 435-6953,,liua@mail.nih.gov,Nonparametric procedures for comparing correlated multiple endpoints with applications to oxidative stress biomarkers,1,Aiyi,,Liu,NICHD/NIH,Chunling,,Liu,,Enrique,,Schisterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Multiple endpoints are common in clinical and epidemiologic studies. To compare multiple endpoints between two independent groups, O'Brien (1984) and Huang et al. (2005) proposed rank-sum based nonparametric testing procedures. In this talk we extend their methods to comparing multiple endpoints between two dependent groups. We further obtain a more powerful test based on optimizing the linear combination of ran-sum statistics. The methods are illustrated using data from the BioCycle Study to compare the levels of oxidative stress biomarkers between a woman's two consecutive menstrual cycles.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,Multiple testing,bxu@schoolph.umass.edu,,Bo Xu,Student,"School of public health, UMASS","RM 414, 715 North Pleasant Street",413-244-6333,,bxu@schoolph.umass.edu,Bootstrap Confidence Intervals for the Predictors of Treatment Means in a One Factor Experimental Design with A Finite Population,1,Bo,,Xu,"Division of Biostatistics , School of Public Health, UMASS, Amherst",Edward,,Stanek,"Division of Biostatistics , School of Public Health, UMASS, Amherst",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A one factor experimental design is developed based on the potential observable outcome framework, sampling from finite population of units and random allocation of treatments.  The predictors for treatment means are obtained by  Royalls (1976) prediction theory where the treatment deviations are represented as realized random effects.   When the variance components are unknown, the empirical predictors are considered.  The confidence intervals for the empirical predictors are calculated using bootstrapping methods.  Several bootstrap methods, such as bootstrapping with replacement (BWR) or bootstrapping without replacement (BWO) are introduced.  Each bootstrap method is developed to account for the sampling from the finite population of units and random allocation of treatments.  Comparisons of the different bootstrapping methods are made methodologically, and via simulation.  We discuss these comparisons, with a goal of recommending an appropriate bootstrapping method for statistical inference in the one factor experimental design.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Random effects,Nonparametric methods,stijn.vansteelandt@ugent.be,,Stijn Vansteelandt,,Ghent University,Krijgslaan 281 (S9),++3292644776,,stijn.vansteelandt@ugent.be,Estimating controlled direct effects in random and outcome-dependent sampling designs,1,Stijn,,Vansteelandt,"Ghent University, Belgium",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Estimating what is the effect of an exposure on an outcome other thanthrough some given mediator, requires adjustment for all risk factorsof the mediator that are also associated with the outcome. When theserisk factors are themselves affected by the exposure, then standardregression methods do not apply. In this presentation, I will brieflyreview methods for accommodating this and discuss their limitationsfor estimating the controlled direct effect, in particular focussingon studies with a continuous mediator. In addition, I will propose apowerful and easy-to-apply alternative which uses G-estimation instructural nested models to address these limitations both for cohortand outcome-dependent sampling designs.",FALSE,FALSE,T3: Statistical Analysis of Cost Effectiveness Data,FALSE,TRUE,TRUE,Tutorial on genetic and microarray analysis,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Epidemiologic methods,nicolae@galton.uchicago.edu,,Dan Nicolae,Associate Professor,Departments of Medicine and Statistics,The University of Chicago,773 702-4837,,nicolae@galton.uchicago.edu,Genome-wide strategies for gene-gene interaction,1,Dan,L,Nicolae,The University of Chicago,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The development of cost-effective genotyping technologies has madegenome-wide association studies the most popular tool for findinggenes for complex traits. Although most of the findings come fromsingle-marker analysis, there is a growing recognition thatinteractions (gene-gene and gene-environment) can play an importantrole in common disease etiology. The multiple comparison adjustmentsassociated with whole genome studies are even more severe wheninteractions are investigated, making efficient statistical methodseven more important. We discuss new methods for gene-gene interactiontesting that are more efficient than classical approaches. Theincrease in power is achieved using several strategies includingreasonable constraints in the two-marker parameter space, and testingprioritization based on the structure and dynamics of the humangenome. A global association test that incorporates the evidence ofmarginal association and interactions will be also discussed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Genomics,arzu.onar@stjude.org,,Arzu Onar,Assistant Member,Biostatistics St Jude Children's Research Hospital,262 Danny Thomas Place,9015955499,,arzu.onar@stjude.org,Safety and Accuracy of Phase 1 Oncology Trials: Traditional Method vs. Rolling 6 vs. CRM,1,Arzu,,Onar-Thomas,"Department of BiostatisticsSt. Jude Children's Research Hospital",Zang,,Xiong,"Department of BiostatisticsSt. Jude Children's Research Hospital",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Various dose-finding designs are available for Phase I pediatric oncology trials and recently the list was augmented by one with the addition of the Rolling 6 Design. The newcommer is quite similar to the empirically-based Traditional Design (also known as the 3+3 or up-and-down method), with the exception that it allows cohorts of up to 6 patients to be registered at one dose level. The intent is to decrease the duration of the trial and limited simulations indicate that Rolling 6 can achieve this goal without notable increase in toxicity. In this talk we will present extensive simulation results which compare the performance of the Rolling 6 design to that of the Traditional Method as well as to results from a frequentust version of the continuous reassessment method. The comparisons are based on accuracy, sample size and toxicity associated with each approach. The advantages/disadvantages of using each design will be highlighted within the context of challenges unique to pediatric oncology trials such as BSA based dosing.",FALSE,FALSE,,FALSE,FALSE,TRUE,Taking 2 half day courses on Sunday,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Cancer applications,amaity@hsph.harvard.edu,,Arnab Maity,Dr.,Harvard School of Public Health,Department of Biostatistics,9792184523,,amaity@hsph.harvard.edu,Testing for Genetic Main Effects in Presence of Gene-Gene  and Gene-Environment Interactions in Genome-Wide Association Studies,1,Arnab,,Maity,"Department of Biostatistics, Harvard School of Public Health",Xihong,,Lin,"Department of Biostatistics, Harvard School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"There is growing evidence that gene-gene interaction or epistasisubiquitously contributes to complex traits. Ourinterest lies in developing testing procedures of effects of a gene,or a group of genes, in thepresence of possible gene-gene interactions. In general, testing forthe effect of a particular gene in the presence of gene-geneinteraction generally requires testing for the correspondinginteraction effect of the gene under consideration with other genes.However, in genome-wide association studies (GWAS), a very high numberof genetic markers are genotyped and the number of possibleinteraction combinations among the genotyped markers is astronomical.Testing for all possible interactions in this situation is likely tolead to loss of power and require a strong parametric assumption. Inthis paper, we propose a kernel machine based method to model theeffects of high-dimensional genetic variants by incorporatinggene-gene interactions, and develop statistical procedures to test forthe effects of a gene or a group of genes in presence of gene-geneinteractions. We will use a garrote kernel method  to constructstatistical score based tests for gene effects that do not requiretesting for the coefficients of all the interaction terms allowing usto obtain more powerful tests using less degrees offreedom. We will investigate the asymptotic properties of our proposedtest and evaluate its performance via simulation studies. We willdemonstrate our methodology by applying them to the Framingham HeartStudy GWAS data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Genomics,desantis@musc.edu,,Stacia M DeSantis,Asst Professor,"Department of Biostatistics, Bioinformatics, and E",Medical University of South Carolina,842-876-1593,,desantis@musc.edu,A Latent Class Model With Hidden Markov Dependence for Array CGH Data,1,Stacia,M,DeSantis,"Department of Biostatistics, Bioinformatics, and Epidemiology, Medical University of South Carolina, Charleston, SC",E. Andres,,Houseman,"Department of Work Environment, University of Massachusetts, Lowell, Lowell, MA",Brent,A,Coull,"Department of Biostatistics, Harvard School of Public Health, Boston, MA",David,N,Louis,"Department of Pathology, Massachusetts General Hospital, Charlestown, MA",Gayatry,,Mohapatra,"Department of Pathology, Massachusetts General Hospital, Charlestown, MA",Rebecca,A,Betensky,"Department of Biostatistics, Harvard School of Public Health, Boston, MA",,,,,,,,,,,,,,,,,"Array CGH is a high-throughput techniquedesigned to detect genomic alterations linked to the development andprogression of cancer. The technique yields fluorescence ratios thatcharacterize DNA copy number change in tumor versus healthy cells.Classification of tumors based on aCGH profiles is of scientificinterest but the analysis of these data is complicated by the largenumber of highly correlated measures. In this paper, we develop asupervised Bayesian latent class approach for classification thatrelies on a hidden Markov model to account for the dependence in theintensity ratios. Supervision means that classification is guided bya clinical endpoint. Posterior inferences are made aboutclass-specific copy number gains and losses. We demonstrate ourtechnique on a study of brain tumors, for which our approach iscapable of identifying subsets of tumorswith different genomic profiles, and differentiates classes by survivalmuch better than unsupervised methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Genomics,High dimensional data,pfeiffer@mail.nih.gov,,Ruth Pfeiffer,,BB/DCEG/NCI/NIH,6210 Executive Blvd #8030,301 5947832,,pfeiffer@mail.nih.gov,On incorporating biomarkers into models for absolute risk prediction models,1,Ruth,,Pfeiffer,"Division of Cancer Epidemiology and GeneticsNational Cancer Institute, NIH, HHS                             ",Mitchell,,Gail,"Division of Cancer Epidemiology and GeneticsNational Cancer Institute, NIH, HHS                       ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," Absolute risk is the probability that an individual who is free of agiven disease at an initial age, a, will develop that disease in thesubsequent interval (a, t]. Absolute risk is reduced by mortality fromcompeting risks. Models of absolute risk that depend on covariateshave been used to design intervention studies, to counsel patientsregarding their risks of disease and to inform clinical decisions.While models that predict the absolute risk of breast cancer,colorectal cancer or melanoma are well calibrated when validated inindependent data,  their discriminatory power is typically low, whichmakes them ill suited for screening applications.  One hope is toimprove their discriminatory ability by incorporating biomarkerinformation. Several statistical issues need to be addressed for suchan undertaking: 1) how to optimally incorporate these markers into arisk prediction model, and  2) how to assess the improvement of amodel from adding markers. We present methods for comparing theperformance of different absolute riskmodels. We illustrate our methods by comparing a model for breastcancer based on a woman's age, and personal characteristics includingreproductive risk factors, with a model that also includes geneticmarkers. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Diagnostic and screening tests,Epidemiologic methods,arwin@mail.med.upenn.edu,,Arwin Thomasson,,University of Pennsylvania,423 Guardian Dr.,5409987737,,arwin@mail.med.upenn.edu,Analyzing patient survival after deceased-donor kidney transplants: the novel use of time-varying covariates,1,Arwin,M,Thomasson,"University of Pennsylvania Department of Biostatistics, Philadelphia, PA",Peter,P,Reese,"University of Pennsylvania School of Medicine, Philadelphia, PA",Justine,,Shults,"University of Pennsylvania Department of Biostatistics, Philadelphia, PA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Analysis of patient survival after deceased-donor kidneytransplantation has typically focused on kidneys received fromstandard criteria donors.  However, due to the limited availability ofhigh-quality organs, clinicians have become increasingly interested inthe outcomes associated with potentially less-desirable organs.  Inparticular, there is a focus on kidneys from donors with an increasedrisk of HIV infection, from donors who experienced cardiac death, andfrom donors with other undesirable health characteristics. In thispresentation we describe the statistical methods used in analysis of aretrospective cohort study using data from the Organ Procurement andTransplantation Network.  In particular, we focus on the use oflogistic regression and non-proportional Cox regression for acomparison of rates of transplant rejection and patient survival forthe different donor types. In addition, we demonstrate the applicationof quasi-least squares regression for multiple correlated binaryoutcomes to analyze absolute two-year allograft survival and delayedgraft function.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Health policy applications,skang@uga.edu,,Sangwook Kang,Assistant professor,University of Georgia,129A Paul D. Coverdell Center,706-583-0211,,skang@uga.edu,Additive hazards model for case-cohort studies with multiple disease outcomes,1,Sangwook,,Kang,"Department of Epidemiology and Biostatistics, University of Georgia",Jianwen,,Cai,"Department of Biostatistics, University of North Carolina at Chapel Hill",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A case-cohort study design is widely used to reduce the cost of largecohort studies while achieving the same goals, especially when thedisease rate is low. A key advantage of the case-cohort study designis its ability to use the same subcohort for several diseases or forsubtypes of disease. In order to compare the effect of a risk factoron different types of diseases, times to different events need to bemodeled simultaneously. Valid statistical methods which take thecorrelations among the outcomes from the same subject into accountneed to be developed. To this end, we consider marginal additivehazards model for case-cohort studies with multiple disease outcomes.We also consider the generalized case-cohort designs which do notrequire sampling of all the cases, which is more realistic formultiple disease outcomes. We propose an estimating equation approachfor parameter estimation with two different types of weights.Asymptotic properties of the proposed estimators are investigated andtheir finite sample properties are assessed via simulations studies.The proposed methods are applied to the Busselton Health Study.",FALSE,FALSE,,FALSE,FALSE,FALSE,I have a class to teach on Tuesday. It would be great if I can present mine on Sunday or Monday so that I don't have to miss my class.,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Multivariate survival,guoy@umich.edu,,Ying Guo,,"University of Michigan, Department of Biostatistic",2297 Stone Road,734-262-1592,,guoy@umich.edu,Regression analysis on a covariate with heteroscedastic measurement error,1,Ying,,Guo,"Department of Biostatistics, University of Michigan",Roderick,,Little,"Department of Biostatistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider the problem of estimating the regression of an outcome Don a covariate X, where X is unobserved, but a variable Y whichmeasures X with error is observed. A calibration sample that measurespairs of values of X and Y is also available; we also consider thecase where the calibration sample includes values of D. The standardapproach for estimating the regression of D on X is to estimate thecalibration curve of X on Y and then replace unknown values of X bypredictions from this curve. An alternative approach is to multiplyimpute the missing values of X given Y and D based on an imputationmodel, and then use multiple imputation (MI) combining rules forinferences about the coefficients of the regression of D on X. Arecent paper by Friedman et al (2008) compares these two approaches.However, their work assumes the measurement error of Y has a constantvariance, whereas in many situations, the measurement error variancevaries as a function of X. We consider modifications of thecalibration prediction method and the multiple imputation method thatallow for non-constant measurement error variance, and compare thesemethods by simulation. The multiple imputation model is shown toprovide better inferences in this setting. Key words: measurement error, heteroskedasticity, regressioncalibration, multiple imputation",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Measurement error,giurcanu@louisiana.edu,,MIHAI C GIURCANU,ASSISTANT PROFESSOR,UNIVERSITY OF LOUISIANA AT LAFAYETTE,"301 RAYBURN ST, APT 567",3528700925,,giurcanu@louisiana.edu,THE BIASED-BOOTSTRAP FOR GMM MODELS,1,Mihai,C,Giurcanu,University of Louisiana at Lafayette,Brett,D,Presnell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this talk, I present some theoretical and empirical properties ofthe uniform and biased-bootstrap distribution estimators forgeneralized method of moments (GMM) models. This version of thebiased-bootstrap is a form of weighted bootstrap with weights chosento satisfy some constraints imposed by the model. A typicalbiased-bootstrap resample is obtained by resampling from a memberwithin a pseudo-parametric family of weighted empirical distributionson the sample. Because of its parametric nature, importance samplingcan be successfully used when the biased-bootstrap is iterated, byre-weighting the first level bootstrap resamples. The resultingprocedure yields an efficient and computationally feasible bootstraprecycling algorithm. I will present some consistency results of boththe uniform and biased-bootstrap estimators of the distributions ofGMM estimators and the J-test statistic. An application to thebootstrap calibrated confidence intervals shows some empirical resultson the finite sample properties of the proposed method. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Estimating equations,Empirical likelihood,krhamilton_7@hotmail.com,,Kiya R. Hamilton,Pre-Doctoral Trainee,University of Alabama at Birmingham,2954 Summit Drive,954-547-9117,,krhamilton_7@hotmail.com,DESIGN OF PREDICTIVE POWER METHOD WITH TWO ENDPOINTS,1,Kiya,R,Hamilton,University of Alabama at Birmingham,Leslie,A,McClure,University of Alabama at Birmingham,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Stochastic curtailment methods, such as conditional power and predictive power, can be used to asses the chance that a significant difference between two treatment groups may occur at the end of the study, given the data collected up to an interim time point. However, stochastic curtailment, methods do not currently allow for simultaneously testing more than one endpoint at a time. We extend the predictive power procedure to include two endpoints. Our focus is on the case where the two endpoints are independent. We will describe the methodology that has been developed to assess the predictive power for two independent outcomes, and how it may be extended to the case where the correlation is non-zero. We will compare our method with other group sequential methods, by assessing the average sample size and whether a 'correct decision' was made by the end of the study. We will also discuss some of the issues that may arise when monitoring multiple endpoints simultaneously. This approach is illustrated with real data from the Nitric Oxide Trial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Multiple testing,pdixon@iastate.edu,,Philip Dixon,Professor,Department of Statistics,Snedecor Hall,515-294-6828,,pdixon@iastate.edu,Statistical Ways to Choose a Distance Measure for Metabolomic Data,1,Philip,M,Dixon,"Department of StatisticsIowa State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Most methods for analyzing metabolomic data require the choice of adistance measure, but this choice is commonly made for non-statisticalreasons.  This talk will discuss two related issues: how to useproperties of the data to choose a distance measure and statisticalcriteria to evaluate the choice of measure.  In various data sets withbiological replicates, I find a power-law relationship between thevariance among replicates and the mean signal.  This suggests a newclass of variance-weighted distance measures.  These measures arecompared to traditional distance measures using two novel evaluationcriteria that focus on the performanceof the distance measure.  These are the repeatability given resamplingof metabolites and the consistency with a structure expected to bepresent in the dataset.  These methods are illustrated using data onmetabolites in the herbal plant genus Echinacea.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Metabolomics,Machine learning,jbao@sph.emory.edu,,Jieqiong Bao,M.S.,Emory University,Department of Biostatistics and Bioinformatics,404-7278210,404-7271370,jbao@sph.emory.edu,Latent Variable Regression for Multiple Outcomes with some Predictors Missing Non-randomly,1,Jieqiong,,Bao,"Department of Biostatistics and Bioinformatics,Emory University",Amita,,Manatunga,"Department of Biostatistics and Bioinformatics,Emory University",Andrew,,Taylor,"Department of Radiology, Division of Nuclear Medicine, Emory University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We are interested in developing a latent model for predicting multiple outcomes in terms of observed covariates. A complication occurs because some covariates are not always observed due to the nature of sampling scheme. For example, in the renal image studies, it is of interest to predict the need for furosemide and kidney obstruction based on characteristics of renal images. The predictors that are crucial for predicting kidney obstruction are only present when only a patient receives furosemide based on a clinical decision.   Two observed outcomes are the need for furosemide and the presence of kidney obstruction for each kidney which are evaluated by three experts. We propose a latent variable regression model to predict the underlying outcome while accounting for the missingness of the predictors. Our modeling framework provides estimates for the between-rater variability and within-rater variability simultaneously under maximum likelihood framework.  We apply to the nuclear medicine data from MAG3 renography studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Latent variables,Imaging,zzhang@ChristianaCare.org,,Zugui Zhang,"Ph.D.,",Senior Biostatistician,"131 Continental Drive, Suite 202",302-623-0673,,zzhang@ChristianaCare.org,Structural Equation Modeling for Quality of Life Data in Cardiovascular Disease,1,Zugui,,Zhang,Christiana Care Health System ,Paul,,Kolm,"Christiana Care Health System, Newark, Delaware",William,S.,Weintraub,"Christiana Care Health System, Newark, Delaware",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Quality of life data frequently arise in medical studies when the comprehensive measurements of medical cares of subjects are investigated.  For instance, in cardiovascular study, patient-reported health status data were collected.    However, the widely used traditional statistical analysis involves mainly descriptive or basic trends, ignoring the characteristics of unobserved factors, which may result in misleading conclusions.  The purpose of this study is to apply Structural Equation Modeling (SEM), a useful and effective tool for evaluating theories involving health related quality of life and other related patient outcomes, to evaluate the construct validity of the outcomes through the specification and testing of hypotheses linking different quality of life measures.  Patients and data were from COURAGE trial, comparing a strategy of percutaneous coronary intervention plus optimal medical therapy to optimal medical therapy alone.  General health status, measured using the RAND-36 with 7 domains, and Health status related to angina, evaluated via the Seattle Angina Questionnaire with 5 domains, were assessed directly from patients at baseline and at 1, 3, 6 and 12 months followed by annual evaluations for two more years.  Results highlight the evaluation of relationships among latent variables derived from quality of life data via SEM.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Latent variables,Applied data analysis,shsu@jhsph.edu,,Shu-Chih Su,,Merck,700 LOWER STATE RD Apt 21 C3,215-353-7780,,shsu@jhsph.edu,On the merits of voxel-based morphometric path-analysis for investigating volumetric mediation of a toxicants influence on cognitive function,1,Shu-chih,,Su,Merck,Brian,,Caffo,Johns Hopkins Bloomberg School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Previous study showed that lifetime cumulative lead dose was associated with persistent and progressive declines in cognitive function and with decreases in MRI-based brain volumes in former lead workers. Moreover, larger region-specific brain volumes were associated with better cognitive function. These findings motivated us to explore a novel application of path analysis to evaluate effect mediation, whether the association of lead dose with cognitive function is mediated through brain volumes, on a voxel-wise basis. Application of these methods to the former lead worker data demonstrated potential limitations in this approach. Moreover, a complimentary analysis using anatomically-derived regions of interest (ROI) volumes yielded opposing results, suggesting evidence of mediation. in the ROI-based approach, there was evidence that the association of tibia lead with function in three cognitive domains  was mediated through the volumes of total brain, frontal gray matter, and possibly cingulate. A simulation study was conducted to investigate whether the voxel-wise results arose from an absence of localized mediation, or more subtle defects in the methodology. Both the lead worker data results and the simulation study suggest that a null-bias in voxel-wise path analysis limits its inferential utility for producing confirmatory results. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Imaging,High dimensional data,fh6e@virginia.edu,,Feifang Hu,Professor,University of Virginia,Department of Statistics,434-924-3014,434-924-3076,fh6e@virginia.edu,Sequential Monitoring Response-Adaptive Randomized Clinical Trials,1,Feifang,,Hu,"Department of StatisticsUniversity of Virginia",Hongjian,,Zhu,"Department of StatisticsUniversity of Virginia",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In clinical trials, sequential monitoring procedures are oftenimplemented to stop the trials earlier, therefore reduce the cost andthe sample size. Response-adaptive randomization has been developedand demonstrated to be desirable randomization for many clinicaltrials. Is it possible to combine these two sequential procedurestogether in a clinical trial? In this talk, we answer this questiontheoretically by showing that the joint distribution of some importantsequential statistics is asymptotically Brownian process inresponse-adaptive randomized trials. Therefore, we can applysequential monitoring procedures to response-adaptive randomizedtrials. Simulated studies also support our theoretical results.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Clinical trials,brian.egleston@fccc.edu,,Brian L. Egleston,Associate Member,Fox Chase Cancer Center,Biostatistics Facility,215-214-3917,215-728-2553,brian.egleston@fccc.edu,Causal inference for intervention effects on nicotine withdrawal symptoms,1,Brian,L.,Egleston,Fox Chase Cancer Center,Karen,L.,Cropsey,University of Alabama School of Medicine ,Amy,B.,Lazev,Fox Chase Cancer Center,Carolyn,J.,Heckman,Fox Chase Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,"One problem with assessing effects of smoking cessation interventions on withdrawal symptoms is that symptoms are affected by whether participants abstain from smoking during trials.  Those who enter a randomized trial but do not change smoking behavior might not experience withdrawal related symptoms.  We provide a hypothetical example that demonstrates why estimating effects within observed abstention groups does not address this problem.  We demonstrate how estimation of effects within groups defined by potential abstention that an individual would have in either arm of a study can provide meaningful inferences.  We describe a methodology to estimate such effects, and use it to investigate effects of a combined behavioral and nicotine replacement therapy intervention on withdrawal symptoms in a prisoner population.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Clinical trials,kims2@mail.nih.gov,,Sung Duk Kim,Research Fellow,NICHD/NIH,6100 Executive Blvd,301-435-6930,,kims2@mail.nih.gov,Variable selection for identifying environmental contaminants associated with human fecundity,1,Sungduk,,Kim,"Biostatistics and Bionformatics Branch, Division of Epidemiology, Statistics and Prevention Research, Eunice Kennedy Shriver National Institute of Child Health and Human Development, NIH",Rajeshwari,,Sundaram,"Biostatistics and Bionformatics Branch, Division of Epidemiology, Statistics and Prevention Research, Eunice Kennedy Shriver National Institute of Child Health and Human Development, NIH",Germaine,M,Louis,"Epidemiology Branch, Division of Epidemiology, Statistics and Prevention Research, Eunice Kennedy Shriver National Institute of Child Health and Human Development, NIH",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A question of considerable interest is in identifying polychlorinated biphenyl (PCB) and lifestyle variables that are associated with human fecundity. Human fecundity is quantitatively measured through time-to-pregnancy, the number of cycles needed to get pregnant, a censored discrete survival time. Furthermore, PCBs measured are typically highly correlated. So in order to assess their association with human fecundity, requires variable selection in high-dimensional setup with outcome of interest being a discrete right-censored survival time. Such problems are also of interest in dealing with microarray data analysis. However, attention has typically focused on censored continuous survival time. Here, we consider a discrete survival time model. In this paper, we propose a Bayesian variable selection approach, which allows the identification of relevant chemical by jointly assessing sets of chemicals. The proposed method provides a unified procedure for the selection of relevant chemicals and the prediction of survivor functions. This is accomplished by building a stochastic search variable selection method within discrete survival model. We will investigate our methods through simulations and analysis of real example involving fecundity.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Variable subset selection/model selection,Survival analysis,dchowdhury@rti.org,,Dhuly Chowdhury,Research Statistician,RTI International,"6110 Executive Blvd, Suite 902",3017708234,,dchowdhury@rti.org,Compare Sample Size adjustment methods for Cluster Randomized Trial,1,Dhuly,,Chowdhury,RTI International,Hrishikesh,,Chakraborty,RTI International,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Cluster randomized trials, where interventions or treatments are randomly assigned to clusters and conclusions are made in individual level.  The individuals within a cluster are correlated and researchers need to account for the within cluster correlations during the sample size calculation and analysis.  The required sample sizes for cluster randomized trials are adjusted to account for correlated observations within clusters.  There are several statistical methods available to adjust sample size for clustering in cluster randomized trials and two widely used methods are (1) design effect adjustment method using intracluster correlation estimate and (2) adjustment based on between cluster coefficients of variation estimate. In this paper, we calculated the required sample sizes assuming the same hypothesis, power, and size to both adjustment methods for different design settings and conducted simulation exercises to compare the power of using the adjusted sample sizes.  We only considered the binary outcomes for all simulation exercises. Our simulation results suggested that if the individuals within a cluster are highly correlated then the design effect adjustment method requires more clusters compared with the coefficients of variation adjusted method to conduct the same cluster randomized trial and if the correlation is low then both methods are equally powerful.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Power analysis/sample size,Clustered data methods,lishaoyu@stt.msu.edu,,Yuehua Cui,Assistant Professor,Shaoyu Li,Department of Statistics and Probability,517-432-7098,517-432-1405,lishaoyu@stt.msu.edu,A regularized regression approach for dissecting genetic conflicts that increase disease risk in pregnancy,2,Yuehua,,Cui,"Department of Statistics and ProbabilityMichigan State University",Shaoyu,,Li,"Department of Statistics and ProbabilityMichigan State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many human diseases developed during pregnancy could be caused by the direct effects of both maternal and fetal genes, and/or by the indirect effects due to genetic conflicts. Genetic conflicts exist when the effects of fetal genes are opposed by the effects of maternal genes, or when there exists conflict between the maternal and paternal genes within the fetal genome. Dissecting genetic conflict effects that increase disease risk during pregnancy presents statistical challenges. In this talk, we consider a unified framework to model and test the genetic conflicts via a regularized regression approach. Our model is developed considering real situation in which the paternal information is often completely missing, an assumption that fails most current family-based study. A mixture model based penalized logistic regression is proposed for data sampled from a natural population. We develop a variable selection procedure to select significant genetic features. Simulation studies show that the model has high power and good false positive control under reasonable sample size and disease allele frequency. Our model provides a powerful tool for dissecting genetic conflicts that increase disease risk during pregnancy.Key Words: Complex disease, Genetic conflicts, Genomic imprinting,                    Maternal-fetal geno-type incompatibility, Penalized logistic regression  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Multiple testing,ligengxi@msu.edu,,Yuehua Cui,Assistant Professor,"Statistical genetics, Bioinformatics, Longitudinal",A-432 Wells Hall,(517) 432-7098,,ligengxi@msu.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,dengy@ipfw.edu,,Yihao Deng,,Indiana University Purdue University Fort Wayne,2101 E Coliseum Blvd,260-481-4185,,dengy@ipfw.edu,Latent variable model for the analysis of binary data collected on nuclear families,1,Yihao,,Deng,Indiana University Purdue University - Fort Wayne,Roy,,Sabo,Virginia Commonwealth University,N. Rao,,Chaganty,Old Dominion University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many genetic and phenotypic studies are focused on binary variableswithin nuclear families. The within family genetic and phenotypicbinary data are naturally dependent and hence correlated. In this paperwe present a latent variable model based on multivariate probit toanalyze the familial correlations. We will discuss some theoreticalproperties of the model including feasible ranges of the correlations.Using a stochastic representation we simplify maximum likelihoodestimation of the parameters and the Fisher information. We illustrateour methods with a real life example concerning the effects oferythrocyte adenosine triphosphate (ATP) levels among Caucasian familymembers.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Categorical data,Latent variables,lishaoyu@stt.msu.edu,,Yuehua Cui,Assistant Professor,Department of Statistics and Probability,Department of Statistics and Probability,517-432-7098,517-432-1405,lishaoyu@stt.msu.edu,A regularized regression approach for dissecting genetic conflicts that increase disease risk in pregnancy,2,Yuehua,,Cui,"Department of Statistics and ProbabilityMichigan State University",Shaoyu,,Li,"Department of Statistics and ProbabilityMichigan State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many human diseases developed during pregnancy could be caused by the direct effects of both maternal and fetal genes, and/or by the indirect effects due to genetic conflicts. Genetic conflicts exist when the effects of fetal genes are opposed by the effects of maternal genes, or when there exists conflict between the maternal and paternal genes within the fetal genome. Dissecting genetic conflict effects that increase disease risk during pregnancy presents statistical challenges. In this talk, we consider a unified framework to model and test the genetic conflicts via a regularized regression approach. Our model is developed considering real situation in which the paternal information is often completely missing, an assumption that fails most current family-based study. A mixture model based penalized logistic regression is proposed for data sampled from a natural population. We develop a variable selection procedure to select significant genetic features. Simulation studies show that the model has high power and good false positive control under reasonable sample size and disease allele frequency. Our model provides a powerful tool for dissecting genetic conflicts that increase disease risk during pregnancy.Key Words: Complex disease, Genetic conflicts, Genomic imprinting,                    Maternal-fetal geno-type incompatibility, Penalized logistic regression  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Multiple testing,jmanjour@fas.harvard.edu,,Justin Manjourides,,Harvard School of Public Health,SPH2-4th Floor,617-448-5758,,jmanjour@fas.harvard.edu,Improving Disease Surveillance by Incorporating Residential History,1,Justin,,Manjourides,"Department of Biostatistics, Harvard School of Public Health",Marcello,,Pagano,"Department of Biostatistics, Harvard School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In environmental studies associated with the etiology of a humandisease it is often of interest to determine whether there is anyrelationship between the location of an individual's abode and whetheror not the person is diseased. We sometimes attempt to answer thisquestion by studying the spatial distribution of individuals'addresses. But if the disease was triggered at some point in the past,then the current address is almost irrelevant. We propose that asubject's residential history be obtained for such studies and that itbe incorporated in the analysis via the incubation distribution forthe disease.In this talk we present a method for testing the difference betweentwo spatial histories.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Environmental and ecological applications,todd.nick@cchmc.org,,Todd Nick,Professor,Cincinnati Children's Hospital Medical Center,3333 Burnet AVE,513-636-0462,,todd.nick@cchmc.org,Statistical validity and power for testing for heterogeneous effects with quantitative traits and its application to pharmacogenetic studies,1,Todd,G,Nick,Cincinnati Children's Hospital Medical Center,Mi-ok,,Kim,,Chunyan,,Liu,,Yu,,Wang,,,,,,,,,,,,,,,,,,,,,,,,,,"In pharmacogenetic studies, it is often of interest to look for evidence of a difference in treatment effect in complementary groups. For example, such groups are typically defined based on a traditional classification of metabolizing group (e.g. poor versus extensive metabolizes). When the trait is quantitative and the comparison is between two groups, the conventional t-test or rank sum test are often used. To detect differences when heterogeneity is present, OBrien proposed extensions to the t-test and rank sum test and called these tests generalized t-test and generalized rank sum test. For the generalized tests, group membership is regressed against the trait using a quadratic model. We extend the generalized tests by using restrictive cubic splines. The statistical properties under different effect size and distributions were evaluated. Additionally, alternative hypothesis were generated assuming only a location shift, only a scale shift, and both location and scale shift. The generalized test using splines and OBriens generalized tests are compared with standard tests.  The generalized t-test using splines performed well in regard to power when the distributions are skewed or contaminated. The method provided no improvement over the generalized t-test when there was only a shift in location.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Biopharmaceutical research,Applied data analysis,okoh@smu.edu,,Ohn Jo Koh,,Southern Methodist University,"Dept. of Statistical Science, 3225 Daniel Avenue",469-826-3889,,okoh@smu.edu,Wavelet packet resampling for fMRI experiments,1,Ohn Jo,,Koh,"Dept. of Statistical Science, Southern Methodist University",William,R,Schucany,"Dept. of Statistical Science, Southern Methodist University",Richard,F,Gunst,"Dept. of Statistical Science, Southern Methodist University",Wayne,A,Woodward,"Dept. of Statistical Science, Southern Methodist University",,,,,,,,,,,,,,,,,,,,,,,,,"Identification of activated brain regions in response to externalstimuli is important for understanding human brain activity. The 4Dspatiotemporal wavelet packet resampling method by Patel et al.generates null data that preserve average background spatialcorrelation better than the wavelet resampling method. Activatedregions are determined by testing observed data against these nulldistributions. Instead of resampling axial slices first and thenacross axial slices, a newly developed method resamples threedimensions of the brain using a 3D wavelet packet decomposition. A newstatistic that measures spatial decorrelation is investigated fordetermining an orthogonal basis for wavelet packet decomposition. Inaddition, a resampling method that generates the null distribution forcomparison of control versus treatment is examined.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Imaging,Nonparametric methods,noeda@muohio.edu,,Douglas A. Noe,Assistant Professor of Statistics,Miami University,301 S Patterson Ave,5135295838,,noeda@muohio.edu,Statistical model for a dual-color tag system for investigating virus-virus interactions,2,Jing,,Zhang,Miami University,Douglas,A,Noe,Miami University,Stephen,E,Wright,Miami University,A John,,Bailer,Miami University,,,,,,,,,,,,,,,,,,,,,,,,,"A test system has been developed for staining cells to detect whether the cells have been infected by one or more viruses.  A distinction between the states of viral infectivity (virus 1 infection, virus 2 infection, both virus 1 and 2 infection, no infection) leads to a multinomial framework in which cell probabilities can be defined in terms of attack rates and a possible window of time in which a cell might be infected by both viruses.  This framework is developed and used to develop tests and interval estimates of model parameters in a study of a single cell line and to develop a test of virus infectivity equivalence between two cell lines.  The performance of this test and estimation procedure is explored in a small simulation study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Infectious disease models,Applied data analysis,lisherry@mail.nih.gov,,yan li,,NCI,6427 parkmont dr,8172725683,,lisherry@mail.nih.gov,Application of nonparametric percentile regression to body mass index percentile curves from survey data,1,Yan,,Li,"Biostatistics Branch, NCI",Barry,I.,Graubard,"Biostatistics Branch,  NCI",Edward,L.,Korn,"Biometric Research Branch, NCI",,,,,,,,,,,,,,,,,,,,,,,,,,,,," Increasing rates of overweight among children in the U.S. stimulated interest in obtaining national percentile curves of body size to serve as a benchmark in assessing growth development in clinical and population settings.  In 2000, the Centers for Disease Control and Prevention (CDC) developed conditional percentile curves for Body mass index (BMI) for ages 2-20 years.  The 2000 CDC BMI-for-age curves are partially parametric and only partially incorporated the survey sample weights in the curve estimation. As a result, they may not fully reflect the underlying pattern of BMI-for-age in the population.  This motivated us to develop a nonparametric double-kernel-based method and automatic bandwidth selection procedure.  We include sample weights in the bandwidth selection, conduct median correction to reduce small-sample smoothing bias, and rescale the bandwidth to make it scale-invariant. Using this procedure we re-estimate the national percentile BMI-for-age curves and the prevalence of high-BMI children in the U.S.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survey research data,Nonparametric methods,claudia.pedroza@uth.tmc.edu,,Claudia Pedroza,Assistant Professor,UT School of Public Health,1200 Herman Pressler Dr.,713-500-9514,,claudia.pedroza@uth.tmc.edu,Statistical Methods in Healthcare Quality Improvement,1,Claudia,,Pedroza,UT School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this talk, we review the methods that are currently being used to evaluate quality improvement in health care. In particular, we discuss the use of SPC control charts and other methods originally derived for industrial quality control. We discuss the advantages and limitations of these methods and propose that a Bayesian approach is ideally suited for QI.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Health services research,Bayesian methods,binzhang@bu.edu,,Bin Zhang,Assistant Professor,Boston University School of Medicine,"650 Albany Street, Suite X200",617-414-1229,,binzhang@bu.edu, Pseudolikelihood Ratio Tests with Biased Observations,1,Bin,,Zhang,Boston University,Joan,X,Hu,Simon Fraser University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper considers pseudolikelihood ratio tests with biased observations using auxiliary information.The pseudolikelihood functions are constructed withoutspecifying the association between the primary varialbe and theauxiliary variables. We derive the asymptotic distributions of the test statistics and examine finite-sample properties of the testing procedures via simulation. The methodology is illustrated byan example involving kindergarten readinessskills in children with sickle cell disease.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Applied data analysis,gao2@oakland.edu,,Xiaoli Gao,Assistant Professor,Oakland University,Department of Mathematics and Statistics,248-370-3440,,gao2@oakland.edu,On the study of LAD-Lasso in high-dimensional settings,1,Xiaoli,,Gao,Oakland University,Jian,,Huang,University of Iowa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Penalized regularization methods can achieve variable selection andestimation simultaneously by using specially designed penaltyfunctions. The LAD regression is an interesting and robust alternativeto the LS method. We propose a penalized LAD regression with the Lassopenalty (LAD-Lasso) and study the consistency properties of LAD-Lassounder some conditions in `large p, small n' settings. In thispresentation, we summarize the theoretical properties of LAD-Lasso onboth estimation and model evaluation aspects  in the high-dimensionalcase. The finite sample behavior of this estimator is studied andcompared to LS-Lasso via simulation studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Variable subset selection/model selection,rfan@stat.tamu.edu,,Ruzong,associate professor,Texas A&M University,447 Blocker Building,979-845-3152,,rfan@stat.tamu.edu,Extended Homozygosity Score Tests to Detect Positive Selection in Genome-wide Scans,1,Ming,,Zhong,Texas A&M University,Kenneth,,Lange,UCLA,Jeanette C.,,Papp,UCLA,Ruzong,,Fan,Texas A&M University,,,,,,,,,,,,,,,,,,,,,,,,,"In this article, we develop test statistics to detect excess homozygosity: (a) an extended genotype-based homozygosity test (EGHT), (b) a hidden Markov model test (HMMT), and (c) an extended haplotype-based homozygosity test (EHHT). The null hypothesis of all three tests assume random mating and Hardy-Weinberg equilibrium (HWE). They differ in how they treat linkage disequilibrium. The null hypothesis of EGHT assumes linkage equilibrium in addition to HWE.The EHHT conditions on existing linkage disequilibrium and it tests haplotype version of HWE.The HMMT stands between these two extremes and assumes pairwise but no higher-order disequilibrium interactions. We evaluate by simulation the false positive rates for the EGHT and HMMT under the null hypothesis of EGHT and find that HMMT is more conservative. All three methods are then applied to the HapMap Phase II data. We are able to replicate previous findings of strong positive selection in 19 autosome genomic regions out of 20 candidates.Over the entire genome, our EGHT and HMMT statistics score lowest in African sample (YRI) and highest in east Asian sample (CHB+JPT), with European sample (CEU) intermediate. Across the genome, EHHT scores are generally low with sharp spikes in only a few regions. Based on the high EHHT scores and population differentiations, we identify new candidate regions which may undergo recent selection.",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_oral,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,jhilbe@aol.com,,Joseph Hilbe,Professor,Arizona State University,7242 W. Heritage Way,520-723-1368,,jhilbe@aol.com,Derivation and software implementation of the canonical negative binomial model,1,Joseph,M,Hilbe,Arizona State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The negative binomial regression model can be parameterized in several  ways. I discuss the software algorithms used to model the negative  binomial for each of these parameterizations, and how each is to be interpreted for a given data situation. The canonical negative binomial, NB-C, developed by the presenter, is described in detail and  related to other parameterizations of the negative binomial; eg NB-1,  NB-2, NB-H, NB-P. and ZIP/ZINB. The presenter is author of /Negative  Binomial Regression/ (2007, Cambridge University Press), has been Software Reviews Editor of The American Statistician since 1997, and was the first to implement the negative binomial into commercial generalized linear models software. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Categorical data,Generalized linear models,Keith.Muller@biostat.ufl.edu,,Keith E. Muller,Professor and Director,University of Florida,Dept of EHPR,352-265-0111 x86331,352-265-8047,Keith.Muller@Biostat.ufl.edu,Achieving Covariance Robustness in the Linear Mixed Model,2,Matthew,J.,Gurka,University of Virginia,Keith,E.,Muller,University of Florida,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The flexibility of the general linear mixed model in describing howrepeated measurements on individuals covary has helped create a mythof robustness to mis-specification of the covariance model, especiallyin large samples.  Previous work has revealed biased inference aboutthe fixed effects, particularly for small samples, when the covariancestructure is misspecified.  In contrast to previous work, we proveanalytically that incorrectly assuming homogeneity within subjects(i.e., compound symmetry) leads to biased inference about the fixedeffects, in both small and large samples.  An unstructured covarianceassumption has appeal when interest lies in the fixed effects. However, convergence can be a problem in small samples.  We rely on acombination of theoretical and numerical results to examine the impactof various covariance model strategies on accuracy of inference aboutthe fixed effects.  Strategies include varying the approximation forthe test statistic, such as the Kenward-Roger approach, and using thesandwich estimator of the fixed effects covariance matrix.  However,we avoid model selection techniques.  The results lead to practicalguidance in achieving accurate inference about fixed effects inferencewith unknown covariance structure.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Biopharmaceutical research,mixed modelschengjie@wustl.edu,,Chengjie Xiong,Dr.,Washington University,"660 S Euclid, Box 8067",3143623635,,chengjie@wustl.edu,Meta-analyses on the rate of change over time when individual patient data are partly available,1,Chengjie,,Xiong,"Division of BiostatisticsWashington UniversitySt. Louis, MO 63110",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Longitudinal studies are often conducted to estimate and compare the longitudinal rate of changes on biological markers. Most up-to-date scientific evidences on the rate of change can only be obtained when all existing estimates from the literature and the most up-to-date individual patient longitudinal data are jointly analyzed statistically. This article provides a unified approach of meta-analysis across a group of longitudinal studies within the framework of likelihood principle. We propose a general linear mixed effects model to conduct meta-analyses when individual patient longitudinal data are only available for some of the studies and summary statistics on the rate of changes have to be used for the rest of the studies. Through an appropriate augmentation of all available data, we show that the standard statistical procedures such as PROC MIXED/SAS can be used to find the maximum likelihood estimates to the rate of change and obtain appropriate statistical inferences. We also propose measures of heterogeneity for the rate of changes based on our proposed model of meta-analyses. Finally, we demonstrate our proposed methodology through a real life example studying the longitudinal rate of cognitive changes as a function of apolipoprotein E4 genotype among elderly individuals.",FALSE,FALSE,,FALSE,FALSE,FALSE,"I would like the presentation to be on March 18, 2009.",oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Random effects,jxl982@psu.edu,,Juyoun Lee,Ph.D. Candidate,"Department of Statistics, Pennsylvania State Unive",330A Thomas Building,8144411761,,jxl982@psu.edu,Sampling tables given a set of conditionals,1,Juyoun,,Lee,"Department of Statistics, Pennsylvania State University",Aleksandra,,Slavkovic,"Department of Statistics, Pennsylvania State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Federal agencies and other organizations publish a data summarized inarrays of non-negative integers, called a contingency table. Whenreleasing the data, it is necessary to prevent the sensitiveinformation of the individuals from being disclosed. In statisticaldisclosure limitation, we must maintain a balance between disclosurerisk and data utility for the purposes of statistical inference. Onemethod of achieving this balance is to release partial informationabout the original data; in practice, many federal agencies andmedical institutions release data summarized in the form of marginalsums, conditional probabilities, or odds-ratios. Sampling methods formulti-way contingency tables given a set of marginal sums have beenstudied in diverse ways while there is almost no literature aboutsampling of tables given a set of conditional probabilities. Here, wefocus on a set of conditional probabilities instead of a set ofmarginal sums. We describe the Markov chain Monte Carlo (MCMC)algorithm based on the algebraic tools for sampling contingency tableswith given conditional probabilities. This algorithm can be used forBayesian computation of posterior distribution and assessment of datautility and disclosure in statistical disclosure limitation. Wedemonstrate the MCMC algorithm with examples and discuss itsadvantages and disadvantages. We then discuss their feasibility forsampling tables given a set of conditional probabilities.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Categorical data,Bayesian methods,yabing_mai@merck.com,,Yabing Mai,,Merck,126 East Lincoln Ave,732 594 6081,732 594 6075,yabing_mai@merck.com,Bias-corrected Logrank Test with Dependent Censoring,1,Yabing,,Mai,Merck,Eric,V,Slud,University of Maryland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The asymptotic property of the logrank and stratified logrank tests varies across different types of assumptions regarding the dependence of the censoring and the survival times. When the treatment group and the covariates are conditionally independent given that the subject is still at risk, the logrank statistic is asymptotically standard normally distributed under the null hypothesis. Given this assumption, the stratified logrank statistic has similar asymptotic properties to the logrank statistic. However, if the assumption of conditional independence fails, the logrank statistic is generally biased and the bias is considered non-negligible We discuss and extend an available bias-correction method of DiRienzo and Lagakos (2001) with unknown and estimated censoring distribution function given the provided treatment group and covariates. We obtain the correct asymptotic distribution of the bias-corrected test statistic when stratum-based Kaplan-Meier estimators of the conditional censoring distribution are substituted into the statistic. Within this framework, we prove the asymptotic unbiasness of the corrected test and find a consistent variance estimator. Major theoretical results and motivations of future studies are confirmed by a series of simulation studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Nonparametric methods,lpoisson@umich.edu,,Laila M Poisson,,University of Michigan,1420 Washington Heights,734-647-0201,(734) 763-2215,lpoisson@umich.edu,Set Enrichment Analysis Methods for Integromic Studies,1,Laila,M,Poisson,"Department of Biostatistics, School of Public Health, University of Michigan, Ann Arbor, Michigan, USA",Debashis,,Ghosh,"Department of Statistics and Huck Institute of Life Sciences, Penn State University, University Park, Pennsylvania, USA",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The medical community is embracing omics technologies and manyelements of systems biology are now being measured on a global scalefor clinical samples. As with gene expression, the follow up to anylist of differential elements is to search for evidence of enrichmentof pathways or other groupings. In this talk we discuss the problem ofintegrating set enrichment analysis to incorporate data from multipleomics technologies. Issues of sampling and inference will bediscussed. Examples are motivated by gene expression and metabolomicdata on matched samples of prostate cancer progression. ",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Metabolomics,Genomics,yj6k@virginia.edu,,Youngsook,Jeon,University of Virginia,1725 JEFFERSON PARK AVE APT 5,571-641-9758,,yj6k@virginia.edu,Randomization Tests of Multi-arm Randomized Clinical Trials,1,Youngsook,,Jeon,University of Virginia,Feifang,,Hu,University of Virginia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Randomization test is a standard inference method for two-armrandomized clinical trials. However, how to conduct randomizationtests for multi-arm clinical trials is unclear. It is becausetreatments other than those of interest are also involved in a randomrearrangement process in the standard randomization procedure forpairwise comparisons. In this talk, we deal with thismultiple-treatment issue of randomization tests. We propose newrandomization testing method by which true difference in the pair oftreatments can be assessed without other treatments interference. Theproposed method is theoretically well-founded and can be easilyimplemented in the practice due to its computational feasibility. Somenumerical studies are also presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Nonparametric methods,Clinical trials,ENAR@mshort.authorized.yon.net,,Margaret Short,,University of Alaska Fairbanks,P.O. Box 750125,505-695-9627,,ENAR@mshort.authorized.yon.net,Bayesian modelling of wind fields using surface data collected over land,1,Margaret,,Short,University of Alaska Fairbanks,Javier,,Fochesatto,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose an approach to modeling wind fields in which the error structure includes the type of instrumentation used to collect such data over land, namely anemometers (for wind speed) and vanes (for wind direction). Thus the model can handle both the periodicity of the wind direction and the non-negativity of the wind speed. Measurement error depends in part on the wind speed and is incorporated in the model using a circular distribution. We use a Bayesian approach and fit the models using Markov chain Monte Carlo.  Model performance is illustrated with a Swiss surface wind data set. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Bayesian methods,samir.lababidi@fda.hhs.gov,,Samir,,Mathematical Statistician,1350 Piccard Drive,240 276 3110,,samir.lababidi@fda.hhs.gov,Does a gene expression classifier have a clinical value?,1,Samir,,Lababidi,FDA / CDRH,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The early papers in microarray gene expression promised better prediction in cancer outcome classification than what have been done before for prognostic and diagnostic medicine. However, it became clear later that the actual gain in predictive ability due to the use of gene expression classifiers may have been sometimes exaggerated and in need of careful evaluation. In fact, a statistical significant classification rate obtained by, e.g., validation in an independent dataset, does not necessarily imply that the classifier would have the clinical value of interest. Here in this talk, we will present statistical analysis methods to establish the clinical value of a classifier and compare its performance to models using clinical covariates alone.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Diagnostic and screening tests,xiaoxiak@biostat.umn.edu,,Xiaoxiao,,university of minnesota,1293 fifield place,651-235-4241,,xiaoxiak@biostat.umn.edu,A Bayesian approach to the alignment of mass spectra,1,Xiaoxiao,,Kong,"Division of Biostatistics, School of Public Health, University of minnesota",Cavan,,Reilly,"Division of Biostatistics, School of Public Health, University of minnesota",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The need to align spectra is a problem that arises in mass spectrometry. Here we present a novel Bayesian alignment approach. The approach is based on a parametric model which assumes the spectrum and alignment function are Gaussian processes, but the alignment function is monotone. We show how to use the EM algorithm to find the posterior mode of the alignment functions and the mean spectrum for a patient population. We apply the method to characterize the proteomeof a set of patients receiving lung transplants.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Proteomics,Bayesian methods,kolaczyk@math.bu.edu,,Eric Kolaczyk,Associate Professor,Boston University,Department of Mathematics and Statistics,617-353-5208,,kolaczyk@math.bu.edu,Network Filtering with Application to Detection of Gene Drug Targets,1,Eric,D,Kolaczyk,Boston University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A canonical problem in statistical signal and image processing is thedetection of faint targets against complex backgrounds, which has beenlikened to the proverbial task of `finding a needle in a haystack'.  Weconsider the task of target detection when the `background' is neitherone- or two-dimensional but rather in the form of an association network.We model the acquisition of network data, including the potential presenceof targets, using a system of sparse simultaneous equation models (SSEMs).In this context, detection is approached as a two-step procedure,involving (i) statistical inference and removal of `background' networkstructure, using tools of sparse inference, and (ii) outlier detection inthe network-filtered residuals. Theoretical performance of the methodologycan be characterized using a combination of tools and concepts from sparseinference, compressive sampling, random matrix theory, and spectral graphtheory.  We illustrate the practical capabilities of this approach usingsimulations and the problem of drug target detection in the context of anetwork of gene interactions.",FALSE,FALSE,,FALSE,FALSE,FALSE,"MWF requires me to be on campus to teach the first week of statistics in a multi-disp course.  This is a brand new course and team-taught (but with CS and Math people, unfortunately, and not other statisticians).  So I cannot turn this over to someone else.  Organizer (H. Ombao) has already notified Brent Coull.",invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Multivariate methods,Microarray analysis,Networkstimmyb@iastate.edu,,Tim Bancroft,Research Assistant,Iowa State University,1220 Truman Place,6124814526,,timmyb@iastate.edu,Estimation of False Discovery Rate Using Permutation P-Values with Different Discrete Distributions,1,Tim,,Bancroft,Iowa State University,Dan,,Nettleton,Iowa State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The false discovery rate (FDR) is a multiple testing error rate which describes the expected proportion of type I errors to the total number of rejected hypotheses.  Benjamini and Hochberg introduced this quantity and provided an estimator that is conservative when the number of true null hypotheses, m_0, is smaller than the number of tests, m.  Replacing m with m_0 in Benjamini and Hochberg's procedure reduces the conservative bias, but requires estimation as m_0 is unknown.  Methods exist to estimate m_0 when the m p-values are distributed continuous U(0,1) under H_0.  This talk discusses how to estimate m_0 and therefore FDR when the m p-values are from a mixture of different discrete distributions resulting from permutation testing for data with many zeros.  The method will be demonstrated through an analysis of proteomics data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Multiple testing,Nonparametric methods,thomas.aquinas@vatican.gov,,Thomas Aquinas,Cardinal,Collegia cardinales,1000 vatican city,800-325-5612,,thomas.aquinas@vatican.gov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,blogan@mcw.edu,,Brent Logan,Associate Professor,Medical College of Wisconsin,8701 Watertown Plank Road,414-456-8849,,blogan@mcw.edu,Marginal models for clustered time to event data with competing risks using pseudo-values,1,Brent,R,Logan,Medical College of Wisconsin,Mei-Jie,,Zhang,Medical College of Wisconsin,John,P,Klein,Medical College of Wisconsin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many time-to-event studies are complicated by the presence of competing risks and by nesting of individuals within a cluster, such as patients in the same center in a multi-center study.  Several methods have been proposed for modeling the cumulative incidence function with independent observations.  However, when subjects are clustered, one needs to account for the presence of a cluster effect either through frailty modeling of the hazard or subdistribution hazard, or by adjusting for the within-cluster correlation in a marginal model.  We propose a method for modeling the marginal cumulative incidence function directly.  We compute leave one out pseudo-observations from the cumulative incidence function at several time points.  These are used in a generalized estimating equation to model the marginal cumulative incidence curve, and obtain consistent estimates of the model parameters.  A sandwich variance estimate is derived to adjust for the within cluster correlation.  The method is easy to implement using standard software once the pseudo-values are obtained, and is a generalization of several existing models.  Simulation studies show that the method has good operating characteristics.  We illustrate the method on a dataset looking at outcomes after bone marrow transplantation.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Clustered data methods,rafiq@hsc.edu.kw,,Rafiqul I Chowdhury,Senior Lecturer,"HIA,FAHSN, Kuwait University",PO BOX- 31470,965 99503456,,rafiq@hsc.edu.kw,"SAS/IML for Parameter Estimation of Logistic Regression for Transition, Reverse Transition and Repeated Transition from Follow-up Data",1,Rafiqul,I,Chowdhury,"Deaprtment of Health Information AdministrationFaculty of Allied Health SciencesKuwait UniversityKuwait",M,A,Islam,"Department of StatisticsFaculty of ScienceDhaka UniversityDhaka, Bangladesh",Shahariar,S,Huda,"Deaprtment of Statistics and Operations ResearchFaculty of ScienceKuwait UniversityKuwait",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the past, most of the works on Markov models dealt with estimation of transition probabilities for first or higher orders and it appears to be restricted due to over-parameterization though several attempts have been made to simplify. Muenz and Rubinstein (1985) employed logistic regression models to analyze the transition probabilities from one state to another for first order and the model was extended for higher order by Islam and Chowdhury (2006). Islam and Chowdhury (2007), using the Chapman-Kolmogorov equations, introduced an improvement over the previous methods in handling runs of events by expressing the conditional probabilities in terms of the transition probabilities generated from Markovian assumptions. They introduced three sets of models namely transition, reverse transition and repeated transition to take account of unequal intervals in the occurrence of events.To estimate the parameters of the models proposed by Islam and Chowdhury (2007), extensive pre-processing and computations are needed to prepare the data before one can use the standard available procedures in existing statistical software. In this paper we demonstrate a program developed using SAS/IML to estimate the parameters of the proposed model. The program has been demonstrated using follow-up data on Health and Retirement Survey (HRS) from USA.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Computational methods,Longitudinal data,paul@prev.org,,Paul J. Gruenewald,Scientific Director,"Prevention Research Center, Pacific Institute for","1995 University Ave., Ste. 450",5104861111x5738,5106440594,paul@prev.org,Modeling the Methamphetamine Epidemic in California,1,Paul,J,Gruenewald,"Prevention Research CenterPacific Institute for Research and Evaluation1995 University Ave., Ste. 450Berkeley, CA  94704",William,R,Ponicki,"Prevention Research CenterPacific Institute for Research and Evaluation1995 University Ave., Ste. 450Berkeley, CA  94704",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Rates of methamphetamine abuse and dependence have reached epidemic levels in California over the past three decades, with rates increasing by about 200% over the past 8 years and currently increasing by 20% per year.  Three central questions are of concern in studies of the epidemic:  Are there natural limits to epidemic growth?  Are there correlated patterns of epidemic growth over space and time?  Are specific population frailties related to rates of growth?  This paper presents the results of Bayesian space-time disease models applied to the assessment of geographic patterns of methamphetamine abuse and dependence across California cities over 27 years.  Results of these modeling exercises illuminate the difficulties of assessing limits to growth of complex evolving epidemics, identify geographically correlated patterns of epidemic growth and decline, and suggest some population characteristics that accelerate rates of spatially correlated growth across human populations.  Some concluding observations are offered regarding modeling needs for predicting the development of future drug epidemics.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Epidemiologic methods,gelfondjal@uthscsa.edu,,Jonathan Gelfond,Assistan Professor,UT - Health Science Ctr at San Antonio,Dept of Epidemiology and Biostats; Mail Code 7933,210-567-0836,210 567 0921,gelfondjal@uthscsa.edu,Order Reversal Detection (ORD) for Analysis of Splice-Junction Microarrays,1,Jonathan,A,Gelfond,UT Health Science Ctr San Antonio,Luiz,,Penalva,UT Health Science Ctr San Antonio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Alternative splicing is a substantial source for expanding the functional capacity of the genome.  Splice junction microarrays allow the interrogation of thousands of splice junctions, but the data analysis is difficult and error prone because of the increased complexity relative to conventional gene expressional analysis.  We present Order Reversal Detection (ORD) as a method for identifying alternative splicing events based upon a straightforward probabilistic model comparing the over or underrepresentation of two or more competing isoforms.  ORD has advantages over commonly used methods it has resistance to false positive errors due to nonlinear trends in microarray measurements.  Further, ORD does not depend on prior knowledge of splicing isoforms.  We also provide a means of visualizing the data, and a means of matching of alternative splicing patterns between different tissue types.  The example data is from different cell lines of glioblastoma tumors assayed with JIVAN microarrays.",FALSE,FALSE,,FALSE,FALSE,TRUE,Workshop for junior researchers,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Microarray analysis,Genomics,Alternative Splicingkwilkins@idcrp.org,,Kenneth J Wilkins,Assistant Professor,"Infectious Disease Clinical Research Program, Dept",Uniformed Services University of the Health Sciences,301-295-0592,301-295-1812,kwilkins@idcrp.org,An approach to sensitivity analysis of nested case-control studies with incomplete follow-up,1,Kenneth,J,Wilkins,"Infectious Disease Clinical Research Program,Dept. of Preventive Medicine & Biometrics,Uniformed Services University of the Health Sciences",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,[ please see abstract in alternate kwilkins entry ],FALSE,FALSE,,FALSE,FALSE,TRUE,T1- Mon 8:30,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Defense and national security applications,Missing data,kaushik.ghosh@unlv.edu,,Kaushik Ghosh,,University of Nevada Las Vegas,4505 Maryland Parkway,702-895-0392,,kaushik.ghosh@unlv.edu,Bayesian Cancer Trend Analysis,2,Pulak,,Ghosh,Emory University,Kaushik,,Ghosh,University of Nevada Las Vegas,Ram,C,Tiwari,Food and Drug Administration,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Annual Percentage Change (APC) summarizes trends in age-adjusted cancer rates over short time-intervals. This measure assumes linearity of the logarithms of rates over the intervals in question, which may not be valid, especially for relatively longer time-intervals. An alternative is the Average Annual PercentageChange (AAPC), which computes a weighted average of APC values over intervals where log-rates are piecewise linear. In this article, we propose a Bayesian approach to calculating APC and AAPC values from age-adjusted cancer rate data. The procedure involves modeling the corresponding counts using age-specificPoisson regression models with a log-link function that contains unknown joinpoints. The regression slope parameters, including slopes at the joinpoints are assumed to have a normal-inverse-gamma setup and the joinpoints are assumed to be uniformly distributed subject to order-restrictions. Finally, age-specific intercept parameters are modeled nonparametrically using a Dirichlet process prior. The proposed method can be used to construct Bayesian credible intervals for AAPC using age-adjusted mortality rates. Simulation studies are used to demonstrate the success of the method in capturing trend-changes. Finally, the proposed method is illustrated using data on prostate cancer incidence.",FALSE,FALSE,T4: Receiver Operating Characteristic (ROC) Curves,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Cancer applications,Bayesian methods,haow@mail.med.upenn.edu,,Hao Wang,,"Department of Biostatistics, University of Penssyl","5th floor, Blockley Hall",2152669203,,haow@mail.med.upenn.edu,Modeling heaping in longitudinal self-reported cigarette counts,1,Hao,,Wang,"Department of Biostatistics and Epidemiology, University of Pennsylvania",Daniel,F,Heitjan,"Department of Biostatistics and Epidemiology, University of Pennsylvania",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In studies of smoking behavior, some subjects report exact cigarette counts, whereas others report rounded-off counts, particularly multiples of 20, 10 or 5.  This form of data reporting error, known as heaping, can bias the estimation of parameters of interest such as mean cigarette consumption. Heaping in daily reported cigarette counts compounds the problem by affecting estimates of change in consumption in addition to marginal mean counts. We present a model to describe longitudinal heaped count data. The model posits that the reported cigarette count is a deterministic function of an underlying precise cigarette count variable and a heaping behavior variable, both of which are latent variables that are at best partially observed. To account for correlations in longitudinal cigarette consumption, our method specifies separately the correlation of true cigarette smoking and the reporting behavior within a subject, but models them simultaneously. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Latent variables,din.chen@sdstate.edu,,Din Chen,Professor,South Dakota State University,1062 Western Ave,605-688-6212,,din.chen@sdstate.edu,Incorporating Historical Control Information into Quantal Bioassay with Bayesian Approach,1,Din,,Chen,South Dakota State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A Bayesian approach with an iterative reweighted least squares is used to incorporate historical control information into quantal bioassays to estimate the dose-response relationship, where the logit of the historical control responses are assumed to have anormal distribution. The parameters from this normal distribution are estimated from both empirical and  full Bayesian approaches with a marginal likelihood function being approximated by Laplace's Method.A comparison is made using real data between estimates that includethe historical control information and those that do not. It wasfound that the inclusion of the historical control information improves the efficiency of the estimators. In addition, this logit-normal formulation is compared with the traditional beta-binomial for its improvement in parameter estimates.Consequently the estimated dose-response relationship is used to formulate the point estimator and confidence bands for $ED(100p)$ for various values of risk rate $p$ and the potency for any dose level.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Biopharmaceutical research,stuart.gansky@ucsf.edu,,Stuart Gansky,,UCSF,"3333 California St, Ste 495",415-502-8094,415-502-8447,stuart.gansky@ucsf.edu,Health Disparity Indices - Simulations of Underlying Dependencies,1,Stuart,A,Gansky,"University of California, San Francisco, Center to Address Disparities in Children's Oral Health",Nancy,F,Cheng,"University of California, San Francisco, Center to Address Disparities in Children's Oral Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The US National Center for Health Statistics (NCHS) recent monographfor reporting health disparities (HDs) (Keppel et al. 2005) includescommonly used health disparities indices (HDIs).  Some authors claimall HDIs depend on the underlying prevalence (e.g. Scanlan 2006). Witha simple simulation we recently illustrated that some HDIs depend onunderlying prevalence but others do not (Cheng et al. 2008).  Thisinvestigation studied how HDIs are affected by underlying factors(e.g. prevalence, sample size, standard error, and strength ofassociation).  HDIs included absolute and relative measures, SlopeIndex of Inequality (SII), Relative Index of Inequality (RII) for meanand ratio, health concentration index (C), and entropy (Theil'sindex). HDIs were estimated for the California Oral Health NeedsAssessment of Children 2004-5, a complex stratified cluster samplesurvey (N=21,399), to assess associations of race/ethnicity andsocioeconomic position (SEP) (%free/reduced-price lunch (FRL) programin schools) to oral health outcomes such as rampant caries.Simulations varied underlying prevalence while keeping other factorsconstant (e.g. sample size, strength of association with SEPcovariate).  Then other factors were varied in simulations.  Westudied the behavior of HDIs based on underlying properties. Support:US DHHS NIH/NIDCR R03DE018116.",FALSE,FALSE,,FALSE,FALSE,TRUE,wife's birthday on March 18 (need to be home that night!),presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Epidemiologic methods,Survey research data,yjinkim@ewha.ac.kr,,Yang-Jin Kim,Ph.D,"Department of Statistics, Ewha Womans University",11-1 Daehyun-Dong Sodaemum-Ku,82-1-3277-6706,,yjinkim@ewha.ac.kr,Regression analysis of clustered interval censored data with informative cluster size,1,Yang-Jin,,Kim,"Department of Statistics, Ewha womans university, 11-1 Daehyun-dong, Sodaemun-gu, Seoul, 120-750, Korea",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Interval censored data are often observed in the study of infectiondata where time to infection is not exactly observed and is only known to occur within a certain interval. Several techniques to analyzeinterval censored data have been suggested with independentassumption. Sometimes, such interval censored data comprised of clusters and consequent failure times do not hold independentassumption any more. Furthermore, when cluster size includes someinformation about the failure time, informative cluster size shouldhave been considered in the analysis. In this study, we discuss a joint model for the analysis for clustered interval censored data with informative cluster size.  To investigate possible association between failure time and cluster size, PH model and ordinal regression model are applied for failure time and cluster size, respectively. Then a bivariate random effect is adopted to connect two models.  Simulation studies areperformed to evaluate a finite sample properties and a lymphatic filariasis study study is analyzed as an example.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Joint models for longitudinal and survival data,Survival analysis,clange@hsph.harvard.edu,,Christoph Lange,,HSPH,615 Huntington Ave,6179906068,,clange@hsph.harvard.edu,"On the adjustment for covariates in genetic association analysis: A novel,simple principle to infer causality",2,Stijn,,Vansteelandt,University of Ghent,Christoph,,Lange,Harvard School of Public Health,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In genetic association studies, different complex phenotypes are often associatedwith the same marker. Such associations can be indicative of pleiotropy (i.e. common geneticcauses), of indirect genetic effects via one of these phenotypes, or can be solely attributableto non-genetic/environmental links between the traits. To identify the phenotypes with theinducing genetic association, statistical methodology is needed that is able distinguish betweenthe different causes of the genetic associations. Here, we propose a simple, general adjustmentprinciple that can be incorporated into many standard genetic association tests which are thenable to infer whether a SNP has a direct biological influence on a given trait other than throughthe SNPs influence on another correlated phenotype. Using simulation studies, we show that,in the presence of a non-marker related link between phenotypes, standard association testswithout the proposed adjustment can be biased. In contrast to that, the proposed methodologyremains unbiased. Its achieved power levels are identical to those of standard adjustmentmethods, making the adjustment principle universally applicable in genetic association studies.The principle is illustrated by an application to three genome-wide association analysis.Key-words: causal diagram; direct effect; genetic pathways; mediation; pleiotropy.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Causal inference,gemechis.djira@sdstate.edu,,gemechis.djira@sdstate.edu,,Assistant Professor,South Dakota State University,605 688 6878,,gemechis.djira@sdstate.edu,A Comparative Study on Constructing Confidence Bands for Effective Doses,1,Gemechis,D,Djira,"Department of Mathematics and Statistics,South Dakota State University,Brookings",Din,,Chen,"Department of Mathematics and Statistics,South Dakota State University,Brookings",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this presentation we conduct a comparative study to construct simultaneous confidence bands for multi-dimensional effective doses in dose-response modeling with a binary response variable and multiple covariates.  The methods based on the inversion of Scheffe simultaneous confidence intervals, the delta method and a boostrap method will be compared through a simulation study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Toxicology/dose-response,Multiple testing,deschau@umich.edu,,Douglas Schaubel,Associate Professor,University of Michigan Department of Biostatistics,1420 Washington Hts.,734-395-5992,,deschau@umich.edu,Estimating the effect of a time-dependent therapy on restricted mean lifetime using observational data,1,Douglas,E.,Schaubel,University of Michigan Department of Biostatistics,John,D.,Kalbfleisch,University of Michigan Department of Biostatistics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In observational studies of survival data, the treatment of interestmay be time-dependent.  The evaluation of a time-dependent treatmentis often attempted through a hazard regression model which features atime-dependent treatment indicator.  However, depending on the natureof the data structure, this approach may not be applicable.We develop semiparametric procedures to estimate the effect onrestricted mean lifetime of a time-dependent treatment in the presenceof the following complicating factors: both an experimental andestablished form of treatment are available; pre- and post-treatmenthazards are non-proportional; subjects may experience periods oftreatment ineligibility; treatment assignment is not randomized.  Theproposed methods involve weighting results from stratifiedproportional hazards models fitted using a generalizationof case-cohort sampling. Asymptotic properties of the proposedestimators are derived, with finite sample properties assessed throughsimulation. The proposed methods are applied to data from theScientific Registry of Transplant Recipients (SRTR), to quantifythe effect on patient survival of expanded criteria donor (ECD) kidneytransplantation. ",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Epidemiologic methods,E_Andres_Houseman@brown.edu,,E. Andres Houseman,Assistant Professor of Community Health (Research),The Warren Alpert Medical School of Brown Universi,"121 South Main Street, Room 211",(401) 863-6283,,E_Andres_Houseman@brown.edu,Recursively partitioned mixture models with applications to DNA methylation array data,1,E. Andres,,Houseman,"Department of Community Health, Center for Environmental Health and Technology, Brown University",Brock,C,Christensen,"Department of Community Health, Center for Environmental Health and Technology, Brown University",Ru-Fang,,Yeh,"Department of Epidemiology and Biostatistics, University of California San Francisco, San Francisco",Carmen,J,Marsit,"Department of Pathology and Laboratory Medicine, Brown University",Margaret,R,Karagas,"Department of Community and Family Medicine, Dartmouth-Hitchcock Medical Center",Margaret,,Wrensch,"Department of Neurological Surgery, University of California San Francisco",Heather,H,Nelson,"Division of Epidemiology and Community Health, University of Minnesota School of Public Health",Joseph,,Wiemels,"Department of Epidemiology and Biostatistics, University of California San Francisco",John,K,Wiencke,"Department of Neurological Surgery, University of California San Francisco",Karl,T,Kelsey,"Department of Community Health, Center for Environmental Health and Technology and Department of Pathology and Laboratory Medicine, Brown University","Epigenetics is the study of heritable changes in gene function thatcannot be explained by changes in DNA sequence, and is one of themodes by which environment is thought to interact with genetics.  Assuch, epigenetic variables constitute an important type ofgenome-scale variable mediating between environment and disease, andpresent unique statistical challenges.  One of the most commonlystudied epigenetic alterations is cytosine methylation, which is awell recognized mechanism of gene silencing that often becomesdysregulated in cancer.  Arrays are now being used to study DNAmethylation at a large number of loci; for example, the IlluminaGoldenGate platform assesses DNA methylation at 1505 loci, while theInfinium platform measures DNA methylation at over 27,000 sites.  Mixture models have been used to identify DNA methylation subgroups inlow-dimensional data sets, but few methods exist for clusteringhigh-dimensional data in a reliable and computationally efficientmanner.  We present a novel model-based recursive-partitioningalgorithm to navigate clusters in a mixture model, and propose methodsfor inferring associations between resulting nested methylatorphenotypes and variables (e.g. upstream environmental variables anddownstream disease phenotypes and clinical outcomes).  We demonstrateour method on normal tissues and various types of tumors.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Genomics,john.brinton@uchsc.edu,,John Brinton,,University of Colorado Health Science Center,4200 Ninth Avenue B119,720 280 7706,,john.brinton@uchsc.edu,Animated graphics and visual metaphors help explain complex mathematical relationships.,1,John,T,Brinton,University of Colorado Health Science Center,Deborah,H,Glueck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"     Statisticians often need to explain complicated mathematical relationships to scientists and medical professionals.  While symbols and equations help people trained in mathematics to understand issues, they can make results unclear to people with training in other scientific disciplines.  A successful presentation accurately conveys the mathematics behind the science to all listeners.      We advocate abandoning equations and symbols when addressing general audiences.  We suggest summarizing statistical results using simple, consistent visual metaphors, in both animated graphics, and still pictures. 	     We demonstrate a number of graphic principles that can produce clarity in communication.  Graphics from recent invited talks on bias serve as a demonstration of our ideas. Pie charts represent percent disease verification, ellipses express the magnitude of correlation between screening test scores, and vertical bar charts illustrate the rate of disease in a population.  The effect of each factor on bias is demonstrated with a movie and stills showing small graphical elements changing next to moving receiver operating characteristic curves.  We provide step by step instructions, SAS and Mathematica code, and presentation quality examples.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Graphical models,Consulting,freisthler@spa.ucla.edu,,Bridget Freisthler,Assistant Professor,UCLA Department of Social Welfare,3250 Public Affairs Building,310-206-1602,,freisthler@spa.ucla.edu,Spatial Relationships between the Substance Use Environment and Child Maltreatment,1,Bridget,J,Freisthler,"UCLA Department of Social Welfare3250 Public Affairs Building, Box 951656Los Angeles, CA 90095",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Substance use has long been studied as both a contributor to and aconsequence of child maltreatment.  For example, studies of clinicalor convenience samples of families already involved with the childwelfare system found a positive relationship between childmaltreatment and substance use. Substance-abusing parents are alsomore likely to be reported multiple times to the child welfare systemfor child maltreatment. Yet the social mechanisms that produce thisrelationship remain largely unarticulated and understudied.  Forexample, already weakened (or frail) neighborhood structure may lackthe appropriate social capital to absorb the negative effects relatedto high densities of alcohol outlets in their community while thedensity of substance abuse services may mitigate that risk.  Withoutgreater attention to understanding how community systems affectpatterns of child maltreatment, effective prevention programs cannotbe developed.  Through a discussion of advanced spatial modelingtechniques, this presentation will delineate ecological pathways bywhich the substance use environment (a) increases the likelihood ofchild maltreatment; and (b) how the density of social services maymoderate this relationship. It is argued that specific family dynamicsare affected by parents substance use and use of community systemsfor acquiring and using alcohol and drugs.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Epidemiologic methods,bponicki@prev.org,,William Ponicki,Associate Research Scientist,Prevention Research Center,1995 University Ave.,510-883-5713,510-644-0594,bponicki@prev.org,Varying Parameter Models in Space and Time,1,William,R,Ponicki,"Prevention Research Center, Pacific Institute for Research and Evaluation, 1995 University Ave., Ste. 450, Berkeley, CA 94706.",Lance,A,Waller,"Department of Biostatistics, Rollins School of Public Health, Emory University, 1518 Clifton Road NE, Atlanta, GA 30322",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Unlike contagious diseases, rates of problems related to alcohol and drug abuse are strongly conditioned by well understood psychological characteristics (e.g., impulsivity), behavioral habits (e.g., daily routine activities), economic circumstances (e.g., access to drugs), and societal structures (e.g., friendship networks).  Human populations exhibit a broad range of heterogeneous risks for such problems, and these risks are often conditional upon one another and correlated with both social strata and the locations of neighborhoods in which people live.  It is thus expected that correlations between individual risk factors and abuse / dependence outcomes will also be heterogeneously distributed across geographic areas.  This presentation examines whether the relationships between measures of abuse and dependence and a number of exogenous variables vary in a spatially-correlated manner.  Bayesian space-time models are used to analyze arrests and hospital discharges related to methamphetamine in a large sample of California cities over the years 1980 through 2006.  Varying parameters are implemented using conditional-autoregressive random effects in a manner analogous to geographically weighted regressions.  The results demonstrate that effects related to exogenous measures vary geographically and over time across areas of California. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Epidemiologic methods,mnason@niaid.nih.gov,,Martha Nason,,"NIAID, NIH",6700A Rockledge Dr. Rm 5235,301-451-5134,,mnason@niaid.nih.gov,Characterizing Immune Responses via Flow Cytometry,1,Martha,,Nason,"Biostatistics Research Branch, NIAID, NIH",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Intracellular Cytokine Staining, a specific application of flowcytometry, can be used to measure the secretions of individual bloodcells that have been exposed to an antigen. When applied to hundredsof thousands of cells in a particular blood sample, this techniqueallows a characterization of the immune response an individual makesto a virus or a component of a virus. For viruses such as HIV, thefeatures that define a beneficial immune response are not entirelyclear, and therefore the kind of response should be targeted byvaccine developers is likewise still nebulous. This talk will describesome of the current aspects of individual immune responses to HIVunder investigation, and explore some measures and methods forcharacterizing the features of these immune responses. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Other,Infectious disease models,Flow cytometry-- invitedbmuthen@ucla.edu,,Bengt Muthen,Professor,UCLA,3463 Stoner Ave,310 390 8587,,bmuthen@ucla.edu,Estimating drug effects in the presence of placebo response,1,Bengt,O,Muthen,UCLA,Hendricks,C,Brown,USF,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Placebo-controlled randomized trials on antidepressants and other drugs often show a response for a sizeable percentage of the subjects in the placebo group.  Potential placebo responders can be assumed to exist also in the drug group, making it difficult to assess the drug effect.  A key drug research focus should be to estimate the percentage of individuals among those who responded to the drug who would not have responded to the placebo (Drug only responders).  This talk investigates a finite mixture model approach to uncover percentages of up to four potential mixture components:  Never Responders, Drug only responders, Placebo only responders, and Always responders.  Two examples are used to illustrate the modeling, a twelve-week antidepressant trial with a continuous outcome (Hamilton D score) and a 7-week schizophrenia trial with a binary outcome (illness level). Growth mixture modeling (Muthen & Asparouhov, 2008) is used to uncover the different mixture components.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Due to other meetings, I prefer to present March 16 in the AM.",oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Longitudinal data,leellio2@ncsu.edu,,Laine Thomas,,NCSU,1001 Brighthurst Dr. Unit 202,919-931-1618,,leellio2@ncsu.edu,Adjusting for Measurement Error When Subject-specific Variance Estimates Are Used as Covariates in a Primary Outcome Model,1,Laine,E,Thomas,"North Carolina State University, Statistics Dept.",Marie,,Davidian,"North Carolina State University, Statistics Dept.",Stefanksi,A,Leonard,"North Carolina State University, Statistics Dept.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In biological studies of health effects a primary endpoint may berelated to the longitudinal profiles of a continuous response. Outcome models where covariates are subject-specific random effectshave been well studied (Li, Zhang and Davidian 2004).  Instead, we maywant to understand the relationship between disease outcome andsubject-specific variability over time (Yang et al 2007, Lyles et al1999).  Substantial bias can occur when variance estimates are imputedas covariates in the outcome model.  To account for this, Lyles et al1999 developed a maximum likelihood method which assumes that thesubject-specific variances come from a lognormal distribution.  Wecompare two approaches that require no assumptions on the distributionof the subject-specific variances.  We adapt the conditional scorestrategy of Stefanski and Carroll (1987) to account for error in thevariance estimators which has a chi-square distribution.  This methodprovides consistent parameter estimation but is limited to aparticular specification of the outcome model.   An alternativeapproach is motivated by the unpublished thesis of Bay (1997) whichadjusts the estimates to have unbiased sample moments for thedistribution of subject-specific variances in the population.  Thishas the benefit of being easily implemented in a variety of outcomemodels.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Joint models for longitudinal and survival data,Measurement error,booil@stanford.edu,,booil jo,,stanford university,"dept. of psychiatry, 401 quarry rd",310 721 2986,,booil@stanford.edu,Causal Inference with Longitudinal Subpopulation Indicators,1,booil,,jo,stanford university,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper focuses on settings where treatment compliance is measuredover time, but the outcome is not measured in parallel, which is verycommon in randomized intervention trials. We propose to formulatecompliance and/or outcome trajectory strata considering potentialoutcomes under only one treatment condition and then estimatedifferential treatment effects conditioning on these strata. Thelatent strata formulated based on this approach can be thought of ascoarse principal strata. The advantage of this approach is that it ispossible to construct subpopulation trajectory strata without pairingcompliance and outcome information. Our proposed approach nicelycomplements that of Lin, Ten Have, & Elliot (in press) and deals withsituations that are difficult to handle in their framework.",FALSE,FALSE,,FALSE,FALSE,TRUE,Monday (16th) would be the best if possible.,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Latent variables,tilmann@stat.washington.edu,,Tilmann Gneiting,,University of Washington,Box 354322,2065435169,,tilmann@stat.washington.edu,Probabilistic Quantitative Precipitation Forecasting Using a Two-Stage Spatial Model,1,Tilmann,,Gneiting,University of Washington,Veronica,J,Berrocal,Duke University,Adrian,E,Raftery,University of Washington,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Short-range forecasts of precipitation fields are required in a wealth ofagricultural, hydrological, ecological and other applications. Forecasts from numerical weather prediction models are often biasedand do notprovide uncertainty information.  Here we present a postprocessingtechnique for such numerical forecasts that produces correlatedprobabilistic forecasts of precipitation accumulation at multiplesites simultaneously.  The statistical model is a spatial version of atwo-stage model that describes the distribution of precipitation witha Bernoulli-Gamma mixture.  Spatial correlation is captured byassuming that two Gaussian processes drive precipitation occurrenceand precipitation amount, respectively.  A site-specifictransformation function retains the marginal right-skewed distributionof precipitation while taking the numerical forecast into account. The two-stage spatial model was applied to forecasts of dailyprecipitation accumulation over the Pacific Northwest in 2004, at aprediction horizon of 48 hours.  The predictive distributions from thetwo-stage spatial model were calibrated and sharp, and outperformedreference forecasts for spatially composite and areally averagedquantities.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Environmental and ecological applications,chwang@jhsph.edu,,Chi Wang,,Johns Hopkins University,615 N. Wolfe St.,410-614-5126,,chwang@jhsph.edu,Exponential Tilt Models in the presence of Censoring,1,Chi,,Wang,Johns Hopkins University,Zhiqiang,,Tan,Rutgers University,Thomas,A.,Louis,Johns Hopkins University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We study application of the Exponential Tilt Model (ETM) to comparesurvival distributions in two groups. As a semi-parametric model, the ETM assumes aparametric form for the density ratio of the two distributions. The ETM accommodatesa broad array of parametric models such as the log-normal and gamma models andserves to complement the proportional hazards and accelerated failure models. Wedevelop a non-parametric likelihood approach to estimating ETM parameters in thepresence of censoring. In simulation studies, we compare the ETM to the ProportionalHazards Model (PHM). When the proportional hazards assumption is not satisfiedbut the ETM assumption is, we show that the ETM has better power for testing thehypothesis of no difference between the two groups. And, importantly, when the ETMrelation is not satisfied but the PHM assumption is, the ETM can still have powerclose to that of the PHM.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Empirical likelihood,Survival analysis,rao@stat.osu.edu,,Youlan Rao,,The Ohio State University,1958 Neil Avenue,614-432-9313,,rao@stat.osu.edu,Determination of Sample Size for Validation Study in Pharmacogenomics,1,Youlan,,Rao,"Department of Statistics, The Ohio State Univeristy",Yoonkyung,,Lee,"Department of Statistics, The Ohio State University",Jason,C.,Hsu,"Department of Statistics, The Ohio State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Pharmacogenomics aims at co-development of a drug that targets a subgroup of patients with efficacy and a device that identifies the responder group through patients' genetic variations. Development of such a prognostic device includes a training stage and a validation stage. The transition from the training stage to the validation stage typically involves change of platforms as a subset of the genes predictive of drug response are identified in the first stage and only those are used in the second stage. With the change in consideration, this paper concerns how to determine sample sizes for the validation stage to meet pre-specified sensitivity and specificity requirements in order to avoid futility of pharmacogenomic development.In particular, taking microarrays which measure gene expression levels as a primary device, we show how to decide the numbers of subjects per group, replicated samples per subject, replicated probes per sample for the validation experiment. The change of platforms is taken into account in the sample sizes calculation by linear mixed effect modeling. Our formulation of sensitivity and specificity requirements calls for confidence lower bounds of both measures, which lead to a slightly conservative procedure. The procedure is illustrated in a proof-of-concept mice experiment.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Power analysis/sample size,"Biologics, pharmaceuticals, medical devices",jianzhu@umich.edu,,Jian Zhu,,"Department of Biostatistics, University of Michiga","109 Observatory St., SPH II",734-277-2723,,jianzhu@umich.edu,Assessing the Convergence of Multiple Imputation Algorithms Using a Sequence of Regression Models,1,Jian,,Zhu,"Dept. of Biostatistics, Univ. of Michigan",Trivellore,E,Raghunathan,"Dept. of Biostatistics, Univ. of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Multiple imputation algorithms using a sequence of regression models,or multiple imputation using chained equations, are commonly used tohandle non-responses in complex survey studies. Although suchalgorithms have several advantages over joint modeling of all surveyvariables, they have a theoretical limitation that the specifiedconditional distributions could be incompatible and therefore theunderlying joint distribution to which the algorithms attempt toconverge may not exist. Previous simulation studies (van Buuren et al2006) show that multiple imputation algorithms using incompatibleconditional distributions seem to work well for some cases. We focuson general multivariate data to assess the convergence properties ofthe imputation algorithms using various types of conditionaldistributions. Additionally, we evaluate the impact of incompatiblemodels on imputation results through simulation studies. We also use alongitudinal study with a general missing pattern to illustrate theperformance of the imputation algorithms.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Computational methods,jkeighle@kumc.edu,,John Keighley,Research Assistant Professor,University of Kansas Medical Center,3901 Rainbow Blvd,913-588-7391,913-588-0252,jkeighle@kumc.edu,Power and Type I Error rates in Repeated Measures Experiments as the Number of Time Points in a Fixed Length Time Interval Increase and Under Several Covariance  Structures for the Repeated Measures,1,John,D,Keighley,University of Kansas Medical Center,Dallas,E,Johnson,Kansas State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Linear mixed models are by far the most popular modeling choice for longitudinal analyses because they offer flexible mean and covariance structure choices. However, researchers can be challenged at designing future experiments for which linear mixed models are used. In particular, how many time points should they collect? This basic question was addressed in this paper by calculating power as a function of equally spaced time points with different expected response functions and covariance structures. The main focus was on testing treatment by time interaction and using various single degree of freedom contrasts. Countering expectations, power decreased or remained constant with an increase in the number of time points. The simulation methods and results in this paper can guide researchers in determining the number of time points for their future study.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Power analysis/sample size,Longitudinal data,mlacey@math.tulane.edu,,Michelle Lacey,Assistant Professor,Tulane University,Dept. of Mathematics,504-862-3439,504-865-5063,mlacey@math.tulane.edu,Analysis of Cancer-Related Epigenetic Changes in DNA Tandem Repeats,1,Michelle,R,Lacey,"Department of Mathematics, Tulane University",Koji,,Tsumagari,"Hayward Genetics Center, Tulane University School of Medicine",Melanie,,Ehrlich,"Hayward Genetics Center, Tulane University School of Medicine",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"DNA methylation is essential for the normal development andfunctioning of organisms, and frequent abnormal increases or decreasesin DNA methylation tags are found in most human cancers and contributeto their development.  Through hairpin bisulfite technology andSouthern blots, the methylation characteristics of genomic regions canbe studied on both a local and regional scale.   We analyze the tandemrepeats Sat2 and NBL2, revealing considerable differences inmethylation patterns between somatic control tissues and ovariancarcinomas, and present a stochastic model for these epigeneticchanges which reflects the site-to-site dependencies evident in theobserved data. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Cancer applications,Epigeneticswangy@uncw.edu,,Yishi Wang,,UNC Wilmington,601 S. college road,9109623292,,wangy@uncw.edu,Estimating the variance of BJE under discrete assumption,1,Yishi,,Wang,UNC Wilmington,Cuixian,,Chen,UNC Wilmignton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Buckley-James estimator is a widely recognized approach in dealing right censored linear regression models. There have been a lot of discussions in the literatures on the estimation of the Buckley-James estimator as well as its asymptotic distribution. But so far, no simulation has been done to estimate the standard error of the estimator. Kong and Yu (2007)  studied the asymptotic distribution under discrete assumption. Based on their methodology, we recalculate the asymptotic variance and formulate the estimation of variance by using plug-in estimators. The simulation suggests that the  estimation is a great approximation compared with the empirical SD. The large sample property of the estimator is discussed",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Nonparametric methods,nicholas.m.murray@ttu.edu,,Nicholas Murray,,Texas Tech University,4803 6th Street,713-598-2258,,nicholas.m.murray@ttu.edu,Stochastic Models of Flow Through a Random Graph,1,Nicholas,M,Murray,"Department of Mathematics and Statistics, Texas Tech University",Clyde,,Martin,"Department of Mathematics and Statistics, Texas Tech University",Dorothy,,Wallace,"Department of Mathematics, Dartmouth College",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Playa Lakes can be found throughout the Panhandle of Texas and areformed by seasonal rains. Many species populate the Playa Lakes. Ineach species, it is possible to track genetic adaptations which can beextremely localized, for example, present in a single Playa Lake. Itis then possible to track the passing of a specific genetic traitthroughout the Playa Lake system.   The goal of this research is to acquire an estimate of the timerequired for a genetic trait to travel between the two most distantPlaya Lakes. This is accomplished by constructing a simple stochasticmodel simulating the movement of populations carrying the genetictrait. The model must take into account that traits can only be passedbetween nearest neighbors and playa lakes can dry out or be filled ona seemingly random basis. The problem reduces to a stochastic flow ona graph that is changing randomly. For small sets of nodes (playas)the problem has the potential for an analytic solution. However, forsystems of the same order of magnitude as the playa system (20,000nodes) experimental results are the best possible. From this model, atime estimate can be constructed for any system size.",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Computational methods,Environmental and ecological applications,chenc@uncw.edu,,Cuixian Chen,,UNC Wilmington,601 S. college road,910-962-3768,,chenc@uncw.edu,THE GMLE BASED BUCKLEY-JAMES ESTIMATOR WITH MODIFIED CASE-COHORT DATA,1,Cuixian,,Chen,UNC Wilmington,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider the estimation problem under the linear regression model with the modified case-cohort design. The extensions of the Buckley-James estimator (BJE) under the case-cohort designs have been studied under an additional assumption that the censoring variable and the covariate are independent. If this assumption is violated, as is the case in a typical real data set in the literature, our simulation results suggest that those extensions are not consistent and we propose a new extension. Our estimator is based on the generalized maximum likelihood estimator (GMLE) of the underlying distributions. We propose a self-consistent algorithm, which is quite different from the one for ultivariate interval-censored data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Nonparametric methods,mlbergquist@yahoo.com,,Mandy Bergquist,,GlaxoSmithKline,PO Box 13398,919-483-1144,919-315-4014,mlbergquist@yahoo.com,Statistical Consulting: Earning Your Place on the Team,1,Mandy,L,Bergquist,GlaxoSmithKline,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As pharmaceutical companies face increasing patent expirations and dramatically shrinking budgets, they expect more from every employee.  Statisticians can rely on government regulation for continued employment, or they can rise to the challenge.  Even in a difficult environment, statisticians can extend their influence and move into larger industry roles by becoming more than role players on the matrix teams to which they contribute.  Good technical skills are valuable, but not enough to succeed.  This presentation draws on the speaker's experience as an internal statistical consultant at a major pharmaceutical company to provide practical advice for statisticians who want to earn a respected position on the team.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Consulting,"Biologics, pharmaceuticals, medical devices",rkennedy@ms.soph.uab.edu,,Richard Kennedy,Postdoctoral Fellow,University of Alabama-Birmingham,1665 University Boulevard,205-975-9148,,rkennedy@ms.soph.uab.edu,Differential DNA Methylation:  Methodology and Study Design,1,Richard,E,Kennedy,"Section on Statistical Genetics, Department of BiostatisticsUniversity of Alabama-Birmingham",Xiangqin,,Cui,"Section on Statistical Genetics, Department of BiostatisticsUniversity of Alabama-Birmingham",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Studies to identify methylation differences between experimental andcontrol (normal) tissues are increasingly common.   Methodologies forexamining differential methylation between two experimental conditionsare less well developed, particularly for two-channel microarrays.  Inthe latter, the two experimental conditions may serve as controls foreach other.  We analyzed publicly available datasets for the Nimblegentwo-color methylation platform.  Using the original datasets,methylation-enriched regions were identified using the Ringo package,and summaries for the degree of methylation were computed.  Thesecorrelated well with the signal for the methylation-enriched channel,indicating that control DNA hybridizations may not be needed. Artificial differential methylation arrays were constructed bycombining methylation-enriched channels from each disease state, whichwere also analyzed using Ringo.  The regions identified using thisapproach overlapped significantly with regions identified using theoriginal arrays.  Taken together, these results offer initial supportthat separate control hybridizations for each array of differentialmethylation studies are not necessary.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Microarray analysis,Epigeneticsbamalam@uab.edu,,Leigh A. Morton,,UAB,2763 Vance Drive,205-535-0626,,bamalam@uab.edu,"Comparison of Simons Two-Stage Design, Sequential Probability Ratio Test, and Triangular Test in Phase II Clinical Trials",1,Leigh,A,Morton,UAB Department of Biostatistics (Graduate Student),David,T,Redden,"Ph.D., UAB Department of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The main objective of phase II clinical trials is to evaluate the efficacy of a new treatment in a manner that avoids enrolling a large number of patients when a drug is truly ineffective while providing adequate power to declare a drug effective when it is.  Therefore, it becomes necessary to evaluate different study designs aimed at reducing expected sample size under the null hypothesis while maintaining power under the alternative hypothesis. Simon suggests a two-stage design in which the study is either terminated or continued by observing the number of responses in the first stage.  The trial is continued into the second stage if a sufficient number of responses are observed.  The sequential probability ratio test (SPRT), originally proposed by Wald, analyzes data after each new observation is obtained.  One of three decisions is then reached: 1) accept the null hypothesis, 2) reject the null hypothesis, or 3) add additional observations and continue with trial based on open-ended boundaries.  The triangular test is similar to the Walds SPRT, but it implements analysis after a discrete number of observations are added and relies on more constrained stopping boundaries.  Using simulations, we compare and contrast the three study designs with respect to expected sample size, type I error, and power. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Power analysis/sample size,Applied data analysis,daniel_beavers@baylor.edu,,Daniel Beavers,,Baylor University Department of Statistical Scienc,P.O. Box 97140,336.3542435,,daniel_beavers@baylor.edu,Bayesian Hierarchical Modeling of Probabilities from Repeated Binary Diagnostic Tests,1,Daniel,P,Beavers,Baylor University Department of Statistical Science,James,D,Stamey,Baylor University Department of Statistical Science,John,W,Seaman III,Baylor University Department of Statistical Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Fallible diagnostic tests introduce bias into statistical inferencevia misclassified binary data.  One approach to assess the quality ofa diagnostic test is to test individuals repeatedly, which can providesufficient data to estimate the sensitivity and specificity of thetest as well as the latent cohort disease prevalence, assuming certainconditions hold.  In our work we consider a Bayesian approach tomodeling the population prevalence as well as the test sensitivity andspecificity.  We add hierarchical random variability components to themisclassification models to estimate the inter-individual variability. Model performance is assessed using the deviance informationcriteria, and we compare our results to existing frequentist methods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Hierarchical models,haiqun.lin@yale.edu,,Haiqun Lin,Associate Professor,Yale University,60 College Street,203-785-4707,,haiqun.lin@yale.edu,Accounting for Unmeasured Confounders with Latent Variable,1,Haiqun,,Lin,"Division of Biostatistics, Yale University School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Inference about the effects attributable to a treatment condition in non-experimental, i.e. observational studies is critical in many areas of public health research. The pivotal aspect of observational studies is that the treatment assignment is not under control of the investigator. Most existing methods adjust for measured confounding variables under the assumption of no unmeasured confounding. In this paper, we present a method for estimating the effect attributable to atreatment condition when there exist unmeasured confounders. We regard unobserved confounders as a latent variable. We first formulate a joint model of the possibly time-varying treatments and the associated responses that can account for unmeasured (as well as measured confounding variables). The marginal estimate of the effect attributable to a possibly time-varying treatment can be obtained by using a weighted generalized estimation equation model using weights constructed from the joint models. Our method is illustrated with the analysis of a data set from ACCESS (Access to CommunityCare and Effective Services and Support) and validated through simulation studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,"I prefer Monday March 16, 2008",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Latent variables,minzhang@stat.purdue.edu,,Min Zhang,Assistant Professor,Purdue University,150 N. University St.,765-496-7921,,minzhang@stat.purdue.edu,Multiple SNP-based approach for genome-wide case-control association study,1,Min,,Zhang,"Department of Statistics, Purdue University",Yanzhu,,Lin,"Department of Statistics, Purdue University",Libo,,Wang,"Department of Statistics, Purdue University",Vitara,,Pungpapong,"Department of Statistics, Purdue University",James,C,Fleet,"Department of Foods and Nutrition, Purdue University",Dabao,,Zhang,,,,,,,,,,,,,,,,,,"Genome-wide association study is challenged by the collection of a large number of SNPs from a relatively small number of individuals, and therefore, one SNP is investigated at a time. However, such univariate association study ignores the multigenic nature of common complex diseases as well as the linkage disequilibrium between SNPs. Here we propose a method to analyze all SNPs in the same linkage group simultaneously by implementing the linear discriminant analysis through a penalized orthogonal-components regression, which is a newly developed variable selection approach for high dimensional data. The proposed method is applied to real genome-wide association study data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Genomics,weiliang.2.shi@gsk.com,,Weiliang Shi,,GlaxoSmithKline,GSK UP4310,610-917-6108,,weiliang.2.shi@gsk.com,Grouped LASSO-Patternsearch Algorithm,1,Weiliang,,Shi,GlaxoSmithKline,Grace,,Wahba,University of Wisconsin-Madison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The LASSO-Patternsearch algorithm (LPS) is an efficient method thatcan identify patterns of multiple dichotomous risk factors foroutcomes of interest in genomic studies. The method is designed forthe case where there is a possibly very large number of candidatepatterns but it is believed that only a relatively small number areimportant. LPS, as a global method can handle very complicatedcorrelations among the predictor variables. However, the number ofvariables is limited due to limited computer memory. The currenttrends in genetic epidemiology are to evaluate as many markers aspossible. The number can easily exceed ten thousand or more. TheGrouped LASSO-Patternsearch algorithm (GLPS) is proposed to tacklethis problem. GLPS, as suggested by its name, begins with dividing thecovariates into small groups.  LPS is run on each group of variables.We collect all the variables that show up in at least one of theseruns, either as main effects or in second order patterns. Then LPS isrun again on all variables surviving the group step. One big advantageof our algorithm is that the runs in group steps are parallel. We canrun them simultaneously on different machines. The computing systemCondor provides us an excellent opportunity to do this job.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Data mining/massive data sets,Machine learning,wangc3@uthscsa.edu,,chen-pin wang,,"Department of Epidemiology and Biostatistics, Univ", 7703 floyd curl drive,210 617 5300,,wangc3@uthscsa.edu,A causal model of baseline and post-treatment confounding for observational studies,1,chen-pin,,wang,UTHSCSA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In clinical practice, the choice of oral glucose-lowering agents in type 2 diabetics depends on patients recent glucose levels as well as other predictors for cardiovascular diseases (CVD). Prior studies suggested that how well patients responses to these drugs in terms of the change in glucose levels may have differential impacts on patients cardiovascular outcomes. This presentation considers a causal model to study the direct effect of two types of glucose-lowering agents on CVD while accounting for confounding due to (i) medication choice at baseline and (ii) heterogeneity in responding to the medication. The proposed model  integrates both the inverse probability weighting (IPW) and principal stratification (PS) modeling techniques. The PS component models the variation in drug response to hyperglycemia, while an IPW estimate, nested within each principal stratum, is used to quantify the PS effect. More specifically, the principal strata are identified by latent variable modeling of glycemia trajectory classes, and IPW refers to the inverse of latent class specific propensity score. We demonstrate the proposed model using a clinical cohort of type 2 diabetics from the VA health care system. The impact of the IPW estimate and misspecification in PS modeling will be discussed.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Wednesday (March 18th) Tuesday (March 17th) ",oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Causal inference,Latent variables,lyliu@ntu.edu.tw,,Li-yu D Liu,Assistant Professor,National Taiwan University,Department of Agronomy,+886-2-33664792,,lyliu@ntu.edu.tw,Differentiating mRNA expression levels of tumor versus non-tumor cells in a cancer tissue,1,Li-yu,D,Liu,National Taiwan University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In recent years the microarray technology has been widely adopted incancer research. The applications of microarray technology in cancerresearch include recapture of regulatory mechanism of oncogenes, development of prognosis classifiers, and drug discovery. Cancer research in vivo means that the experimentation has been conducted on the living tissue, from whichthe complex biological kinase can be directly observed. However, if the tissueunder experiment is a mixture of tumor and non-tumor cells, the observed geneexpressions in a microarray might not well represent the mRNA levels from tumor cell.Microscopy combined with laser-assisted microdissection provides a way to reducethe cellular heterogeneity of the tissue but laser microdissection units remainvery expensive. In this study, we propose an alternative to estimate the mRNA levels oftumor cells with little cost  to estimate from the microarray gene expression ofthe mixture tissue via normalization. The gene expression levels before and after normalization are used to conduct feature selection andclassification, respectively, and the results will be compared to assess the validity of theproposed methods.    ",FALSE,FALSE,,FALSE,FALSE,FALSE,,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Microarray analysis,Computational methods,Microarray analysisdaniel.polhamus@utsa.edu,,Dan Polhamus,Doctoral Student,"University of Texas, San Antonio",7721 Hunter Oaks,210-379-0737,,daniel.polhamus@utsa.edu,Practical estimation and discussion of neuronal phase-response (phase-resetting) curves,1,Daniel,G,Polhamus,"University of Texas, San Antonio",Charles,J,Wilson,"University of Texas, San Antonio",Carlos,A,Paladini,"University of Texas, San Antonio",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Phase-resetting curves provide valuable insight to neuralinterconnectivity and, as both an oscillatory and neuronal stimulationmodel, are well documented.  Estimation of the PRC from real-lifeexperimental neurons is complicated by the inherent variability ofbiological electrical systems and human interaction with thesesignals.  In the process of discussing current empirical methodologyfor the estimation of phase-resetting curves in the context ofsimulated and real data, we comment on aberrations from the expectedbehavior.  Much of this variability can be traced back todistributional characteristics of the baseline inter-spike intervals(ISI).  We propose corrections to current empirical methodology anddemonstrate the robust nature of our proposed PRC estimation, relativeto the aforementioned aberrations and data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Other,Applied data analysis,Neurostatisticsyychi@biostat.ufl.edu,,Yueh-Yun Chi,Assistant professor,University of Florida,1329 SW 16th Street Room 5232,352-265-0111 ext. 85854, 352-265-8047,yychi@biostat.ufl.edu,"The Univariate Approach to Repeated Measures ANOVA for High Dimension, Low Sample Size",1,Yueh-Yun,,Chi,"Division of Biostatistics, Department of Epidemiology and Health Policy Research, University of Florida",Keith,E,Muller,"Division of Biostatistics, Department of Epidemiology and Health Policy Research, University of Florida",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The univariate approach to Gaussian repeated measures (UNIREP)extends naturally to High Dimension, Low Sample Size (HDLSS) data. The UNIREP test requires only orthonormal invariance and therefore canbe computed with HDLSS data, in contrast to multivariate techniques. However, the simulations demonstrate that traditional tests fail badlyin controlling type I error rate.  We describe a sphericity parameterestimator and associated UNIREP test for HDLSS data that controls typeI error rate well for HDLSS data with any population covariancematrix.  The method appropriately accounts for the repeated measuresor multivariate nature of HDLSS data such as seen in metabolomics,imaging, and genomics.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Prefer to present on the 16th or in the morning of the 17th of March, given the regular teaching duty on Wednesdays.",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,High dimensional data,Multivariate methods,keles@stat.wisc.edu,,Sunduz Keles,Assistant Professor,"University of Wisconsin, Madison",1300 University Avenue,608-263-4533,,keles@stat.wisc.edu,A Hierarchical Semi-Markov Model for Detecting Enrichment with Application to ChIP-Seq Experiments,1,Sunduz,,Keles,"University of Wisconsin, MadisonDepartments of Statistics and of Biostatistics and Medical Informatics",Pei Fen,,Kuan,"University of Wisconsin, MadisonDepartments of Statistic ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Protein-DNA  interactions play a fundamental role in gene regulation.Significant progress has been made in profiling transcription factorbinding sites and histone modifications with  ChIP-chip, and morerecently ChIP-Seq experiments. Despite the numerous model basedapproaches developed for the analysis of ChIP-chip data, limitedstatistical tools are available for ChIP-Seq data. We develop acomprehensive model based approach for detecting enrichment fromChIP-Seq experiments with a hierarchical semi-Markov model. Theproposed model is applicable for the analysis of experiments with (1)single  ChIP-Seq sample with and without input control  and (2)multiple ChIP-Seq samples.  We introduce a new meta analysis approachfor controlling the FDR at peak level and allow for the boundaries ofthe binding sites to be declared probabilistically. This bypasses thecommon heuristic postprocessing methods to merge contiguous bins as peaks. We also propose and investigate various  models for  observedtag counts from  ChIP-Seq experiments, that account for sequencingdepths and biases due to amplification and sequence-specificaffinities. Although our discussion is dedicated to ChIP-Seqexperiments, the proposed hierarchical semi-Markov model can beapplied to other types of data which exhibit spatial structure (e.g.,ChIP-chip), by modifying the  emission distributions.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Hierarchical models,ruitaozhang@hotmail.com,,Ruitao Zhang,Student,University of Massachusetts,"121 North Main ST, I-5",413-323-6043,,ruitaozhang@hotmail.com,Optimal Coefficients for Simple Random Sampling Without Replacement using Godambe's General Linear Estimator,1,Ruitao,,Zhang,"Division of Biostatistics and Epidemiology, Department of Public Health, UMASS, Amherst, MA 01002",Ed,,Stanek,"Division of Biostatistics and Epidemiology, Department of Public Health, UMASS, Amherst, MA 01002",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ABSTRACTGodambe (1955) proved that a best linear unbiased estimator does not exist based on sampling.  Recently, we have developed unique optimal coefficients for estimating the population total when sampling without replacement of  and   using Godambe's general linear class of estimators. The solution to the estimating equations requires that all subjects values are not equal to zero.  We discuss solutions to this problem, and extend  solutions to the populations with one or two zero values or ties when sampling without replacement of  and  . Other extensions are developed to the problem where n=3 from N=4 and n=4 from N=5 following the same methods with no zeros. Then we will generalize to N-1 from N. The estimator will also be extended to include values with at least one zero values or ties.  This basic research is important since it explores conceptual issues in formally integrating sampling into statistical inference, identifying promising avenues to be explored.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,Estimating equations,Estimation Accounting for Survey Samplingrappold.ana@epa.gov,,Ana Rappold,statistician,US EPA,105 Hillspring Lane,919843 9504,,rappold.ana@epa.gov,Data Assimilation for Prediction of Fine Particulate Matter,1,Ana,G,Rappold,US Environmental Protection Agency,Marco,A,Ferreira,University of Missouri - Columbia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A substantial portion of air quality research and management isconcerned with the nature of adverse health effects associated withexposure to fine particulate matter (PM2.5).  Currentassessment of population based daily exposure is limited by thespatial and temporal availability of the monitoring networks. There isa strong interest in developing methodology for assimilation of theobserved data and  mathematical model predictions such as EPA'sCommunity Multi-scale Air Quality model (CMAQ). CMAQ is adeterministic model of atmospheric pollutant transport  which providesvolume averaged predictions on the grid.  Although such modelpredictions are biased they contribute important spatial and temporalfeatures by information such as weather patterns, land use, emissions,etc.The process of data assimilation involves several importantstatistical problems such as integration of areal and point data,and treatment of spatial and temporalsimilarities in data as well as bias. To address these problems, wedevelop a Bayesianhierarchical model for space-time assimilation of two types of data.The joint distribution of the data is modeled as a spatial processdynamically evolving through time. We provide insight in spacialy varying bias associated with CMAQ data by introducing additive multi-scale spatio-temporal model for bias.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Environmental and ecological applications,cai@bios.unc.edu,,Jianwen Cai,Professor,University of North Carolina at Chapel Hill,CB #7420,919-966-7788,,cai@bios.unc.edu,Joint modeling of longitudinal categorical data and survival data,1,Jianwen,,Cai,University of North Carolina at Chapel Hill,Jaeun,,Choi,University of North Carolina at Chapel Hill,Donglin,,Zeng,University of North Carolina at Chapel Hill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In many biomedical studies, it is of interest to study the covariate effect on both longitudinal categorical outcomes and survival outcomes. For example, in cancer research, it is of interest to study the treatment effect on both quality of life which is a categorical outcome measured longitudinally and survival time. In this talk, we will discuss such joint models. Random effects are introduced into thesimultaneous models to account for dependence between longitudinal categorical outcome and survival time due to unobserved factors. EM algorithms are used to derive the point estimates for the parameters in the proposed model and profile likelihood function is used to estimate their variances. The asymptotic properties areestablished for our proposed estimators. Finally, simulationstudies are conducted to examine the finite-sample properties ofthe proposed estimators and a liver transplantation data set isanalyzed to illustrate our approaches.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Joint models for longitudinal and survival data,Random effects,jang@uga.edu,,Woncheol Jang,Assistant Professor,University of Georgia,Department of Epidemiology and Biostatistics,706-583-8923,,jang@uga.edu,Analysis of Long Period Variable Stars with a Nonparametric Significance Test of No Trend,1,Woncheol,,Jang,"Department of Epidemiology and Biostatistics, University of Georgia",Cheolwoo,,Park,"Department of Statistics, University of Georgia",Jeongyoun,,Ahn,Department of Statistics,Martin,,Hendry,"Department of Physics and Astronomy, University of Glasgow",,,,,,,,,,,,,,,,,,,,,,,,,"The study of variable stars has a long and illustrious history in astronomy, making crucial contributions to our understanding of manyfields, from stellar birth and evolution to the calibration of theextragalactic distance scale. Variable stars are characterized byshowing significant variation in their brightness over time. Weperform a time series analysis of the periods between maximumbrightness of a group of 378 long period variable stars. The objectiveof this study is to identify the stars which display certain trends intheir period, via multiple testing of a mean fornon-stationary time series model.   We test the null hypothesis of notrend in each time series based on high dimension normal meaninference, while controlling the false discovery rate to adjustmultiplicity. Functional clustering and principal component analysisis used to account for the non-stationary error structure.  Weinvestigate the performance of the proposed methods using  simulationstudies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Functional data analysis,Nonparametric methods,chungy@email.unc.edu,,Yeonseung Chung,,Harvard Scool of Public Health,677 Huntington Ave,919-357-7761,,chungy@email.unc.edu,Nonparametric Bayes Conditional Distribution Modeling with Variable Selection,1,Yeonseung,,Chung,"Department of Biostatistics, Harvard School of Public Health",David,,Dunson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This research considers methodology for flexibly characterizing the relationship between a response and multiple predictors. Goals are (1) to estimate the conditional response distribution addressing the distributional changes across the predictor space, and (2) to identify important predictors for the response distribution change both with local regions and globally. We first introduce the probit stick-breaking process (PSBP) as a prior for an uncountable collection of predictor-dependent random probability measures, and propose a PSBP mixture (PSBPM) of normal regressions for modeling the conditional distributions. A global variable selection structure is incorporated to discard unimportant predictors, while allowing estimation of posterior inclusion probabilities. Local variable selection is conducted relying on the conditional distribution estimates at different predictor points. An efficient stochastic search algorithm is proposed for posterior computation. The methods are illustrated through simulation and applied to an epidemiologic study. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,studentaward_submission_poster,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Nonparametric methods,av8@rice.edu,,ALEJANDRO VILLAGRAN,,RICE UNIVERSITY,DEPARTMENT OF STATISTICS,17133486288,,av8@rice.edu,Wavelet-based Functional Mixed Models via DPM,1,Alejandro,,Villagran,Rice University,Sang Han,,Lee,New York University,Marina,,Vannucci,Rice University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recently, various experimental designs in public health andbioinformatics require the use of functional mixed models (FMM), i.e.,a functionalized extension of linear mixed models, conditional onmodern technologies allowing researchers to record data sampled on afine grid. Morris and Carrol (JRSS-B, 2006) developed wavelet-basedfunctional mixed models that are flexible enough to accommodate abroad range of functional data. We attempt toextend these methods by fitting the functional mixed model in thewavelet domain and by relieving the assumption of normality on therandom effect functions to any distributional form via a DirichletProcess Mixture (DPM) prior. We use the Gibbs sampler algorithm toestimate the posterior distributions of the parameters. We illustratethis methodology witha simulated example.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Functional data analysis,Nonparametric methods,gina@wubios.wustl.edu,,Gina D'Angelo,Assistant Professor,Washington University,"660 South Euclid Avenue, CB 8067",314-362-3758,,gina@wubios.wustl.edu,An EM approach for partial correlation and missing data,1,Gina,,D'Angelo,"Washington University School of Medicine, Division of Biostatistics",Chengjie,,Xiong,"Washington University School of Medicine, Division of Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the cognitive neuroscience area it is often of interest to studybrain co-activation and relationships among these regional brainmeasures and psychometric measures.  Many of these studies by designhave frequency matching on age and gender, and often it is necessaryto adjust for these covariates.  Partial correlation is a statisticalmeasure that can be used to correlate two variables while adjustingfor other variables.  In the presence of data that are missing atrandom, complete case analysis will lead to biased and inefficientresults.  We will extend the partial correlation coefficient in thepresence of missing data using the EM algorithm and compare it with amultiple imputation method and complete case analysis.  Anotherobjective of this work is to compare partial correlations betweengroups, and we will extend the correlation analysis of variance(CORANOVA) approach (Bilker et al., 2002) to handle missing at randomdata.  We will compare the EM algorithm to the multiple imputationmethod and complete case analysis method using simulation studies andthese methods will be illustrated with a depression diffusion tensorimaging (DTI) study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Missing data,Imaging,mandrade@mayo.edu,,Mariza de Andrade,Professor,Mayo Clinic,200 First Street SW,507-284-1032,507-284-9542,mandrade@mayo.edu,Evaluating the heterogeneity of polygenic variance component by sex and age on cardiovascular risk factors in Brazilian families,1,Suely,R,Giolo,"Dept. of Statistics, Federal University of Parana, BrazilHeart Institute, University of Sao Paulo, Brazil",Julia,M,Soler,"Dept. of Statistics, University of Sao Paulo, Brazil",Alexandre,C,Pereira,"Heart Institute, University of Sao Paulo, Brazil",Mariza,de, Andrade,"Dept. of Health Sciences Research, Mayo Clinic, USA",Jose,E,Krieger,"Heart Institute, University of Sao Paulo, Brazil",,,,,,,,,,,,,,,,,,,,,"In family studies it is important to evaluate the impact of genes and environmental factors on traits of interest as well as to investigate genes that can explain differences and similarities between individuals, particularly when comparing heritability between subgroups of individuals such as young and old or males and females. In order to evaluate the evidence for heterogeneity in genetic and environmental sources of variance in males and females and also in young and old individuals on six quantitative cardiovascular risk factors (diastolic and systolic blood pressure, LDL and HDL-cholesterol, fasting blood glucose and triglycerides), we used variance components models allowing for heterogeneity and three survival censored traits (age of diagnosis of hypertension, diabetes and cholesterol) using the random-effects Cox proportional hazards model. We used information of 81 families, involving 1,675 people of the Baependi family heart study. We found evidence of sex and/or age differences in variance components for some of the traits analyzed. In some cases such differences affected the genetic variance, in others the environmental variance, and in other cases both of these variances. For visualizing such sources of heterogeneity we will present some useful graphics. ",FALSE,FALSE,T4: Receiver Operating Characteristic (ROC) Curves,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Random effects,michelle.quinlan@boehringer-ingelheim.com,,Michelle Quinlan,Graduate Student,University of Nebraska-Lincoln,340 Hardin Hall North,402-472-2903,,michelle.quinlan@boehringer-ingelheim.com,The Statistical Evaluation of Patient Response Questionnaires,1,James,,Schwenke,"Boehringer Ingelheim Pharmaceuticals, Inc.",Michelle,,Quinlan,University of Nebraska-Lincoln,Rachel,,Ginsburg,Cornell University,Chad,,Nivens,"Boehringer Ingelheim Pharmaceuticals, Inc.",,,,,,,,,,,,,,,,,,,,,,,,,"The Cough and Sputum Assessment Questionnaire (CASA Q) is a patient-reported questionnaire to assess cough and sputum symptoms in patients with Chronic Obstructive Pulmonary Disease (COPD). Principal Component Analysis (PCA) was conducted on the 25-question questionnaire to determine a set of response measures that are optimal, most sensitive, and most responsive in characterizing the patient population and potential effects to treatment. Traditional methods for analyzing response questionnaires through pre specified domain scores do not always give complete information concerning patient response and patient populations. PCA is a value added analysis allowing for an investigation of the correlation among the questions across domains. This approach allows for a data driven analysis of the patient response data to determine the most optimal response scores to characterize the CASA Q. Benefits of using PCA on the CASA Q include; increased understanding of the questionnaire; possible increased power to quantify and detect therapeutic changes in clinical and lifestyle COPD symptoms; and, possible increased ability to define patient populations. The results of a PCA on study results from a COPD study will be summarized. Various attributes concerning patient response and patient populations will be discussed in addition to the standard domains associated with the CASA Q. Key words: COPD, CASA-Q, Principal Component Analysis",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biopharmaceutical research,Multivariate methods,bill.barry@duke.edu,,Bill Barry,Assistant Professor,Duke University,2424 ERWIN ROAD SUITE 802,919-681-5047,,bill.barry@duke.edu,Strategies for Applying Gene Signatures to Prospective Clinical Studies,1,William,T,Barry,"Department of Biostatistics and Bioinformatics, Duke University Medical Center",Michael,,Datto,"Department of Pathology, Duke University Medical Center",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genome-wide expression profiling has been used extensively inpre-clinical cancer research, having led to many new insightsregarding breast cancer biology and disease heterogeneity.  However,many approaches for identifying gene signatures in association totumor subtypes and clinical outcome are only applicable toretrospective analyses.  In order to translate these discoveries intoprognostic and predictive biomarkers, the algorithms must be modifiedto be used in a completely prospective manner.  Herein, we generatemicroarrays on 51 replicate samples (18 patients) obtained from theDuke Breast SPORE tissue repository. Single-gene and multi-genebiomarkers are applied in a prospective manner to assess concordanceand validation against standard immunohistochemical assays (IHC) oftumor biology.  A single-gene predictor of ER shows completeconcordance in 17 of 18 patients and >95% agreement with IHC; likewise, novel single-gene predictors of PR and EGFR are generatedfrom IHC results.  Multi-gene signatures of response to chemotherapieshave been generated from a Bayesian probit regression model by Pottiet al (2006) and will be applied in a randomized  neoadjuvant breastcancer trial.  Intra-class correlation for adriamycin and docetaxelsignatures were 89% and 86%, respectively.  Bootstrap-based confidenceintervals illustrate the robustness of applying gene signatures in aprospective manner.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Clinical trials,tor.tosteson@dartmouth.edu,,"Tor D. Tosteson, ScD",Professor,Dartmouth Medical School,Rubin 7927,603 653-3677,603 653-9094,tor.tosteson@dartmouth.edu,Augmenting Instrumental Variables Estimators in a Two-Stage Design,1,Tor,D,Tosteson,"Dartmouth Medical School,Hanover, NH",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider an example of an epidemiologic study relating two continuous exposure measurements for arsenic, toenail (T) and tap water (W) concentrations, to a continuous measure of gene expression serving as a potential intermediate marker of cancer incidence.  Because of budget concerns, gene expression has been evaluated on a relatively small subsample of controls in a large case-control study which has toenail and water concentrations for most participants.  Both exposure measurements are subject to measurement error, but the toenail is thought to be unbiased for the true biological exposure.  In a study in which one observes (Y,W,T) and a simple linear model applies, the instrumental variables method of moments estimator for the slope coefficient can be expressed as cov(Y,W)/cov(T,W). Because there is only a small sample, it appears that a more accurate estimate might be obtained by augmenting the data available for determining cov(T,W) with data from the main study.  We compare the performance of the ordinary instrumental variable estimate, the augumented instrumental variable estimate, and a maximum likelihood estimate under normality assumptions with asymptotic methods and small sample simulations. The results are applied to data from the arsenic case-control study.",FALSE,FALSE,T3: Statistical Analysis of Cost Effectiveness Data,FALSE,TRUE,TRUE,"SC1,SC5",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Measurement error,Epidemiologic methods,gdiao@gmu.edu,,Guoqing Diao,Dr.,George Mason University,"Department of Statistics, George Mason University, MS 4A7",703-993-9113,,gdiao@gmu.edu,Semiparametric Cure Rate Models for Current Status Data,1,Guoqing,,Diao,"Department of Statistics, George Mason University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this research we study a class of semiparametric cure rate modelsfor the analysis of current status data. Thisclass includes the commonly used mixture cure rate model andproportional hazards cure model as special cases.We show that the nonparametric maximum likelihood estimators for theregression parameters of these models areconsistent, asymptotically normal, and asymptotically efficient. Weconduct extensive simulation studies toevaluate the performance of the proposed method. An illustration witha real study is provided.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Empirical likelihood,pryseley.assamnkouibert@uhasselt.be,,Assam N. Pryseley,Student,University of Hasselt,Agoralaan - building D,+32-11-268293, +32-11-268299,pryseley.assamnkouibert@uhasselt.be,Information Theoretic Approach to Surrogate Markers Evaluation for Time-to-Event Clinical Endpoints,1,Pryseley,N.,Assam,"Center for StatisticsHasselt UniversityAgoralaan - building D3590 DiepenbeekBelgium",Abel,E.,Tilahun,"Center for StatisticsHasselt UniversityAgoralaan - building D3590 DiepenbeekBelgium",Ariel,,Alonso,"Center for StatisticsHasselt UniversityAgoralaan - building D3590 DiepenbeekBelgium",Geert,,Molenberghs,"Center for StatisticsHasselt UniversityAgoralaan - building D3590 DiepenbeekBelgium",,,,,,,,,,,,,,,,,,,,,,,,,"Recent work in the area of surrogate markers validation in themultitrial framework led to definitions in terms of the quality oftrial- and individual-level association between a potential surrogateand a true (clinical) endpoint (Buyse et al. 2000, Biostatistics 1, 49- 69). A drawback is that different settings have led to differentmeasures at the individual level. A unified framework for thedifferent settings was developed based on information theory, foroutcomes with distribution in the exponential family, leading to adefinition of surrogacy with an intuitive interpretation (Alonso andMolenberghs, 2007, Biometrics 63, 180 - 186). The later approach hasbeen applied to a wide range of settings but not to time-to-event(censored) true endpoint, which is the primary objective of this workwith focus on the individual-level association. The performance offour measures for surrogate markers validation based on informationtheoretic approach (ITA) for time-to-event true endpoints wasinvestigated through a simulation study and applied to a case study.Keywords: Surrogate endpoint; True endpoint; Information TheoreticApproach (ITA); Frailty parameter; Regression Calibration; LikelihoodReduction Factor (LRF); Kullback-Leibler distance",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biomarkers/surrogate markers,Survival analysis,xzmpc@mizzou.edu,,Xinyan,,University of Missouri,146 Middlebush Hall,(573)2562880,,xzmpc@mizzou.edu,Parametric Analysis of Interval Censored Data with Informative Cluster Size,1,Xinyan,,Zhang,"Department of Statistics, University of Missouri, 146 Middlebush Hall, Columbia, MO 65211, U.S.A.",Jianguo,,Sun,"Department of Statistics, University of Missouri, 146 Middlebush Hall, Columbia, MO 65211, U.S.A.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper considers the problem of parameter estimate for interval censored failure time data with informative cluster size. These types of data often arise in biomedical research settings. Ignoring the possible informativeness of cluster size could lead to a biased and misleading result. For right censored data, Cong et al. (2007) and Williamson et al.(2007) proposed weighted score function and within cluster resampling method. However, for this topic, very little research was done on interval censored data because of its complicated nature. In this project, we extend the idea and proposed weighted generalized estimating score equation method and within-cluster resampling approaches to interval censored data. Simulation studies are conducted for the evaluation of the presented approaches and demonstrate that the proposed methods produce unbiased parameter estimates. The proposed methods are also illustrated by application to a lymphatic filariasis survival data. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Clustered data methods,yy@math.asu.edu,,Yan Yang,Asst Prof,Arizano State University,Dept of Mathematics and Statistics,4809656475,,yy@math.asu.edu,Conditional Assessment of Zero-inflated Mixture Models,1,Yan,,Yang,Arizona State University,Doug,G,Simpson,University of Illinois at Urbana-Champaign,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The class of zero-inflated mixture models has been widely used toanalyze data bounded below by zero with an excess of zeroobservations. However, little attention has been focused onassessing the adequacy of these models. We propose a conditionaldecomposition approach that separately evaluates the fit for valuesat the boundary and the fit for values exceeding it. The model-basedconditional mean and quantiles for values greater than the lowerbound and marginal mean and quantiles for all values are derived.Confidence intervals with delta method standard errors are thenimplemented for the probability of the boundary event, conditionalmean and conditional quantiles to assess inflated mixture models. Asimulation study is conducted to investigate the finite-samplebehavior of the intervals. The usefulness of the proposed methods isillustrated for data from an ultrasound safety study and from ameasles vaccine study, where conditional evaluations help suggest areason for the lack of fit of current models and thus lead toimproved ones.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Other,Nonlinear models,"Model diagnostics; Zero-inflated Models, Finite mixture models"phsu@azcc.arizona.edu,,Chiu-Hsieh Hsu,,University of Arizona,Arizona Cancer Center,5206265054,,phsu@azcc.arizona.edu,A Double Robust Local Multiple Imputation,1,Chiu-Hsieh,,Hsu,University of Arizona,Qi,,Long,Emory University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A robust local multiple imputation approach is proposed to recover information for missing observations. To conduct the imputation, we use two working models to create two predictive scores. One score is derived from a linear regression model to predict the missing values. The other is derived from a logistic regression model to predict the missing probabilities. The two scores are then used to decide the resampling and imputing probabilities via kernel regression for each missing observation. Under an assumption of missing at random mechanism, the imputation approach is shown to be robust to misspecification of either of the two working models and to be robust to misspecification of the distribution of the data. In addition, simulation comparisons with other methods suggest that the method works well in a wide range of populations. The approach is demonstrated on a dataset from a colorectal polyp prevention trial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Nonparametric methods,anniequ@illinois.edu,,Annie Qu,Associate Professor,University of Illinois at Urbana Champaign,"Department of Statistics, 101 Illini Hall",217-244-8334,217-244-7190,anniequ@illinois.edu,Incorporating Correlation for Multivariate Failure Time Data When Cluster Size Is Large,3,Li,,Wang,Oregon State University,Lan,,Xue,Oregon State University,Annie,,Qu,University of Illinois at Urbana-Champaign,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a new estimation method for multivariate failure timedata using the quadratic inference function (QIF) approach. Theproposed method efficiently incorporates within-clustercorrelations. Therefore it is more efficient than those which ignorewithin-cluster correlation. Furthermore, the proposed method is easyto implement. Unlike the weighted estimating equations in Cai &Prentice (1995), it is not necessary to explicitly estimate thecorrelation parameters. This simplification is particularly usefulin analyzing data with large cluster size where it is difficult toestimate intracluster correlation. Under certain regularityconditions, we show the consistency and asymptotic normality of theproposed QIF estimators. A Chi-squared test is also developed forhypothesis testing. We conduct extensive Monte Carlo simulationstudies to assess the finite sample performance of the proposedmethods.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Multivariate survival,Estimating equations,yaoli@ufl.edu,,Yao Li,,"Statistics, University of Florida",956 Innsbruck Drive,352-8703920,,yaoli@ufl.edu,An Epidemiological Model for Genetic Mapping of Viral Pathogenesis,1,Yao,,Li,"Department of Statistics, University of Florida",Arthur,,Berg,"Department of Statistics, University of Florida",Maryon,M,Chang,"Department of Epidemiology and Health Policy Research, University of Florida",Rongling,,Wu,"Departments of Public Health Sciences and Statistics, Pennsylvania State University",,,,,,,,,,,,,,,,,,,,,,,,,"Several serious human infectious diseases, such as AIDS, hepatitis B, influenza, and rabies, are the consequence of the interactions between the causative agent (virus) and host through the regulation of environmental and epidemiological factors. The identification of genes from the virus and host genomes will facilitate the understanding of the etiology of the diseases and the application of this information to develop antiviral drugs. Here, we will present a statistical model for characterizing genes and their interactions responsible for an infectious disease in a human population. The model incorporates the epidemiological mechanisms of the disease into a statistical framework for genetic haplotyping with multi-locus sequence data. In particular, the effects of genes from the transmitters (close contacts of patients) will be embedded in the model, allowing the characterization and test of genes from three different genomes (virus, host and transmitter). A testing procedure is constructed for study main genetic effects of different genomes and their epistatic interactions with different orders. We detect that high-order epistasis can be estimated, provided that an adequately large sample size (e.g., 2000 or more) is given. The new epidemiological model was investigated and validated through simulation studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Epidemiologic methods,qli@biostat.ufl.edu,,Qin Li,,University of Florida,"Biostat, Department of Epidemiology and Health Policy Research",(352)2650111ext85927,(352)2658047,qli@biostat.ufl.edu,Haplotyping Inherited Human Diseases with a Family-Based Design,1,Qin,,Li,"Department of Statistics, University of Florida",Arthur,,Berg,"Department of Statistics, University of Florida",Rongling,,Wu,"Department of Statistics, University of Florida;Department of Public Health Sciences, Department of Statistics, Pennsylvania State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The understanding of the genetic etiology of inherited diseases is oneof the main focuses in medical genetic research. Recent geneticanalyses suggest that genes may trigger their effects on inheriteddiseases through different expression of haplotypes constructed byalleles at multiple loci. We derive a statistical model forcharacterizing risk haplotypes associated with an inherited disease.The model is founded on a family-based design composed of the father,the mother, and their offspring. Each member in a family is genotypedfor a panel of loci, although the disease can be phenotyped only forthe offspring. A two-level hierarchical likelihood is formulated toestimate population genetic parameters with parental information andquantitative genetic parameters (including haplotype effects) and therecombination fraction with offspring data. A complex structure of theEM algorithm is derived, providing precise and efficient estimates ofall the underlying parameters. Simulation studies are performed totest the statistical behavior of the model under different samplingstrategies (few families vs. large size or many families vs. smallsize), different heritabilities and a range of genetic parameters. Thenew model will provide a timely tool for detecting risk haplotypes forinherited diseases from an increasing amount of family-based data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,jie.yang@stjude.org,,Jie Yang,,St Jude Children's Research Hospital,"262 Danny Thomas Place, MS 768",9015952666,,jie.yang@stjude.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,zwang@galton.uchicago.edu,,Zuoheng Wang,,University of Chicago,108 Eckhart Hall,773-702-8330,,zwang@galton.uchicago.edu,Testing Untyped SNPs in Case-Control Association Studies with Related Individuals,1,Zuoheng,,Wang,"Department of Statistics, University of Chicago",Mary Sara,,McPeek,"Departments of Statistics and Human Genetics, University of Chicago",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Genome-wide association study has been a popular tool for identifyinggenetic factors influencing human complex disease.  However only asubset of the total variations across the genome are genotyped in therecent high-throughput genotyping platforms.  We introduce a newapproach to testing for association between untyped variants and a binarytrait, using linkage disequilibrium (LD) information between typedmarkers and untyped markers.  This information can often be obtainedfrom an appropriate reference panel such as HapMap.  We construct a1-d.f. incomplete-data quasi-likelihood score (IQLS) test based on anIQLS function.  Because the external LD information is only used toform the alternative mean model, our method maintains the nominalsingle-SNP type I error rate even when the external LD information ismisspecified.  As a result, our method is robust to an inappropriatechoice of the reference samples for testing untyped SNPs. Furthermore, the IQLS function was previously proposed to address bothdependent and partially-observed data, our 1-d.f. IQLS testappropriately accommodates the problem of haplotype-phase ambiguity ingenotype data, and can also be applied to case-control associationstudies in which some sampled individuals are related, with knownrelationships.  We apply the method to test for association with type2 diabetes in the Framingham Heart Study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,fabebe@cau.edu,,Fisseha Abebe,Associate Professor,Clark Atlanta University,223 James Brawley Dr,404-880-8975,,fabebe@cau.edu,Characterization of mRNA secondary structure using Weibull random Variable,1,Fisseha,,Abebe,Clark Atlanta University,William,,Seffens,Clark Atlanta University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Characterization of transcriptomes using Weibull random variable was first obtained by Cherkasov, Sui, Brunham, and Jones (2004). We have found that Weibull random variable with its general functional form also approximates, with very high accuracy, the distribution of mRNA folding free energy. The variation in the distribution between genomes is characterized by the Weibull reliability parameter _. The results of this work demonstrate that reliability analysis can be modified to provide useful insights to model a number of structural genomic studies.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Nonlinear models,Bayesian methods,kwj2@pitt.edu,,Kwonho Jeong,Doctoral student,University of Pittsburgh,513 North Neville St.,412-251-8877,,kwj2@pitt.edu,Estimation and comparison of the predictiveness curve for repeated measures design,1,Kwonho,,Jeong,University of Pittsburgh,Abdus,M,Sattar,University of Pittsburgh,Lisa,,Weissfeld,University of Pittsburgh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the Genetic and Inflammatory Marker of Sepsis (GenIMS) study (a large multicenter cohort study), a number of pro-inflammatory and anti-inflammatory continuous biomarkers associated with severe sepsis and death have been measured longitudinally. In this work, we are proposing to extend the theory of the predictiveness curve (PC) that has been developed by Huang, Pepe and Feng (Bcs2007) for longitudinally measured continuous biomarkers data. We fitted the PC using longitudinally measured GenIMS biomarker data for comparison of their effectiveness in predicting the risk of death. The PC has provided a common scale (zero to one) across various markers for comparing the usefulness of a given marker relative to other potential markers. Using this graphical tool, we have compared population distribution of risk of death for a number of competitive biomarkers associated with the disease. An extensive simulation study has been undertaken to establish the properties of the proposed methods under differing scenarios. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Biomarkers/surrogate markers,ROC analysis,xiang.qinfang@endo.com,,QINFANG (Steve) XIANG,Statistician,Endo Pharmaceuticals Inc.,100 Endo BLVD,610-459-6433,,xiang.qinfang@endo.com,R programs for calculating sample size and power in bioequivalence trials,1,Qinfang,,Xiang,"Research and Development Biostatistics,Endo Pharmaceuticals, Inc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Crossover designs are the primary statistical designs for bioavailability and bioequivalence studies. Sample size and power calculation could be challenging for high order crossover designs since the calculations involve non-central t-distribution and therefore numerical integration or approximation has to be used. Some existing standard statistical software for power and sample size calculations do not provide such functionalities. R language is open source for statistical computing and has been widely used in academics and industry around the world. This presentation introduces a set of R programs to determine sample size and power for average bioequivalence testing using crossover designs, including standard 2_2 crossover designs, commonly used four higher order crossover designs, replicated 2_2m (me2) crossover designs, and Williams designs. Some power curves and sample size tables are also easily to be generated using the provided R programs.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Power analysis/sample size,Biopharmaceutical research,saonli@umn.edu,,Saonli Basu,Assistant Professor,University of Minnesota,A 460 Mayo Building MMC 303,6126242135,,saonli@umn.edu,A likelihood-based approach for detecting gene-gene interaction in a case-control study,1,Saonli,,Basu,"Division of Biostatistics, University of Minnesota",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many complex traits of medical relevance such as Diabetes, Asthma, andAlzheimer's disease are controlled by multiple genes. Statisticalmethods for the detection of gene- gene interactions in a case-controlstudy can be categorized broadly into parametric and nonparametricapproaches. Among these different nonparametric approaches, there is agrowing popularity of the Multifactor Dimensionality Reduction (MDR)approach and it has been recently extensively used for gene-geneinteraction detection in many real studies. The strong point in favorof MDR is that it can detect snps associated with a disease. Itsearches through any level of interaction without considering thesignificance of the main effects. It is therefore able to detecthigh-order interactions even when the underlying main effects arestatistically not significant. We have implemented similar datareduction strategy of MDR within a likelihood framework, which can beused to assess the statistical significance of a k-order gene-geneinteraction. This approach also can estimate the combined effect of agroup of high-risk snps under our model. By means of simulationstudies, we have compared the performance of our proposed model withother existing techniques. The performances of all these methods arealso studied on a real dataset using ROC analysis.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Genomics,sstock@hsph.harvard.edu,,Shannon Stock,,Harvard University,15 Queensberry St,559-977-5967,,sstock@hsph.harvard.edu,Recursive Partitioning for Longitudinal Markers Based on a U-Statistic,1,Shannon,,Stock,Harvard University,Victor,,DeGruttola,Harvard University,Chengcheng,,Hu,The University of Arizona,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The development of HIV resistance mutations can reduce the efficacy of specific antiretroviral drugs used to treat HIV infection, and cross-resistance within classes of drugs is common.  Recursive partitioning has been extensively used to identify resistance mutations associated with a reduced virological response measured at a single time point; here we describe a statistical method that accommodates a large set of genetic or other covariates and a longitudinal response. Our recursive partitioning approach for continuous longitudinal data uses the kernel of a U-statistic as the splitting criterion. The method is flexible and avoids the need for parametric assumptions regarding the relationship between the observed response trajectories and covariates. Under the assumption that some longitudinal measurements are missing at random, we extend our estimators to accommodate such missingness by incorporating inverse probability weights using either recurrent event processes or direct modeling of the probability of observing a response. The performance of our method is explored using simulation studies and by investigating its asymptotic properties. We illustrate this method using data combined from a variety of clinical research studies that investigated the drug Abacavir among patients for whom baseline HIV genotype was available. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,Nonparametric methods,zhangl3@ccf.org,,Li Zhang,Assistant Staff,Cleveland Clinic Foundation,he Department of Quantitative Health Sciences,216-445-7747,,zhangl3@ccf.org,Modeling Haplotype-Haplotype Interactions in Case-Control Genetic Association Studies,1,Li,,Zhang,Cleveland Clinic Foundation,Rongling,,Wu,Pennsylvania State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Haplotype analysis has been increasingly used to study the geneticbasis of human diseases, but models for characterizing geneticinteractions between haplotypes from different chromosomal regionshave not well been developed in the current literature. In this talk,we will present a statistical model for testing haplotype-haplotypeinteractions for human diseases with a case-control geneticassociation design. The model is formulated on a contingency table inwhich cases and controls are typed for the same set of molecularmarkers. We derive the EM algorithm to estimate and test differencesin the pattern of genetic variation between cases and controls.Traditional quantitative genetic principles are integrated into themodel to characterize epistatic interactions of haplotypes fromdifferent chromosomal regions. The model allows the partition ofepistasis into different components due to additive x additive,additive x dominant, dominant x additive, and dominant x dominantinteractions. A testing procedure is framed to test the roles of eachof these components in the pathogenesis of human diseases. Simulationstudies and real examples are used to validate the usefulness andutilization of the model.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Epidemiologic methods,qixuan@umich.edu,,Qixuan Chen,,University of Michigan,"3645 GreenBrier Blvd., Apt.151C",510-396-5231,,qixuan@umich.edu,Bayesian Inference of Finite Population Distribution Functions and Quantiles from Unequal Probability Samples,1,Qixuan,,Chen,"Department of Biostatistics, University of Michigan School of Public Health",Michael,R.,Elliott,"Department of Biostatistics, University of Michigan School of Public Health",Roderick,J.A.,Little,"Department of Biostatistics, University of Michigan School of Public Health",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper develops two robust Bayesian model-based estimators offinite population distribution functions and associated quantiles forcontinuous variables in the setting of unequal probability sampling,where inferences are based on the posterior predictive distribution ofthe non-sampled values.  The first method fits a multinomial ordinalprobit regression model of the distribution function evaluated atmultiple values on a penalized spline of the selection probabilities. Finite population quantiles are then obtained by inverting thedistribution function.  However, heavy computation is involved ininverting the distribution function; therefore we consider the secondmethod that posits a smoothly-varying relationship between thecontinuous outcome and the selection probabilities by modeling boththe mean function and the variance function using penalized splines. Simulation studies show that both methods yield estimators that aremore efficient with closer to the nominal level credible intervalsthan the design-based estimators.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Survey research data,Epidemiologic methods,shim@stat.wisc.edu,,Heejung,,University of Wisconsin-Madison,"Department of Statistics, UW-Madison, Medical Science Center, 1300 University Ave.",608-469-8871,,shim@stat.wisc.edu,Joint Bayesian estimation of phylogeny and sequence alignment,1,Heejung,,Shim,University of Wisconsin at Madison,Bret,,Larget,University of Wisconsin at Madison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Phylogeny and sequence alignment are estimated separately in the traditional techniques : first estimate a multiple sequence alignment, and then infer a phylogeny based on the sequence alignment estimated in a previous step. We develop a joint model for co-estimating phylogeny and sequence alignment which avoids biased and exaggerated estimations from the traditional approach. Our indel model allows indel events to involve more than one letter and overlap each other. Since our method doesn't use a dynamic programming, we expect improvement in time complexity. We use a Bayesian approach using MCMC to estimate the posterior distribution of phylogenetic tree and a multiple sequence alignment.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Bayesian methods,seier@etsu.edu,,Edith Seier,PhD,East Tennessee State University,10 Brooklawn Ct,(423) 439 5812,,seier@etsu.edu,The statistical component of the Symbiosis experience at ETSU,1,Edith,,Seier,"Department of MathematicsEast Tennessee State University",Karl,,Joplin,"Department of Biological SciencesEast Tennesee State University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Symbiosis, a project with HHMI funding (Grant # 52005872, 2006-2010) ,is our answer to the National Research Council initiative BIO2010: Transforming Undergraduate Education for Future Research Biologists.The Symbiosis project is mainly intended for biology majors. However, the courses are also open to mathematics majors who wish to have an early vision of the applications of mathematics and statistics to the field of biology. We have developed a sequence of three double-credited integrated courses that start in the first semester of the freshman year, and are co-taught by faculty from biology, mathematics and statistics.This poster focuses on the statistical component of the three Symbiosis courses. Among the characteristics of the statistical component are the early introduction of statistical inference via randomization methods and exposing the students to the R language. The material prepared for the statistical component of the sequence of courses can be easily used in statistics courses/workshops for undergraduate life sciences students.",FALSE,FALSE,,FALSE,FALSE,TRUE,Tutorial: T3-Genetic & Microarray data analysis ,contributedposter,,,,FALSE,FALSE,FALSE,FALSE,Biostatistics Education,Statistical education,tanzy@cmu.edu,,Tanzy Love,Dr,University of Rochester Medical Center,Department of Biostat and Comp Bio,4126571633,,tanzy@cmu.edu,Effect Modification of Prenatal Mercury Exposure Association with Developmental Outcomes by Social and Environmental Factors,1,Tanzy,M,Love,University of Rochester Medical Center,Sally,,Thurston,University of Rochester Medical Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Seychelles Child Development Study (SCDS) is testing thehypothesis that prenatal exposure to low doses of MeHg from maternalconsumption of fish is associated with the child's developmentaloutcomes.  No deleterious relationships between exposure to MeHg andcognitive functions have been identified in the primary analysis ofthe main cohort.  However, secondary regression analysis of thiscohort found a small effect modification by both caregiver IQ andhousehold socio-economic status(SES) score (Davidson et al. 1999). They showed that children with higher IQ caregivers and higher SES hada significant positive relationship between Mercury levels andintelligence.  We use latent classification techniques and a newcohort of children with measured nutrient information (particularlylong-chain fatty acids like Omega 3) to further examine the relationship. ",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Latent variables,Environmental and ecological applications,hoffmann@mcw.edu,,Ke Yan,Senior Biostatistician,Quantitative Health Sciences,Department of Pediatrics,414-955-7636,414-955-6331,hoffmann@mcw.edu,Methods for Calibrating Bivariate Laboratory Data,1,Ke,,Yan,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",Raymond,G,Hoffmann,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",Shi-Hwan,,Li,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",Robert,,Montgomery,"Department of PediatricsMedical College of Wisconsin",Pippa,,Simpson,"Quantitative Health SciencesDepartment of PediatricsMedical College of Wisconsin",,,,,,,,,,,,,,,,,,,,,"A laboratory calibrates a response curve for an assay to a standard byusing known  quantities (X).  The observed assay values (Y) are usedwith each standard X value to obtain a least squares fit.  Providedthe curve is monotonic, the least squares fit can be inverted to forma calibration curve that gives an adjusted value (X) for any newobservation (Y'). Thus the calibration problem is often described asan inverse regression problem. VonWillebrands Disease (VWD) is a clotting disorder where there aretwo related measurements which characterize the disease.  Moreover,the ratio of the two is also quite important diagnostically.  Both ofthese measurements need to be calibrated, to standards and preferablynot separately. Working with this data has motivated us to develop andcompare methods for the bivariate calibration problem.  Although theunivariate calibration problem has been extensively studied, resultsfor the bivariate calibration problem appear to be almost non-existentin the statistical literature.  The results depend on the level ofcorrelation between the two measurements, as well as how close theratio is to one.   ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Measurement error,Agreement,wkim@cas.usf.edu,,Wonkuk Kim,Assistant Professor,University of South Florida,Mathematics and Statistics,813-974-9551,,wkim@cas.usf.edu,Long term survivor models and two component mixture models,1,Wonkuk,,Kim,"Mathematics and StatisticsUniversity of South Florida",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The test of whether survival data follow a long term survivor model or a two component mixture model can be made out using the likelihood ratio test. The sample size and the asymptotic power of the likelihood ratio test are calculated under the generalized type I censoring. An example when the component density function is an exponential distribution is given. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Power analysis/sample size,Survival analysis,ywang42@emory.edu,,Yaping Wang,,Emory University,1231 Clairmont Road,404-254-2017,,ywang42@emory.edu,Association Study of G Protein-Coupled Receptor Kinase 4 Gene Variants with Essential Hypertension in Northern Han Chinese,1,Yaping,,Wang,"Department of Biostatistics and Bioinformatics, Emory University",Biao,,Li,"Institute of Biophysics, Chinese Academy of Sciences, Beijing, China",Weiyan,,Zhao,"Division of Population Genetics and Prevention, Cardiovascular Institute and Fu Wai Hospital",Pei,,Liu,"The School of Public Health Southeast University, Nanjing, China",Qi,,Zhao,"Department of Epidemiology, Tulane University",Shufeng,,Chen,"Division of Population Genetics and Prevention, Cardiovascular Institute and Fu Wai Hospital",Hongfan,,Li,"Division of Population Genetics and Prevention, Cardiovascular Institute and Fu Wai Hospital",Dongfeng,,Gu,"Division of Population Genetics and Prevention, Cardiovascular Institute and Fu Wai Hospital",,,,,,,,,"To investigate the association between polymorphisms in the G protein-coupled receptor kinase 4 gene (GRK4) (R65L, A142V and A486V) and essential hypertension in northern Han Chinese, we conducted a case-control study consisting of 503 individuals with essential hypertension (HT) and 490 age-, gender-, and area-matched normotensive (NT) controls. The three GRK4 variants were genotyped by PCR-RFLP analysis. Both haplotype and single locus analysis were used to process the genotyping data. The A486 allele showed a significant association with HT (P < 0.001). A total of 6 haplotypes were observed in the entire population, with the haplotypes L-V-A and R-A-A being found to be significantly related to hypertension (P = 0.001).",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Biomarkers/surrogate markers,hkatki@gmail.com,,Hormuzd Katki,Ph.D.,National Cancer Institute,6120 Executive Blvd Room 8016,301-594-7818,,hkatki@gmail.com,Insights into p-values and Bayes Factors via False Positive and False Negative Bayes Factors,1,Hormuzd,A,Katki,National Cancer Institute,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Bayes Factor and likelihood ratio have been shown to have astronger theoretical justification than p-values for quantifyingstatistical evidence.  However, when the goal of a study is solelyhypothesis testing, the Bayes Factor is a black box that does notyield insight about false positive versus false negative results.  Iintroduce the False Positive Bayes Factor and the False Negative BayesFactor and show that they are approximately the two components of theBayes Factor.  In analogy with diagnostic testing, decomposing theBayes Factor into the False Positive/Negative Bayes Factors providesadditional insight not obvious from the Bayes Factor. The FalsePositive/Negative Bayes Factors rely on only the p-value and the powerunder an alternative hypothesis, forging a new link of the p-value tothe Bayes Factor.  This link can be exploited in data analysis tounderstand any potentially contradictoryinferences drawn by Bayes Factors versus p-values.  The FalsePositive/Negative Bayes Factors provide insight in a genome-wideassociation study of prostate cancer by helping to reveal the two SNPsdeclared positive by p-values and Bayes Factors that with future dataturned out to be false positives.",FALSE,FALSE,T3: Statistical Analysis of Cost Effectiveness Data,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Diagnostic and screening tests,pippam.simpson@gmail.com,,Pippa Simpson,Director QHS,MCW,CRI - room C3440,414 955 4521,,pippam.simpson@gmail.com,Missing animals in toxic treatment studies,1,Pippa,M,Simpson,MCW,Shun,H,Li,MCW,Ke,,Yan,MCW,Bevan,E,Huang,CSIRO,Calvin,,Williams,MCW,Dipeca,,Haribhai,MCW,Raymond,G,Hoffmann,MCW,,,,,,,,,,,,,"Animals often have missing outcomes in studies of toxicity. Not infrequently it is because they become moribund and must be sacrificed. When death is not the outcome of interest, the primary outcome may be missing because of its value or other factors related to the outcome. Under these conditions the missing pattern is not MAR, but MNAR, and must be modeled. For example, a study of a carcinogen may have the number of tumors palpated in mice over time as an outcome, but the mice are sacrificed when the tumor burden is too great. Our analysis is motivated by a study of genetically engineered mice treated with different T regulatory cells with the weight pattern over time as an outcome of interest. One of the main criteria for sacrificing mice was excessive weight loss. In this case there is information about the possible mechanism for missing data. We use a nonlinear random coefficient model and look at various ways to model the dropout pattern. This includes a logistic regression model for dropout at each time conditional on the mouse still being in the study, on previous values of weight and possibly other variables. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Missing data,Random effects,cchang@njit.edu,,Chung Chang,Assistant Professor,"Department of Mathematical Sciences, New Jersey In","Department of Mathematical Sciences, New Jersey Institute of Technology",973-642-4097,,cchang@njit.edu,non-parametric estimation of a lifetime distribution with incomplete censored data,1,Chung,,Chang,"Department of Mathematical SciencesNew Jersey Institute of TechnologyUniversity HeightsNewark, New Jersey 07102-1982",Wei-Yann,,Tsai,"Department of BiostatisticsMailman School of Public HealthColumbia University722 West 168th Street, 6th FloorNew York, New York 10032",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the analysis of lifetime data, under some circumstances, censoringtimes for unfailed units are missing or only known within an interval(e.g., warranty data). Motivated by such examples, we consider astatistical model in which censoring times are incomplete. We proposean iterative method to obtain a nonparametric estimator of thesurvival function and conduct a simulation study to discuss itsproperty.   ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Nonparametric methods,zliu@gwu.edu,,Zhenyu Liu,,"Department of Statistics, The George Washington Un","2140 Pennsylvania Ave, NW",202-994-7583,,zliu@gwu.edu,Nonparametric Classifications of Tumors Using Gene Expression Data Based on the Triangle Data Depth,1,Zhenyu,,Liu,"Department of Statistics,The George Washington University",Reza,,Modarres,"Department of Statistics,The George Washington University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A reliable and precise classification of tumors using gene expression data is essential for successful diagnosis and treatment of cancers. Gene expression data are difficult to analyze using classical multivariate analysis as such data contain a very large number of variables (genes) relative to the number of observations (tumor samples), presenting a 'large p, small n' challenge to classical discriminate analyses and classification methods that require n>p. Notion of data depth provides an alternative way of ordering multivariate observations in high dimensional space; hence, reducing the high dimensional problem to one dimensional problem. Recently, we proposed the triangle data depth and applied it to the classical two-sample testing of equality of distribution functions in high dimensions. The triangle data depth enjoys computational simplicity and efficiency in high dimensions as its time complexity is O(n^2), which is independent of p. In this paper, we propose new nonparametric classification methods based on the triangle data depth for high dimensional settings such as gene expression observations. We will use real and simulated data to explore the properties of the proposed methods using the triangle depth and compare them with existing methods for the classification of tumors based on gene expression profiles. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Microarray analysis,Multivariate Classificationliping.huang@uky.edu,,Liping Huang,,University of Kentucky,700 Woodland Ave Apt G202,8595366746,,liping.huang@uky.edu,Regularized estimation in AFT models with high-dimensional covariates,1,Liping,,Huang,University of Kentucky,Mai,,Zhou,University of Kentucky,Arne,C,Bathke,University of Kentucky,,,,University of Kentucky,,,,,,,,,,,,,,,,,,,,,,,,,"Several recent researches have focused on the use of AFT models to predict survival times of future cancer patients by investigating their gene expression profiles based on microarray analysis. We investigate use of the elastic net regularization approach to estimation and variable selection in the accelerated failure time model with high-dimensional covariates based on the so called inverse probability of censor weighting method. Huang, Ma and Xie (2006) studied a similar setting using LASSO. However, after ordering the survival times, if the last patient is censored, the current weighting method for that patient is problematic especially so for data with high censoring. We propose and investigate some modified weighting methods for the last patient in this study and show that some modification has improved the prediction performance. We use V-fold cross-validation for tuning parameter selection. The proposed method is evaluated using simulations and applied to real data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Microarray analysis,survival datavanja@uchicago.edu,,Vanja Dukic,Associate Professor,U of Chicago,5841 S Maryland Ave,773-834-2172,773-702-1979,vanja@uchicago.edu,Non-parametric ROC curve meta-analysis with varying number of thresholds,1,Vanja,M,Dukic,"Department of Health StudiesUniversity of Chicago",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Standard meta-analytic methods combine information on only oneparameter, such as a simple treatment effect. For meta-analysis ofdiagnostic test accuracy, measures of both sensitivity and specificityfrom different trials are of meta-analytic interest, summarized as abivariate measure of accuracy, or possibly as a receiver operatingcharacteristic (ROC) curve. Motivated by an analysis of serumprogesterone tests for diagnosing non-viable pregnancy, we developsimple fixed-effects and random-effects summary ROC estimators, basedon a flexible density estimation technique. We contrast theperformance and risk of the new estimator to the simpler bivariatenormal summary ROC estimator in a series of simulations. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,ROC analysis,Bayesian methods,rwu@hes.hmc.psu.edu,,Rongling Wu,Professor,Pennsylvania State University,Department of Public Health Sciences,(717)531-7178,,rwu@hes.hmc.psu.edu,An Algorithm for Constructing an Imprinted Map of the Cancer Genome,1,Louie,R.,Wu,"Buchholz High School, Gainesville, FL",Yao,,Li,"Department of Statistics, University of Florida",Rongling,,Wu,"Department of Statistics, University of Florida, Departments of Public Health Sciences, Departments of Statistics, Pennsylvania State University",Arthur,,Berg,"Department of Statistics, University of Florida",,,,,,,,,,,,,,,,,,,,,,,,,"Genetic imprinting is the differential expression of an allele due toits parental origin. Increasing evidence shows that the number ofimprinted genes is much higher than previously thought and thatimprinting may influence cancer risk. Here we will present astatistical model for detecting genes that trigger imprinting effectson cancer risk by using genotyped single nucleotide polymorphisms(SNPs). The model is derived with a set of unrelated families(composed of two parents and offspring) sampled from a naturalpopulation. The model is incorporated with one of the existinghypotheses about cancer pathogenesis  chromosomal instability due toaneuploidy, a syndrome caused by extra or missing chromosomes andconstituting some of the most widely recognized genetic disorders inhumans. We develop an algorithm for estimating and testing theimprinting effects of genes on cancer risk by distinguishing theparental origin of chromosome doubling. If the SNPs used are assayedfrom an entire genome, the proposed model can be expanded to constructan imprinted map of the cancer genome. The imprinting model, whosestatistical properties are investigated through simulation studies,will provide a useful tool for studying the genetic architecture ofcancer risk.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Cancer applications,Statistical genetics,dhunter@stat.psu.edu,,David,Associate Professor,Penn State University,310 Thomas Building,8148630979,8148637114,dhunter@stat.psu.edu,Exponential-Family Random Graph Models for Biological Networks,1,David,,Hunter,Penn State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In the field of social networks, the use of Exponential-Family RandomGraph Models, or ERGMs, is becoming increasingly popular as thetheoretical understanding and software implementations of these modelsimprove.  Recently, researchers have begun to use the same tools toexplore biological networks as well.  After giving a briefintroduction to ERGMs, this talk critically examines recent workapplying ERGMs to biological networks, offers some suggestions forfuture work in this field, and provides a brief illustration ofanalysis of a biological network using ERGMs.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Computational methods,Biomarkers/surrogate markers,berg@ufl.edu,,Arthur Berg,Assistant Professor,University of Florida,Department of Statistics,352-392-1947,,berg@ufl.edu,An Algorithm for Constructing an Imprinted Map of the Cancer Genome,1,Louie,R,Wu,"Buchholz High School, Gainesville, FL 32606",Yao,,Li,"Department of Statistics, University of Florida, Gainesville, FL 32611",Rongling,,Wu,"Department of Statistics, University of Florida, Gainesville, FL 32611Departments of Public Health Sciences and Departments of Statistics, Pennsylvania State University, Hershey, PA 17033",Arthur,,Berg,"Department of Statistics, University of Florida, Gainesville, FL 32611",,,,,,,,,,,,,,,,,,,,,,,,,"Genetic imprinting is the differential expression of an allele due toits parental origin. Increasing evidence shows that the number ofimprinted genes is much higher than previously thought and thatimprinting may influence cancer risk. Here we will present astatistical model for detecting genes that trigger imprinting effectson cancer risk by using genotyped single nucleotide polymorphisms(SNPs). The model is derived with a set of unrelated families(composed of two parents and offspring) sampled from a naturalpopulation. The model is incorporated with one of the existinghypotheses about cancer pathogenesis - chromosomal instability due toaneuploidy, a syndrome caused by extra or missing chromosomes andconstituting some of the most widely recognized genetic disorders inhumans. We develop an algorithm for estimating and testing theimprinting effects of genes on cancer risk by distinguishing theparental origin of chromosome doubling. If the SNPs used are assayedfrom an entire genome, the proposed model can be expanded to constructan imprinted map of the cancer genome. The imprinting model, whosestatistical properties are investigated through simulation studies,will provide a useful tool for studying the genetic architecture ofcancer risk.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Cancer applications,Tatiyana.Apanasovich@jefferson.edu,,Tatiyana Apanasovich,Assistant Professor,Thomas Jefferson University,1015 chestnut str,6072278854,,Tatiyana.Apanasovich@jefferson.edu,"Zero-Inflated Binomial Spatial Models, With Applications To Colon Carcinogenesis",1,Tatiyana,V., Apanasovich,Thomas Jefferson Unievrsity,Marc,G., Genton,,Raymond,J,Carroll,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"When considering regression models for binomialspatial data in experimental colon carcinogenesis data, we haveobserved distinct regions where background rates of responseabruptly become zero. These zero-inflated regions cause difficultiesin, for example, low-order basis function modeling, because modelfits attempt to reproduce the regions of zeros. We cast this generalproblem as one in which there are two spatial processes. The firstis the underlying smooth regression surface, while the second is thespatial process that leads to regions of zeros. The methods areapplied to an experiment involving aberrant crypt foci in coloncarcinogenesis experiments.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Cancer applications,asparouhov@hotmail.com,,Tihomir Asparouhov,,Mplus,2723 Sherwood Dr,801-582-1296,,asparouhov@hotmail.com,Bayesian Estimation of Multilevel Mixture Models,1,Tihomir,,Asparouhov,Mplus,Bengt,,Muthen,UCLA,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We describe a general multilevel mixture model where latent class variable appear not only on the individual-level but also on the cluster-level. A fully Bayesian approach is used for model estimation using the Gibbs sampler. We contrast this estimation method with the maximum-likelihood estimation method. We illustrate the technique with multilevel analysis of achievement data with classification of both students and schools.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Hierarchical models,Bayesian methods,yongpark@umich.edu,,Yong Seok Park,Student,University of Michigan,1667 McIntyre ST,(734) 272-5547,(734) 763-2215,yongpark@umich.edu,Constrainted Survival Analysis,1,Yong Seok,,Park,"Biostatistics Department, University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"When we study survival analysis, we often have prior knowledge aboutstochastically ordered survival distributions.  For example, when weconsider time to event of cancer patients from diagnosis of cancer fordifferent tumor stages, the survival probability in lower stage groupare believed to be larger than that in higher stage group at any time. Researchers have proposed various approaches to restricted survivalestimators. Brunk et al(1966) and Dykstra(1982) studied theconstrained maximum likelihood estimators of two stochasticallyordered distribution from uncensored and censored data.   Rojo(1996)proposed and studied a new restricted survival estimators oftwo-sample case.  El Barmi et al extended Rojos estimators to thek-sample case assuming k populations with survival functions S1eS2 e,. . . , e Sk, k e2.  In this paper, we propose a criterion to extendRojos estimators under more general constraints than El Barmi et al,such as S1eS2eS4, S1eS3eS4.  We also propose an algorithm to obtainestimators under this criterion and study the properties of proposedestimators. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Constrained estimation/order restricted inference,htiwari@uab.edu,,Hemant K. Tiwari,Associate Professor,University of Alabama at Birmingham,420D RPHB,(205) 934-4907,(205) 975-2541,htiwari@uab.edu,Within-Cluster Resampling (Multiple Outputation) for Analysis of Family Data: Ready for Prime-Time?,1,Hemant,K,Tiwari,University of Alabama at Birmingham,Amit,,Patki,University of Alabama at Birmingham,David,B,Allison,University of Alabama at Birmingham,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Hoffman et al. (2001) proposed an elegant resampling method for analyzing clustered binary data. The focus of their paper was to perform association tests on clustered binary data using within-cluster-resampling (WCR) method. Follman et al., (2003) extended Hoffman et al.s (2001). Follmann et al. (2003) termed their procedure multiple outputation because all excess data within each cluster is thrown out multiple times. Herein, we refer to this procedure as WCR-MO. For any statistical test to be useful for a particular design, it must be robust, have adequate power, and be easy to implement and flexible. WCR-MO can be easily extended to continuous data and is a computationally intensive but simple and highly flexible method. Considering family as a cluster, one can apply WCR to familial data in genetic studies. Using simulations, we evaluated WCR-MOs robustness for analysis of a continuous trait in terms of type 1 error rates in genetic research. WCR-MO performed well at the 5% ±-level. However, it provided inflated type I error rates for ±-levels less than 5% implying the procedure is liberal and may not be ready for application to genetic studies where a levels used are typically much less than 0.05.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Genomics,schen33@emory.edu,,Shuo,,Chen,"1518 Clifton Rd., N.E., 3rd Floor",615-330-0061,,schen33@emory.edu,Connectivity analysis based on fMRI and DTI brain imaging data,1,Shuo,,Chen,"Department of Biostatistics and Bioinformatics Rollins School of Public HealthEmory University",DuBois,,Bowman,"Department of Biostatistics and Bioinformatics Rollins School of Public HealthEmory University",Gordana,,Derado,"Department of Biostatistics and Bioinformatics Rollins School of Public HealthEmory University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Recent development has shown promising advantage of usingmultimodality brain imaging data to build connectivity between regionsof the human brain. However, it still remains challenging to combinedifferent types of imaging data effectively and appropriately. Onegoal of our study is to stabilize the connectivity discovery from fMRIdata by utilizing the information of tractography probability from DTIdata. We proposed a novel approach based on mixture criteria oflikelihood and stability of connectivity structure. The tuningparameter between likelihood and stability is determined by thevariability of connectivity structure of different subjects.   ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Imaging,High dimensional data,phil.reiss@nyumc.org,,Philip Reiss,,New York University,"215 Lexington Ave., 16th floor",917-494-0260,,phil.reiss@nyumc.org,Testing for equal means with two samples of high-dimensional correlation matrices,1,Philip,T.,Reiss,New York University,Sang Han,,Lee,Nathan S. Kline Institute for Psychiatric Research,Eva,,Petkova,New York University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In functional connectivity studies with functional magnetic resonance imaging, each subject's data consist of a highly multivariate time series representing activity levels within different brain regions, and interest centers on the correlations among these time series.  In particular, neuroscientists may wish to test for differences in such correlations between age or diagnostic groups.  Because the number of correlations is typically much higher than the combined sample size, Hotelling's two-sample T^2 test cannot be used to compare the set of correlations between groups.  This talk will present a novel approach that employs closed-form expressions for optimal shrinkage both to improve estimation of the correlations themselves, and to allow for computationally feasible permutation testing for group differences based on a Hotelling-type statistic. ",FALSE,FALSE,,FALSE,FALSE,FALSE,"if possible, please do not schedule at the same time as the invited sessions on functional data analysis, feature selection, or brain imaging",oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Imaging,High dimensional data,pczhang@umich.edu,,Peng Zhang,Research Fellow,University of Michigan,439 West Hall,734-764-1196,,pczhang@umich.edu,Surrogate decision rule in Q-learning,1,Peng,,Zhang,University of Michigan,Bin,,Nan,University of Michigan,Susan,A.,Muphy,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Q-learning is a technique in reinforcement learning to learn an actionfunction which maximizes the value function. We consider the problemwhere the action is binary and the decision rule is linear. Bycontrolling the difference in the value function between the optimaldecision and the surrogate decision, we developed the algorithm andtheory to find the best surrogate decision rule in a subspace. Thisgives us another decision rule with the following properties: 1) Itdepends on fewer variables, and therefore is more feasible inpractice; 2) The surrogate decision rule helps to eliminate covariateswith no qualitative interactions; 3) The new decision usually haslower variance; 4) One can obtain a sequence of surrogate decisions atdifferent levels of bounds on the difference in value, and choose oneby the expert option.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Variable subset selection/model selection,swang@biostat.wisc.edu,,Sijian Wang,,"University of Wisconsin, Madison","K6/420 CSC, 600 Highland Ave",608-265-9167,,swang@biostat.wisc.edu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,pkubilis@ufl.edu,,Paul S. Kubilis,Assistant Research Scientist,University of Florida,419 McCarty Hall C,352-392-1946x237,352-392-8555,pkubilis@ufl.edu,A Flexible Regression Modeling Framework for Analyzing Seagrass Areal Coverage as Characterized by Braun-Blanquet (BB) Vegetation Cover Scores,1,Paul,S,Kubilis,IFAS Statistics Department - University of Florida,Mary,C,Christman,IFAS Statistics Department - University of Florida,Penny,,Hall,Florida Fish and Wildlife Research Institute,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The Florida Fish and Wildlife Conservation Commission (FWC) has been monitoring several species of seagrass in Florida Bay since 1995. Of interest is whether the areal coverage and extent of individual species have been changing over time, particularly in response to recent efforts at restoring natural fresh water flow through the Everglades. Seagrass bottom cover sampling is done using a spatially stratified design with restricted randomization of sampling locations within strata. At each location, 4-12 subsamples (quadrats) are observed and species presence/absence is recorded. If a particular species is present, the Braun-Blanquet (BB) vegetation ground cover score, an unequally spaced ordinal categorical variable, is used to describe the proportion of bottom area covered by the seagrass species within each quadrat. By viewing these BB-scores as interval-censored observations of a continuous random variable representing percent bottom cover, we have developed a flexible regression modeling framework for characterizing the status and trend of BB-score seagrass abundance. This model-based approach assumes an underlying Bernoulli distribution for the presence/absence of a seagrass species within a sampling quadrat, and an underlying Beta distribution for the true proportion of cover when the species is present.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Applied data analysis,pulakghosh@gmail.com,,Pulak Ghosh,Associate Professor,Emory University,1332 Brookhaven Circle,404-518-5128,,pulakghosh@gmail.com,Joint Modeling of Multivariate Longitudinal Data for Mixed Responses and Survival with Application to  Multiple Sclerosis Data,1,Pulak,,Ghosh,"Department of BIostatistics, emory University",Anneke,,Neuhaus,Sylvia Lawry Centre for Multiple Sclerosis Research,Martin,,Daumer,Sylvia Lawry Centre for Multiple Sclerosis Research,Sanjib,,Basu,Northern Illinois University,,,,,,,,,,,,,,,,,,,,,,,,,"Multiple sclerosis (MS) is one of the most chronic neurologicaldiseases in young adults with around 2.5 million affectedindividuals worldwide (Compston 2006). The most common presentingsymptoms are inflammation of the optic nerve, weakness, sensorydisturbances, gait disturbances and bladder dysfunction.So far only standard analysis methodology toestimate risks for relapse occurrence has been used. This includesmostly single endpoint survival analysis in which MRI informationis shrunken to baseline values or aggregated measures such asmeans. In the present analysis we aim to establish a model that allowsthe description and prediction of occurrence of relapses byconsidering processes in the brain (visualized on T1 and T2weighted MRI) simultaneously. These complex processes, togetherwith clinical baseline information, have never been considered inone model so far. We will use our model to evaluate strength ofdependencies of multivariate longitudinal MRI measures with theoccurrence of MS relapses.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Joint models for longitudinal and survival data,Bayesian methods,matyxx@langate.gsu.edu,,Yuanhui Xiao,Assistant professor,Georgia State University,30 Pryor Street,404-413-6405,,matyxx@langate.gsu.edu,Computation of Exact p-values for Nonparametric Test,1,Yuanhui,,Xiao,Georgia State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"As large sets of high-throughout data in genomics and protemics become more readily available, there is a growing need for fast algorithms designed to compute the exact p-values of distribution-free tests. In this talk we present some issues regarding exact p-values  as well some ideas about computing exact p-values for a class of distribution-free tests. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,Microarray analysis,NONlizhu@srph.tamhsc.edu,,Li Zhu,AssociateProfessor,Texas A&M Health Science Center,Department of Epidemiology and Biostatistics,979-458-0079,979-458-1877,lizhu@srph.tamhsc.edu,Solving the Misalignment Problem in the Analysis of Drug Issues,1,Li,,Zhu,"Texas A&M Health Science CenterDepartment of Epidemiology and Biostatistics",Lance,,Waller,Emory University,Paul,,Gruenewald,Prevention Research Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"It is well known that the choice of geographic unit can affect the interpretation of maps and the results of spatial analysis, a phenomenon known as the modifiable areal unit problem. A model-based hierarchical structure is developed to analyze data whose geographic units are misaligned in both space and time. We extend the multivariate Gaussian models to generalized linear mixed models that incorporate several forms of discrete data. The approach is illustrated with a dataset relating illicit drug use, a range of sociodemographic variables, geography, and time in Tracy, CA. Here geography is indicated with ZIP codes which change over a period of eleven years. Computing is implemented via a carefully tailored Metropolis-Hastings algorithm, with map summaries created using a geographic information system.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Epidemiologic methods,ltrippa@mdanderson.org,,Lorenzo,,MD Anderson Cancer Center,1400 Hermann Pressler Drive,393409132317,,ltrippa@mdanderson.org,Bayesian nonparametric combination of multiple diagnostic measurements.,1,Lorenzo,,Trippa,MD Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Receiver operating characteristic (ROC) curves are widely applied for evaluating the discriminatory ability of diagnostic tests. In many cases a multitude of tests for disease diagnosis are available. In presence of multiple diagnostic measurements it is often of interest to construct a composite score to improve the classification accuracy. We propose a Bayesian nonparametric model in order to synthesize multiple diagnostic measurements.  The model allows to construct composite scores and to evaluate their discriminatory ability. We will illustrate the use of the model through simulated data and an oncology study.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,ROC analysis,fedarko@umich.edu,,Rebecca R Andridge,,University of Michigan,Department of Biostatistics,734-274-1101,,fedarko@umich.edu,Proxy Pattern-Mixture Analysis for Survey Nonresponse,1,Rebecca,R,Andridge,University of Michigan,Roderick,J,Little,University of Michigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider assessment of nonresponse bias for the mean of a surveyvariable Y subject to nonresponse. We assume that there are a set ofcovariates observed for nonrespondents and respondents. To reducedimensionality and for simplicity we reduce the covariates to a proxyvariable X that has the highest correlation with Y, estimated from aregression analysis of respondent data. We consider adjustedestimators of the mean of Y that are maximum likelihood for apattern-mixture model with different mean and covariance matrix of Yand X for respondents and nonrespondents, assuming missingness is anarbitrary function of a known linear combination of X and Y. Wepropose a taxonomy for the evidence concerning bias based on thestrength of the proxy and the deviation of the mean of X forrespondents from its overall mean, propose a sensitivity analysis, anddescribe Bayesian versions of this approach. We propose using thefraction of missing information from multiple imputation under thepattern-mixture model as a measure of nonresponse bias. Methods aredemonstrated through simulation and data from the third NationalHealth and Nutrition Examination Survey (NHANES III).",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Survey research data,Missing data,jsharp@clemson.edu,,Julia Sharp,,Clemson University,237 Barre Hall,864.656.3252,,jsharp@clemson.edu,Statistically Appraising Affinity-Isolation Experiment Process Quality,1,Julia,,Sharp,Clemson University,John,,Borkowski,Montana State University,Denise,,Schmoyer,Oak Ridge National Laboratory,Greg,,Hurst,Oak Ridge National Laboratory,,,,,,,,,,,,,,,,,,,,,,,,,"Identifying valid protein-protein interactions relies on qualitydata from affinity-isolation experiments.  The quality of anexperiment can be reduced from biological error, processing error,and random variability.  If these errors are of any magnitude, theidentification of interacting protein pairs will be hindered.  Aknown mixture of proteins and peptides is processed through a massspectrometer as a quality control mixture.  Statistical qualitycontrol (SQC) procedures, including cumulative sum, the individualmeasurement, and moving range charts are used to assess thestability of the affinity isolation process using the qualitycontrol mixture.  The SQC measures presented can assist insetting preliminary control limits for identifying out-of-controlprocesses and investigate assignable causes for shifts.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Proteomics,Applied data analysis,dxykbay02@sneakemail.com,,Alex F. Bokov,Ph.D.,University of Texas Health Science Center,Epidemiology and Biostatistics (Mailcode 7933),210 723-9814,210 567-0921,dxykbay02@sneakemail.com,Survomatic: A User-Friendly Package for Analysis of Survival and Mortality Data.,1,Alex,F.,Bokov,"University of Texas Health Science Center at San Antonio, Department of Epidemiology and Biostatistics",Scott,D.,Pletcher,"Baylor College of Medicine, Department of Molecular and Human Genetics and Huffington Center on Aging",Jonathan,A.L.,Gelfond,"University of Texas Health Science Center at San Antonio, Department of Epidemiology and Biostatistics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The log-rank test is widely used in biomedical research fordetermining whether two distributions of survival times aresignificantly different from one another. However, the log-rank testcan fail to detect even a large difference between survival curves ifthey cross or if one group dies at younger or older ages than theother only during acertain segment of the overall lifespan for that population. This isparticularly problematic in the field of aging and longevity research,where the focus is on survival at extreme ages rather than just meanor median survival. Quantile regression and fitting of mortalitymodels (such as the Gompertz model) to the data are both alternativeapproaches which are more sensitive and more robust to complexdifferences between survivorship functions. Here we present an opensource, cross-platform, R-based software package that incorporates twoquantile regression tests as well as maximum likelihood estimation ofbest-fit mortality parameters. This package comes with an optionalgraphical interface that makes it possible for researchers to usewithout having to know the R statistical language. Download site:http://rmodest.r-forge.r-project.org/",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,TRUE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,FALSE,Survival analysis,Demography and population studies,aandrei@biostat.wisc.edu,,Alina Andrei,Postdoctoral Research Associate,University of Wisconsin-Madison,Department of Biostatistics and Medical Informatics,(608)263-7623,,aandrei@biostat.wisc.edu,Detecting non-linear dependencies in gene coexpression networks,1,Alina,,Andrei,"University Of Wisconsin-Madison, Department of Biostatistics andMedical Informatics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The reconstruction of gene regulatory networks is a main goal of many biological endeavors. Gene coexpression networks (GCNs) have beenwidely used as a simple and efficient approach to summarizedependencies among genes, while being computationally inexpensive. Ina GCN, the association between two genes is most often quantified byusing the Pearson correlation coeffcient (PCC) or the mutualinformation. However, the PCC is suitable for capturing lineardependencies, while the mutual information requires that expressiondata be discretized, hence incurring a certain information loss. Ingeneral, the problem concerning types of dependencies in microarraydata is not well studied. We propose an efficient approach toautomatically detect non-linear dependecies among genes. Simulationstudies show that the method detects non-linear associations with highsensitivity for moderate sample sizes, while controlling the FDR.Practical advantages are demonstrated using a breast cancer study dataset.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Microarray analysis,Genomics,graphical modelsxlin@hsph.harvard.edu,,Xihong Lin,Professor,Harvard School of Public Health,"Dept of Biostatistics, Building 2, Room 419",6174322914,6174325619,xlin@hsph.harvard.edu,Variable Selection in longitudinal data using regularized likelihoods,1,Xihong,,Lin,Harvard School of Public Health,,,, ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Variable selection becomes an increasingly important problem forlongitudinal studieswith high-dimensional genomic and proteomicdata, e.g., genome-wide association studies with longitudinal penotypes.We propose variable selection methods using regularized likelihoodsincludingthe seamless L_0 penalized likelihood, for mixed models.The theoretical properties of these methods are studied and theirfinite sampleperformance is evaluated using simulations. The methods areillustrated using several data examples.",FALSE,FALSE,,FALSE,FALSE,TRUE,"Please schedule my session on Monday or Tuesday, because I need to teach on Wednesday and I already have to cancel my class on Monday.",invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Longitudinal data,kwilkins@post.harvard.edu,,Kenneth J. Wilkins,Assistant Professor of Biometrics,"Infectious Disease Clinical Research Program, PMB",Uniformed Services University of the Health Sciences,301-295-0592,301-295-1812,kwilkins@post.harvard.edu,An approach to sensitivity analysis of nested case-control studies with outcome-dependent follow-up,1,Kenneth,J,Wilkins,"Infectious Disease Clinical Research Program, Department of Preventive Medicine & Biometrics, Uniformed Services University of the Health Sciences, Bethesda, Maryland",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper presents a likelihood-based approach to case-controlanalysis, appropriate when conducted within a well-defined cohort that(1) affords estimation of disease incidence, and (2) involves distinctstages of outcome-dependent follow-up. A Department of Defense /Veterans Affairs (VA) trauma casualty cohort motivates this method, withnested sampling of cases and controls from medical databases to targetestimation of disease-exposure association. Subjects are followed fromtrauma through the evacuation chain to military hospitals for boneinfection (osteomyelitis). Late-developing cases are ascertainedwithin the VA system. However, the probability of additional VAfollow-up may involve some dependence on disease risk; estimates ofoverall risk are thus sensitive to unverifiable assumptions about thisdependence. Using cohort-based estimates of disease incidence bystage, the proposed approach adapts the case-control weighted targetedmaximum likelihood estimation of Rose & van der Laan (2008) to examinehow estimated associations change under distinct outcome/follow-updependency assumptions posited within a sensitivity analysis. Thepaper concludes with simulations and an illustration using the U.S.Military HIVNatural History Study cohort.",FALSE,FALSE,,FALSE,FALSE,TRUE,T1 - Mon 8:30,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Defense and national security applications,Missing data,fhsu@wfubmc.edu,,Fang-Chi Hsu,,Wake Forest University School of Medicine,Medical Center Blvd,336-716-8457,,fhsu@wfubmc.edu,A support vector machine approach for genome-wide copy-number-variation study,1,Shyh-Huei,,Chen,"Department of Industrial Management, National Yunlin University of Science and Technology, Yunlin, Taiwan",Fang-Chi,,Hsu,"Department of Biostatistical Sciences, Wake Forest University School of Medicine, Winston-Salem, North Carolina",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Extensions of genome-wide association studies (GWAS) to copy-number variations (CNV) have already suggested that CNV should be accounted for a crucial component of human phenotypic variations.  Many CNV-discovery approaches are available in the literature; however, only few studies of CNV analysis associated with risk of common diseases have been reported because such association studies rely on the discoveries of CNV-containing regions which are hard to be identified.  By considering CNV as factors of resulting in human genetic disorders, a classification approach can be adopted to perform CNV association study.  In this study, a support vector machine with a heuristics search approach is proposed.  This method was applied to explore the association with prostate cancer risk in a case-control genome-wide CNV study, using the Affymetrix 5.0.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Machine learning,Data mining/massive data sets,zhangdb@purdue.edu,,Dabao Zhang,Assistant Professor,Department of Statistics,150 N. University St,(765)494-6046,,zhangdb@purdue.edu,Two-dimensional correlation optimized warping algorithm for aligning GCXGC-MS data,1,Dabao,,Zhang,"Department of Statistics, Purdue University",Xiaodong,,Huang,"Department of Chemistry and Bindley Bioscience Center, Purdue University",Fred,E,Regnier,"Department of Chemistry and Bindley Bioscience Center, Purdue University",Min,,Zhang,"Department of Statistics, Purdue University",,,,,,,,,,,,,,,,,,,,,,,,,"A two-dimensional (2-D) correlation optimized warping (COW) algorithmhas been developed to align 2-D gas chromatography coupled withtime-of-flight mass spectrometry (GC_GC/TOF-MS) data. By partitioningraw chromatographic profiles and warping the grid pointssimultaneously along the first and second dimensions on the basis ofapplying a one-dimensional COW algorithm to characteristic vectors,nongrid points can be interpolatively warped. This 2-D algorithm wasdirectly applied to total ion counts (TIC) chromatographic profiles ofhomogeneous chemical samples, i.e., samples including mostly identicalcompounds. For heterogeneous chemical samples, the 2-D algorithm isfirst applied to certain selected ion counts chromatographic profiles,and the resultant warping parameters are then used to warp thecorresponding TIC chromatographic profiles. The developed 2-D COWalgorithm can also be applied to align other 2-D separation images,e.g., LC_LC data, LC_GC data, GC_GC data, LC_CE data, and CE_CE data.",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Metabolomics,Computational methods,aklein25@uwo.ca,,Andreas Klein,Professor,University of Western Ontario,"SSC, Dept. of Psychology",5196612111-82721,,aklein25@uwo.ca,Detection of Surrogates using a Potential Outcomes Framework,1,Andreas,G,Klein,"University of Western Ontario, Dept. of Psychology",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper proposes a definition of surrogacy that is based on potential outcome notation. Surrogacy is defined as an association between individual causal effects of a treatment on an intermediate variable and an outcome. The idea that a surrogate marker is an indicator of an unobserved intermediate mechanism that lies in the pathway of a causal process is given an exact formalization.  Methodological difficulties with regard to Prentice's concept of surrogacy are discussed. A simulation-based procedure is presented that - under certain assumptions about the data and the nature of the causal process - can quantify the association between the individual causal effects on the intermediate variable and the outcome. The application of the new procedure is illustrated using an empirical data set. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Causal inference,Markers and surrogate markers,elinder@unh.edu,,Ernst Linder,Professor,University of New Hampshire,Dept. of Math. and Statistics,603 862 2687,,elinder@unh.edu,Median polish algorithms for automated anomaly detection in environmental sensor networks,1,Ernst,,Linder,"Dept. of Mathematics & Statistics, University of New Hampshire",Zoe,,Cardon,"Woods Hole, Marine Biology Laboratory",Jared,,Murray,"Dept. of Mathematics & Statistics, University of New Hampshire",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Environmental sensors malfunction often due to a variety of reasons,such as power failure, hardware deterioration, or interference byanimals.  Sensor networks are typically deployed for the purpose ofnew discovery, thus, an a-priori statistical model of the underlyingscientific reality is not available.  Therefore it is a challenge todecide which data represent typical behavior and which data isanomalous or extreme, indicating sensor malfunctioning.  Forcontemporaneous sensor responses we propose to fit a sequential medianpolish.  The two factors of the median polish are sensor effect, andtime effect.  We apply two outlier rules for automated anomalous datadetection:  One for extreme traces, and another for extreme residuals. We train the algorithm for various outlier thresholds relative to amanually cleaned data from 147 psychrometers that were utilized in anexperiment on water uptake, transpiration and redistribution of 21sage plant in a semi-arid area in Utah during the summer of 2007.  Wealso examine the use of median polish algorithms for discovery ofanomalous and interesting behavior of individual plants.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Data mining/massive data sets,lcbm9@mizzou.edu,,Ling Chen,,University of Missouri-Columbia,1308 Duval CT,573-673-6541,,lcbm9@mizzou.edu,A multiple imputation approach to the analysis of interval-censored failure time data with the additive hazards model,1,Ling,,Chen,University of Missouri-Columbia,Jianguo,,Sun,University of Missouri-Columbia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This paper discusses regression analysis ofinterval-censored failure time data, which occur in many fields includingdemographical, epidemiological, financial, medical, and sociologicalstudies. For the problem, we focus on the situationwhere the survival time of interest can be described by the additivehazards model and a multiple imputation approach is presented forinference.  A major advantage of the approach is its simplicity andit can be easily implemented by using the existing software packagesfor right-censored failure time data. Extensive simulation studiesare conducted and indicate that the approach performs well forpractical situations and is comparable to the existing methods. Themethodology is applied to a set of interval-censored failure time dataarisingfrom an AIDS clinical trial.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Survival analysis,Missing data,hxu@mcg.edu,,Hongyan Xu,,Medical College of Georgia,1120 15th St,706-721-3785,,hxu@mcg.edu,Haplotype inference from samples with population strcture,1,Hongyan,,Xu,"Department of Biostatistics, Medical College of Georgia, Augusta, GA, 30912",Varghese,,George,"Department of Biostatistics, Medical College of Georgia, Augusta, GA, 30912",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"With the progress of genome-wide association studies of complextraits, it is apparent that single-locus approach does not provideenough power to detect disease susceptible loci because most of themconfer small to moderate effect size. Consequently, multi-locus orhaplotype approaches are becoming increasingly popular in suchstudies. However, current genotyping technologies do not providehaplotype phase information. Statistical methods have been developedand shown to be rather efficient and cost-effective way to inferhalpotype from unphased genotype information in population basedsamples. Hidden population structure could be a potential problemaffecting allele frequency and haplotype structure. None of currentmethods takes account of population structure in the process ofhaplotype construction. In this study, we developed a statisticalmethod based on sample likelihood to infer haplotype from thegenotypes of population samples with substructure and explicitlytaking account of population structure in the samples. It couldfacilitate the large-scale genome-wide association studies insubstructured or admixed populations such as African Americans.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Genomics,robert.platt@mcgill.ca,,Robert Platt,Associate Professor,McGill University,4060 Ste Catherine W #205,514-934-1934x23288,514-412-4331,robert.platt@mcgill.ca,Multivariate meta-analysis: modelling correlation structures,1,Robert,W,Platt,McGill University,Khajak,,Ishak,United BioSource,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Meta-analyses of randomized trials with multiple outcomes typically treat the outcomes as independent and analyze them separately.  Several authors have proposed frequentist and Bayesian multivariate models for multiple outcomes, designed to perform joint analysis to account for correlation between outcomes.  We review assumptions implicit in these models, in particular related to the type and source of apparent correlation between outcomes.  We examine whether the correlations measured from aggregated meta-analytic data reflect the relationships of interest, or whether these are distorted by other factors like correlated errors within studies that might induce similarity between observed outcomes.  The focus of our analysis was not the estimation of correlations, but rather the dependencies reflected in the true correlations underlying each study.  We simulated studies of the effect of a treatment on two dichotomous endpoints, incorporating various sources of correlation.  We found considerable overlap in correlations observed across different scenarios in which we varied the strengths of the associations of interest; thus, the true relationship between treatment effects could not always be inferred accurately.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Clinical trials,Multivariate methods,clk@fhcrc.org,,Charles Kooperberg,,Fred Hutchinson Cancer Research Center,1100 Fairview Ave N / M3-A410,206-6677808,206-6674142,clk@fhcrc.org,Predictive models for genome-wide association studies,1,Charles,,Kooperberg,Fred Hutchinson Cancer Research Center,Michael,,LeBlanc,Fred Hutchinson Cancer Research Center,Valerie,,Obenchain,Fred Hutchinson Cancer Research Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"One of the objectives of genome-wide association studies (GWAS) isoften to attempt to refine 'risk prediction models' basedon traditional epidemiological data, using SNPs that areassociated with disease. Methods such as the lasso, lars, andthe elastic net have been proposed in recent years toconstruct risk prediction models. While these methodscan deal with many predictors, dealing with the 100,000s ofpredictors of a GWAS is still often beyong the capacity ofthese methods. Some multi-stage strategies, which may influencethe selection of smoothing parameters, are needed. An additionalcomplication is that in GWAS often some interest is ingene x gene and gene x environment interactions. The componentsof these interactions often are multi-dimensional, and requireadditional tools to be combined with individual SNPs in riskprediction models. ",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Genomics,High dimensional data,hwchoi912@gmail.com,,Hyungwon Choi,Post-Doctoral Research Fellow,"Pathology Department, University of Michigan",M4237 A.I. Nesvizhskii Lab,734-474-4407,,hwchoi912@gmail.com,Model-based Validation of Protein-Protein Interactions in Large-Scale Affinity Purification-Mass Spectrometry Experiments,1,Hyungwon,,Choi,University of Michigan,Ashton,,Breitkreutz,"Samuel Lunenfeld Research Institute, Mount Sinai Hospital, Toronto",Brett,,Larsen,"Samuel Lunenfeld Research Institute, Mount Sinai Hospital, Toronto",Anne-Claude,,Gingras,"Samuel Lunenfeld Research Institute, Mount Sinai Hospital, Toronto",Mike,,Tyers,"Wellcome Trust Centre for Cell Biology, University of Edinburgh, UK",Zhaohui,S,Qin,University of Michigan,Alexey,I,Nesvizhskii,University of Michigan,,,,,,,,,,,,,"Large-scale affinity purification (AP) and mass spectrometry (MS) havefacilitated the identification of protein-protein interaction (PPI)networks on the global scale. However, elucidating the complex andtransient nature of the signaling network is challenging if theidentification step results in many false positives. In this work, wepresent a computational approach for model-based assessment ofsignificance of observed interactions using semi-quantitative measuresof absolute protein abundance based on spectral counts. We describe anadvanced probability model-based method, called Significance Analysisof Interactome (SAINT), for selecting high confidence interactionsfrom experimental noise in the AP-MS data. The proposed model wasapplied to a large-scale AP-MS dataset of PPIs in budding yeast wholecell lysate, with a specific focus on the protein kinases. Thereconstructed kinase network from multiple tag experiments is composedof close to 900 proteins with more than 1500 high-probabilityinteractions. As expected in a signaling network, the structure of theidentified network clearly reveals a notably high degree ofconnectivity among kinases. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Proteomics,High dimensional data,dan@stat.duke.edu,,Daniel Merl,,Duke University,Dept Statistical Science,9197244553,,dan@stat.duke.edu,Bayesian nonparametric analysis of site-specific selection effects in serially DNA sequences,1,Daniel,,Merl,Duke University,Raquel,,Prado,"University of California, Santa Cruz",Athanasios,,Kottas,"University of California, Santa Cruz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We present a novel statistical model for investigating the effects ofnatural selection in serially sampled protein coding sequencealignments. The major methodological development here is the use of acombined Dirichlet process and variable selection prior for thedistribution of site-specific selection effects in order to accomplishsimultaneous variable selection and clustering.  The potential ofthese modelling assumptions for use in the problem ofinferring site-specific selection effects is clear: variation at manysites will be adequately explained by the baseline effects associatedwith neutral selection, while those sites that are found to havenon-zero selection effects will be subsequently clustered according toassociation with some number of unique selection profiles.  Wedemonstrate the method using sequence data collected to measure theeffects of nevirapine treatment on the HIV1 polymerase gene.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Bayesian methods,Statistical genetics,hongzhe@upenn.edu,,Hongzhe Li,Professor,Department of Biostatistics,215 Blockley Hall,215 573-5038,,hongzhe@upenn.edu,Methods for Identifying the Genetic Variants Associated with High-order Expression Modules,1,Hongzhe,,Li,University of Pennsylvania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many recent studies have shown that the level of expression of genecan be heritable and many eQTLs have been identified by traditionallinkage/associaiton analysis. These studies mainly focus on theeffects of genetic variants on the levels of expression of genes.However, genetic variants can also affect expression coordinationamong genes and therefore high-order expression modules. This paperproposes a regression-on-correlation (RegCor) model to link thegenetic variants to expression association between two genes in orderto identify the variants that are association with high-orderexpression modules. A score test and a regularized MLE procedure areproposed for hypothesis testing and for selection of genetic variants.Some theoretical results, simulations and application to expressiondata in yeast segregants will be presented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Genomics,High dimensional data,rpeng@jhsph.edu,,Roger Peng,,Johns Hopkins University,615 N Wolfe St E3527,410-955-2468,,rpeng@jhsph.edu,Spatial misalignment in time series studies of air pollution and health data,1,Roger,D,Peng,Johns Hopkins Bloomberg School of Public Health,Michelle,L,Bell,Yale University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"  Time series studies of environmental exposures often involve  comparing daily changes in a toxicant measured at a point in space  with daily changes in an aggregate measure of health.  Spatial  misalignment of the exposure and response variables can bias the  estimation of health risk and the magnitude of this bias depends on  the spatial variation of the exposure of interest.  In air pollution  epidemiology, there is an increasing focus on estimating the health  effects of the chemical components of particulate matter.  One issue  that is raised by this new focus is the spatial misalignment error  introduced by the lack of spatial homogeneity in many of the  particulate matter components.  Current approaches to estimating  short-term health risks via time series modeling do not take into  account the spatial properties of the chemical components and  therefore could result in biased estimation of those risks.  We  present a spatial-temporal statistical model for quantifying spatial  misalignment error and show how adjusted heath risk estimates can be  obtained using a regression calibration approach and a two-stage  Bayesian model.  We apply our methods to a database containing  information on hospital admissions, air pollution, and weather for  20 large urban counties in the United States.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Spatial/temporal modeling,Environmental and ecological applications,rmcroberts@fs.fed.us,,Ronald E. McRoberts,Dr.,"Northen Research Station, U.S. Forest Service",1992 Folwell Avenue,651 649-5174,,rmcroberts@fs.fed.us,Model-based inference for natural resource inventories,1,Ronald,E,McRoberts,"Northern Research Station, U.S. Forest Service",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Natural resources inventory programs have responded to the traditional user question 'How much?' using sample data and probability-based inference.  Increasingly, users also are asking 'Where?' and request maps depicting the spatial distributions of resources and estimates compatible with the maps.  For small area estimation and/or for complex relationships between response and ancillary variables, probability-based inference may be unsuitable.  For these situations, model-based inference that relies on inventory sample plot data and ancillary satellite image data has been found to be useful.  Although maps consisting of model-predictions may be relatively easy to construct, inference compatible with the maps may be much more difficult.  First, whereas probability-based estimates or population parameters are known to be at least asymptotically unbiased, such is not the case for model-based estimates.  Thus, bias assessment becomes crucial for model-based approaches to inference.  Second, computational intensity for even small areas may become prohibitive because of the large number of pixels and necessity of considering correlations among pixel predictions.  The presentation focuses on practical solutions to both problems, bias assessment and computational intensity. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Forestry/agriculture applications,Spatial/temporal modeling,valeri.v.fedorov@gsk.com,,Valerii,Dr.,GSK,1250 S. Collegeville Rd,6109175499,,valeri.v.fedorov@gsk.com,Penalized designs of multi-response experiments,1,Valerii,V,Fedorov,GlaxoSmithKline,Rongmei,,Zhang,GlaxoSmithKline,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The major theme of the presentation is optimal design of dose-response experiments when several (continuous or categorical) potentially correlated responses are observed simultaneously for every experimental unit. While the narration is built around two endpoints the generalization for the higher dimension is also discussed. I also address some ethical aspects of dose response experiments. In the traditional optimal design setting one tries to gain as much information as possible without explicit concern about patients in the trial, i.e. doing what is best for the targeted population (collective ethics). The currently popular procedures gravitate to individual ethics: doing what is best (accordingly to the current knowledge) for a newly arriving patient. To compromise between these two extremes we maximize information per a unit of penalty, which depends on efficacy and toxicity. Necessary and sufficient conditions, algorithms and software are developed and discussed for locally optimal, composite and adaptive designs. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Clinical trials,gareth@usc.edu,,Gareth James,Associate Professor,University of Southern California,512 Hoffman Hall,213 740 9696,,gareth@usc.edu,Forward-Lasso Adaptive SHrinkage,1,Gareth,,James,Univeristy of Southern California,Peter,,Radchenko,Univeristy of Southern California,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Both Forward Selection (FS) and the Lasso can perform variable selection in high dimensional regression problems. Although the Lasso is the solution to an optimization problem while FS is purely algorithmic, the two methods utilize surprisingly similar approaches. Both add to the model the variable which has the highest correlation with the residual vector, the only difference being in the level of shrinkage. We propose a new method, Forward-Lasso Adaptive SHrinkage (FLASH), which incorporates both FS and the Lasso as special cases. FLASH is fitted using a variant of the LARS algorithm and works well in situations where neither Lasso nor FS succeeds. We prove that it can be formulated as the solution to a weighted Lasso optimization problem. We also demonstrate, on an extensive set of simulations and a real world data set, that FLASH generally outperforms many competing approaches.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Variable subset selection/model selection,finleya@msu.edu,,Andrew O. Finley,Dr.,"Department of Forestry, Michigan State University",126 Natural Resources Building,517-432-7219,,finleya@msu.edu,Hierarchical spatial models with remotely sensed predictors for mapping tree species assemblages across large domains,1,Andrew,O,Finley,"Department of Forestry and Department of Geography, Michigan State University",Sudipto,,Banerjee,"Division of Biostatistics, University of Minnesota",Ronald,E,McRoberts,"Northern Research Station, USDA Forest Service",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Spatially explicit data layers of tree species assemblages, referredto as forest types or forest type groups, are a key component inlarge-scale assessments of forest sustainability, biodiversity, timberbiomass, carbon sinks, and forest health monitoring.  This talkexplores the utility of coupling georeferenced national forestinventory (NFI) data with readily available and spatially completeenvironmental and remotely sensed predictor variables throughmultinomial probit regression with varying levels of random spatialeffects to predict forest type groups across large forestedlandscapes. Hierarchical models with spatially varying coefficientsthat exploit the spatial proximity of the NFI plot array andnon-stationarity of predictor variables are proposed to improve theaccuracy and precision of forest type group classification atlocations where we have observed predictors but not inventory plots.The richness of these models, however, incurs onerous computationalburdens and we need to resort dimension reducing spatial processeswithout sacrificing the richness in modeling. We illustrate using NFIdata from Michigan, USA, where we provide a comprehensive analysis ofthis large study area and demonstrate improved classification withassociated measures of uncertainty.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Spatial/temporal modeling,fanyingy@marshall.usc.edu,,Yingying Fan,,University of Southern California,Information and Operations Management Department,2137409916,,fanyingy@marshall.usc.edu,Estimation in Additive Models with Highly Correlated Covariates,2,Jiancheng,,Jiang,"Department of Mathematics and Statistics, University of North Carolina at Charlotte",Yingying,,Fan,"Information and Operations Management Department, University of Southern California",Jianqing,,Fan,"Department of Operations Research & Financial Engineering, Princeton University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Motivated by normalizing Affymetrix array data, we explorenonparametric estimation of highly correlated confounding effectswhile leaving the high-dimensional treatment effects as nuisanceparameters.  A simple difference operation reduces the problem toadditive models with highly correlated covariates.  We introduce twonovel approaches for estimating the nonparametric components,integration estimation and pooled backfitting estimation. The formeris designed for highly correlated intensity effects, and the latter isuseful for non-highly correlated intensity effects. Asymptoticnormalities of the proposed estimators are established. Simulationsare conducted to demonstrate finite sample behaviors of the proposedmethods.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Nonparametric methods,Microarray analysis,Additive modelmdaniels@stat.ufl.edu,,Michael Daniels,Professor,Department of Statistics,University of Florida,352 273 1845,352 392 5175,mdaniels@stat.ufl.edu, Bayesian Semiparametric Selection Models with Application to a Breast Cancer Prevention Trial,2,Chenguang,,Wang,"Department of StatisticsUniversity of Florida",Michael,,Daniels,"Department of StatisticsUniversity of Florida",Daniel,,Scharfstein,"Department of BiostatisticsThe Johns Hopkins University",J,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider inference for the treatment difference of outcomes fromlongitudinal cancer studies, in which repeatedly measured outcomes maybe informatively missing dueto drop out (withdraw of consent or loss to follow-up) orprotocol-defined events (progression ordeath). It is known that in the presence of informative missingness,the treatment difference is notidentifiable unless unverifiable assumptions are made. We posit a highdimensional semiparametricselection model that uses a semiparametric missing data mechanism(MDM) and a nonparametric(saturated) model for the full-data response distribution.  This modelis usefulfor conducting sensitivity analysis and incorporating expert opinionabout the missingness. Wepropose reducing the dimensionality by introducing shrinkage priorsfor the high order interactionsin the full-data response and MDM models. We explore the degree ofidentification ofpotential sensitivity parameters by the data and develop a new way toelicit response-specific priordistributions using expert opinions.  This modeling approachis applied to an analysis of data from the Breast Cancer PreventionTrial (BCPT).",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Bayesian methods,Missing data,rasmus_nielsen@berkeley.edu,,Rasmus Nielsen,,"Department of Statistics, UC-Berkeley",367 Evans Hall,5106428752,,rasmus_nielsen@berkeley.edu,Bayesian Association Mapping,2,Anders,,Albrechtsen,"Department of Biostatistics, University of Copenhagen",Rasmus,,Nielsen,"Department of Statistics, UC-Berkeley",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For most common diseases with heritable components, not a single or a few single-nucleotide polymorphisms (SNPs) explain most of the variance for these disorders. Instead, much of the variance may be caused by interactions (epistasis) among multiple SNPs or interactions with environmental conditions. We present a new powerful statistical model for analyzing and interpreting genomic data that influence multifactorial phenotypic traits with a complex and likely polygenic inheritance. The new method is based on Markov chain Monte Carlo (MCMC) and allows for identification of sets of SNPs and environmental factors that when combined increase disease risk or change the distribution of a quantitative trait. Using simulations, we show that the MCMC method can detect disease association when multiple, interacting SNPs are present in the data. When applying the method on real large-scale data from a Danish population-based cohort, multiple interactions are identified that severely affect serum triglyceride levels in the study individuals. The method is designed for quantitative traits but can also be applied on qualitative traits. It is computationally feasible even for a large number of possible interactions and differs fundamentally from most previous approaches by entertaining nonlinear interactions and by directly addressing the multiple-testing problem.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Statistical genetics,Invited session on: New Statistical Methods for Detecting Epistasis Interactions inbbekele@mdanderson.org,,B. Nebiyou Bekele,Associate Professor,M. D. Anderson Cancer Center,1400 Pressler St.,713-563-4282,713-563-4243,bbekele@mdanderson.org,Adaptive randomization for multi-arm comparative clinical trials based on joint efficacy/toxicity outcome,1,B. Nebiyou,,Bekele,M. D. Anderson Cancer Center,Yuan,,Ji,M. D. Anderson Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"An outcome-adaptive randomization scheme for comparative clinicaltrials in which the primary endpoint is a joint efficacy/toxicityoutcome is presented. Under the proposed scheme, the randomizationprobabilities are unbalanced adaptively in favor of treatments withsuperior joint outcomes characterized by higher efficacy and lowertoxicity. This type of scheme is advantageous from the patients'perspective since on average, more patients are randomized to superiortreatments. We extend the approximate Bayesian time-to-event model inCheung and Thall (2002) to model the joint efficacy/toxicity outcomesand perform posterior computation based on a latent variable approach.Consequently, this allows us to incorporate essential informationabout patients with incomplete follow-up. Based on the computedposterior probabilities, we propose an adaptive randomization schemethat favors the treatments with larger joint probabilities of efficacyand no toxicity. We illustrate our methodology with a leukemia trialthat compares three treatments in terms of their 52-week molecularremission rates and 52-week toxicity rates. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Bayesian methods,wiklec@missouri.edu,,Chris Wikle,,University of Missouri,146 Middlebush Hall,573-882-9659,,wiklec@missouri.edu,A Bayesian Bioclimate Model for the Lower Trophic Ecosystem in the North Pacific Ocean,1,Christopher,K,Wikle,University of Missouri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A wide variety of physical-biological models of varying complexityhave been developed for components of the U.S. West Coast upwellingecosystem.  For example, the physical-biological interface iseffective for demonstrating high-resolution properties ofphytoplankton bloom and zooplankton population dynamical response. Wedevelop a simple stochastic three-component model (nitrogen,phytoplankton, zooplankton) in a hierarchical Bayesian framework. Theunderlying system is multivariate and highly nonlinear. The model isdemonstrated for the upwelling region of the North Pacific. Theexample demonstrates the interplay between model formulation, data,and interpretation of posterior distributions in relatively simplestochastic models of a complicated physical-biological system.Ultimately, such models can be used for pan-regional syntheses andclimate change impact studies for coastal ocean ecosystems.",FALSE,FALSE,,FALSE,FALSE,TRUE,roundtable luncheon host,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Spatial/temporal modeling,hjanes@scharp.org,,Holly Janes,,Fred Hutchinson Cancer Research Center,1100 Fairview Ave N M2-C200,206 667 6353,,hjanes@scharp.org,Calculating disease risk: Evaluating risk prediction models using risk stratification tables,1,Holly,,Janes,Fred Hutchinson Cancer Research Center,Margaret,S,Pepe,Fred Hutchinson Cancer Research Center,Jessie,,Gu,University of Washington,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"A multitude of new markers are being evaluated in the hopes that theymay aid patients and clinicians in predicting an individual's risk ofdisease.  A new approach to evaluating these markers, called the riskstratification approach, was recently proposed by Nancy Cook andcolleagues (Cook 2006).  This involves cross-tabulating riskpredictions on the basis of models with and without the new biomarker,and has been widely adopted in the literature.  We argue thatimportant information with regard to three important model validationcriteria can be extracted from risk stratification tables: 1) modelfit or calibration; 2) capacity for risk stratification; and 3)accuracy of classifications based on risk.  However, we also cautionagainst misuses of the method.  For example, risk stratificationtables are not useful for comparing non-nested risk prediction models,nor can they be directly applied to evaluate models fit tocase-control data.  We provide alternative suggestions for evaluatingrisk prediction models in these settings.  The concepts areillustrated using models for predicting breast cancer risk.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Other,Diagnostic and screening tests,Risk predictiontimothy.gregoire@yale.edu,,Timothy G Gregoire,Professor,Yale University,360 Prospect Street,203-432-9398,203-432-3809,timothy.gregoire@yale.edu,A combined design and model-based derivation of the MSE of estiamted aboveground biomass from profiling airborne laser system,1,Timothy,G,Gregoire,Yale University,Ross,F,Nelson,NASA-Goddard Space Flight Center,Erik,,Naesset,Norwegian University of Life Sciences,Goran,,Stahl,Swedish University of Agricultural Sciences,Terje Gobakken,,,Norwegian University of Life Sciences,,,,,,,,,,,,,,,,,,,,,"Profiling airborne laser altimetry has been used to discern a height profile of forests over an extensive area. When coupled with ground sampling within a double sampling framework, an estimator of aboveground forest biomass is obtained. The estimator is affected by multiple sources of statistical error stemming not only from the design but also from a reliance on statistical models to predict individual tree biomass and to link the remotely sensed data to the ground sample. Accounting for these errors when deriving the design-cum-model based mean square error presents some intriguing challenges which are the focus of this presentation.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Forestry/agriculture applications,Environmental and ecological applications,aroyle@usgs.gov,,J Andrew Royle,Research Statistician,USGS Patuxent Wildlife Research Center,12100 Beech Forest Rd.,3014975846,,aroyle@usgs.gov,Hierarchical Models for Spatial Capture-Recapture Data,1,J. Andrew,,Royle,USGS Patuxent Wildlife Research Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Estimating  density  is an objective  of many  animal  populationstudies. Methods for estimating population size from ostensiblyclosed  populations are widely used,  but  ineffective  for estimating  absolutedensity  because  the area sampled is difficult to quantify as a resultof movement of individuals.A number of ad hoc methods for 'adjusting' estimates of nominal sample areaare inwidespread use. In this paper I describe a hierarchical  modeling  framework for estimating density from spatialcapture-recapture  data.  The  hierarchical  model  consists of aprocess model, describing the locations of individuals (or their home ranges)and their exposure and detection due to movement. The observation  model  describes  the  probability  of encounter as a function ofindividual location.I develop a framework for Bayesian  analysis of this class of modelsbased on data augmentation, which allows for a straightforward  implementationin the freely  available  software  WinBUGS or in R. Applications of the model to camera trapping studies are given.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Environmental and ecological applications,Bayesian methods,herve.cardot@u-bourgogne.fr,,Cardot,Professor,Universite de Bourgogne,"9, Av. Alain Savary",(0)3 80 45 06 88,,herve.cardot@u-bourgogne.fr,Functional Principal Components Analysis with Survey Data,1,Herve,,CARDOT,"Institut de Mathematiques, Universite de Bourgogne",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"This talk aims at presenting how functional principal componentsanalysis can be performed when curve data are collected with surveysampling strategies. We consider  estimators based on theHorvitz-Thompson approach and derive asymptotic properties  of themean function and the eigenelements of the covariance function with the help of the influence function. We also prove that we can getconsistent estimators of the asymptotic variance.   A simulationstudy, which focuses on stratified sampling,  allows to confirm thegood properties of our estimators.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Functional data analysis,Survey research data,aivanova@bios.unc.edu,,Anastasia Ivanova,Associate Professor,University of North Carolina at Chapel Hill,CB# 7420,919 843 8086,,aivanova@bios.unc.edu,Finding the Dose with the Best Efficacy/Tolerability Profile,1,Anastasia,,Ivanova,University of North Carolina at Chapel Hill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The goal of a phase II dose-finding study is to find the best dose toinvestigate in subsequent trials. Such a dose should have goodefficacy, which is often established by comparing the efficacy of thedrug with placebo and/or active control. Doses with high efficacymight have higher rates of adverse events, hence both outcomes shouldbe taken into consideration for dose selection. We present anefficient adaptive dose-finding strategy to estimate the dose whichhas the best efficacy/tolerability profile.  ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Adaptive design/adaptive randomization,Consulting,lingyunliu@northwestern.edu,,Lingyun Liu,,Northwestern University,2006 Sheridan Rd.,312-593-5247,,lingyunliu@northwestern.edu,Testing multiple endpoints in group sequential designs,3,Ajit,,Tamhane,Northwestern University,Cyrus,,Mehta,Cytel,Lingyun,,Liu,Northwestern University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FALSE,FALSE,,FALSE,FALSE,FALSE,,,,,,FALSE,FALSE,FALSE,FALSE,,,xsliu@jimmy.harvard.edu,,Xiaole Shirley Liu,Dr,"Dana-Farber Cancer Insitute, Harvard School of Pub","44 Binney St, CLSB11022",617-632-2472,,xsliu@jimmy.harvard.edu,Integrative modeling of transcription and epigenetic regulation,1,Xiaole Shirley,,Liu,Department of Biostatistics and Computational Biology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"High throughput chromatin immunoprecipitation experiments (ChIP-chipand ChIP-seq) have accelerated transcription regulation studies, yetcreated challenges for data analyses. I will discuss a few algorithmswe have developed for the analysis of ChIP-seq, and report someobservations about epigenetic regulation. I will also discuss anintegrative algorithm we used to assign regulated gene targets andpredict gene expression changes from binding.",FALSE,FALSE,T2: Introduction to Bayesian Analysis,FALSE,FALSE,FALSE,"This is an invited oral presentation, which is due 11/15. I am sorry I submitted the abstract late. ",oral_presentation,,,,FALSE,FALSE,FALSE,TRUE,Genomics,Computational methods,vickersa@mskcc.org,,Andrew Vickers,Associated Attending Research Methodologist,Memorial Sloan-Kettering Cancer Center,1275 York Avenue,646 735 8142,,vickersa@mskcc.org,"Decision curve analysis: a simple, novel method for the evaluation of prediction models, diagnostic tests and molecular markers",1,Andrew,J,Vickers,Memorial Sloan-Kettering Cancer Center,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Prediction models and molecular markers and models are currently evaluated in terms of accuracy, using metrics such as sensitivity and specificity or the area-under-the-curve (AUC). A model is thought to be a good one if it is accurate; a marker is claimed to be of value if it increases accuracy of clinical predictors. However, it is unclear exactly how high an AUC, or how great an increment in AUC, is sufficient to justify clinical use of a model or marker. Reclassification metrics similarly fail to give a clear answer as to clinical value: how much reclassification is enough to warrant measuring a marker?. Evaluating models and markers in terms of clinical consequences is the remit of a field known as 'decision analysis'. The drawback of traditional decision analysis is that it requires additional information, for example, on the benefits, harms and costs of treatment, or on patient preferences for different health states.Decision curve analysis is a simple, decision-analytic method that can be directly applied to the data set of a model or marker, without the need for external data. Because the method is decision analytic, it can be used to tell us whether or not to use a model in the clinic, or whether a marker is worth measuring. Further information on decision curve analysis can be found at www.decisioncurveanalysis.org",TRUE,FALSE,,FALSE,FALSE,FALSE,,oral_presentation,,,,FALSE,FALSE,FALSE,FALSE,Other,ROC analysis,IMS on Evaluating Markers for Risk Predictionlingyunliu@northwestern.edu,,Lingyun Liu,,Northwestern University,2006 Sheridan Rd.,312-593-5247,,lingyunliu@northwestern.edu,Testing Primary and Secondary Endpoints in a Group Sequential Design,3,Ajit,,Tamhane,Northwestern University,Cyrus,,Mehta,Cytel,Lingyun,,Liu,Northwestern University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We consider a clinical trial with one primary and one secondaryendpoint where the secondary endpoint is tested only if the primaryendpoint demonstrates significance. The trial uses a group sequentialdesign with two looks. The type I error rate for the primary endpointis controlled by choosing any \alpha-level boundary, e.g., the standardO'Brien-Fleming or the Pocock boundary.  Given any particular primaryboundary, we study the problem of determining the secondary boundaryto control the FWER when the primary null hypothesis is false and thesecondary null hypothesis is true. We study this FWER analytically andnumerically and find that it is maximized when the correlationcoefficient Á between the two endpoints equals 1. We also characterizethe value of the primary mean that maximizes the FWER; For theO'Brien-Fleming and Pocock choices for the primary boundary, thecritical constants, using either the former or the latter choice forthe secondary boundary, are computed for different values of \rho.Numerical studies indicate that the O'Brien-Fleming choice for theprimary boundary and Pocock choice for the secondary boundarygenerally gives the best secondary power performance among the fourcombinations of these two boundaries that were studied.",FALSE,FALSE,,FALSE,FALSE,FALSE,,presentation_or_poster,,,,FALSE,FALSE,FALSE,TRUE,Clinical trials,Multiple testing,thsing@umich.edu,,Tailen Hsing,Professor,Department of Statistics,439 West Hall  1085 South University,734 657 8820,734 763 4676,thsing@umich.edu,Deciding the Dimension of Effective Dimension Reduction Space for Functional Data,2,Yehua,,Li,"Department of Statistics, University of Georgia",Tailen,,Hsing,"Department of Statistics, University of Michigan",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"In this talk, we discuss regression models with a functional predictorand a scalar response, where the response depends on the predictoronly through a finite number of projections. The linear subspacespanned by these projections is called the effective dimensionreduction (EDR) space. To determine the dimensionality ofthe EDR space, we focus on the principal component scores of thefunctional predictor, and propose three sequential testing proceduresunder the assumptionthat the predictor has an elliptically contoured distribution. Theproposed procedures are supported by theory and validated by asimulation study. Applications on two real data sets will also bepresented.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Functional data analysis,Multivariate methods,nychka@ucar.edu,,Douglas Nychka,,National Center for Atmospheric Research,PO Box 3000,3034971711,,nychka@ucar.edu,The other world of large spatial data sets,1,Douglas,,Nychka,NCAR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Many problems in analyzing the Earth's climate depend on large spatial data sets. These problems typically break standard and exact methods for spatial statistics and approximate approaches are needed. This talk will use a suite of regional climate simulations to illustrate some techniques for introducing sparsity into covariance modelsand the use of conditional simulation for inference. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Spatial/temporal modeling,Computational methods,michael.schimek@donau-uni.ac.at,,Michael G. Schimek,Professor,Danube University Krems,Center for Bioinformatics and Biostatistics,+432732-893-2601,,michael.schimek@donau-uni.ac.at,A graphical solution for the multiple top-k list problem with applications in molecular medicine,1,Michael,G.,Schimek,"Danube University, Krems, Austria",Eva,,Budinska,"Masaryk University, Brno, Czech Repulic",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"For a fixed set of objects, let us have L lists representing theirindividual rank positions. For many studies in molecular medicine itis typical that the probability for consensus rankings is decreasingwith increasing distance from the top rank position. An important taskis to consolidate such lists, i.e. to identify those objects that arecharacterized by rankings of high conformity across the assessments upto position k. HALL and SCHIMEK (COMPSTAT 2008 Proceedings, p.433-444)have addressed this inference problem for the case of two assessors.They could develop a moderate deviation-based procedure for randomdegeneration in paired rank lists. For each pairwise comparison of theL lists a specific k-value is obtained. These k-values cansubstantially vary because the degree of correspondence between thepartial lists is in practice not high due to various irregularities ofthe assessments. The integration of the thus obtained results into acommon set of conforming objects is highly dependent on technicalassumptions, and the choices of the tuning parameters. Therefore, itwould be of great value to have a graphical representation that allowsus to monitor the integration process. The development and applicationof such a graphical tool is described in this paper. ",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Other,High dimensional data,Data integrationshili@stat.osu.edu,,Shili Lin,Professor,Ohio State University,Department of Statistics,614-292-7404,,shili@stat.osu.edu,Finite Mixture of Sparse Normal Linear Models in High Dimensional Feature Space with Applications to Genomics Data,1,Shili,,Lin,The Ohio State University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"The development of modern technology has led to the generation of high dimensional data in many areas of scientific research such as statistical genomics. In such applications, one is often interested in studying the relationship between a response variable and a high dimensional vector of features. However, it is often the case that only a small number of the features contribute to the response variable, and this leads to the problem of sparse regression modeling. Such data may also exhibit heterogeneity. As such, each homogeneous sub-population rather than the entire population may be suitably modeled by a linear regression. In such situations, finite mixture of regression (FMR) models provide a powerful tool for learning the structure of the data. In this talk, I will outline a two-step procedure for treating this problem. The first step is to reduce the high dimensionality of the original feature space to a relatively lower dimension. Then we learn about the sparse Normal finite mixture based on a FMR modeling approach. We will demonstrate this approach with an application to a genome wide association study with single nucleotide polymorphism data. This is join work with Drs. Abbas Khalili and Jiahua Chen.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,High dimensional data,Statistical genetics,jliu@stat.harvard.edu,,Jun Liu,Professor,Harvard University,Department of Statistics,617-495-1600,,jliu@stat.harvard.edu,Bayesian Detection of Gene-Gene Interactions Associated with Type 1 Diabetes within MHC region,3,Yu,,Zhang,"Department of Statistics, Penn State University",Jing,,Zhang,"Department of Statistics, Harvard University",Jun,S,Liu,"Department of Statistics, Harvard University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We propose a Bayesian model for simultaneously inferring haplotype-blocks and selecting SNPs within blocks that are associated with the disease, either singly, or through epistatic interactions with other markers. Simulation results show that this approach is uniformly more powerful than our previous BEAM algorithm for epistasis mapping and other methods. We applied the method to WTCCC type 1 diabetes data, and discovered some interesting two-way interactions within the MHC region on chromosome 6. We found very strong interactions within and between (HLA-) DQ-DR region and its upstream region from TNXB to BTNL2. Our results indicate that the LD structure in the MHC region may have changed substantially in cases compared with that in controls.",FALSE,FALSE,,FALSE,FALSE,TRUE,"I have to teach 3 classes on Tuesdays. If possible, could you arrange the session on Sunday or Monday? thanks.",invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Statistical genetics,Bayesian methods,cdlinkle@stat.brown.edu,,Crystal Linkletter,,Brown University,Box G-S121-7,401-863-6321,,cdlinkle@stat.brown.edu,Neural Functional Connectivity Networks,1,Crystal,,Linkletter,Brown University,Hernando,,Ombao,Brown University,Mark,,Fiecas,Brown University,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Network models are increasingly being used to represent structure in complex networks. One application of such models is to explore the impact of networks as substrates for dynamic processes (e.g. the spread of an infectious disease). That is, given a network which constrains the flow of the process, the resulting behavior is explored. In this talk, we consider the inverse problem: Can observing a process happening on a network inform us about the underlying network? In neuroscience, functional connectivity between different regions of the brain can be represented as a network. The presence of correlation between fMRI scans taken at two regions is indicative of functional connectivity between those regions. We propose a model-based approach to estimating functional connectivity in the brain based on the premise of uncovering underlying latent network structure.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Graphical models,Latent variables,zhou@bios.unc.edu,,Haibo,Professor,"Dept of Biostatistics, University of North Carolin","CB# 7420, McGarvran Greenburg Hall",919-966-3885,919-966-3804,zhou@bios.unc.edu,Partial Linear Model for Data from an Outcome dependent sampling design,1,Haibo,,Zhou,"Dept of BiostatisticsUniversity of North Carolina at Chapel Hill",Guoyou,,Qin,"Dept of BiostatisticsUniversity of North Carolina at Chapel HillDepartment of BiostatisticsFudan University",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Outcome-dependent sampling (ODS) has beenwidely used in biomedical studies because it is a cost effective wayto improve study efficiency. However, the models considered in theliterature are limited to the framework of linear models due to thechallenge in terms of both theory and computation. Partial linearmodel (PLM) is a powerful inference tool to nonparametrically modelthe relation between an outcome and exposure variables. In thisarticle, we consider a partial linear model for data from an ODSdesign. We propose a semiparametric maximum likelihood method toachieve the inference of PLM. We develop the asymptotic propertiesand  simulation studies show that the ODS design can produce moreefficient estimate than the traditional simple random samplingdesign with the same sample size. We demonstrate the proposed method via a real data set analysis.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Nonlinear models,Nonparametric methods,prathouz@uchicago.edu,,Paul Rathouz,Associate Professor,"Department of Health Studies, University of Chicag","5841 S. Maryland, MC 2007",773 834 1970,773 702 1979,prathouz@uchicago.edu,Longitudinal studies of binary response data following case-control and stratified case-control sampling: Design and analysis,2,Jonathan,S,Schildcrout,"Department of Biostatistics, Vanderbilt University School of Medicine",Paul,J,Rathouz,"Department of Health Studies, University of Chicago",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We discuss design and analysis of longitudinal studies after case-control sampling, wherein interest is in the relationship between a longitudinal binary response that is related to the sampling (case-control) variable, and a set of covariates.  We propose a semiparametric modelling framework based on a longitudinal GEE response model and an ancillary model for subjects' case-control status.  In this approach, the analyst must posit the population prevalence of being a case, which is then used to compute an offset term in the ancillary model.  Parameter estimates from this model are used to compute offsets for the longitudinal response model. Examining the impact of population prevalence and ancillary model misspecification, we show that time-invariant covariate parameter estimates, other than the intercept, are reasonably robust, but intercept and time-varying covariate parameter estimates can be sensitive to such misspecification.  We study design and analysis issues impacting study efficiency, namely: choice of sampling variable and the strength of its relationship to the response, sample stratification, choice of working covariance weighting, and degree of flexibility of the ancillary model.  The research is motivated by a longitudinal study following case-control sampling of the time course of ADHD symptoms.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Longitudinal data,Epidemiologic methods,Sergei.2.Leonov@gsk.com,,Sergei Leonov,"Director, Research Statistics",GlaxoSmithKline,1250 South Collegeville Rd,(610) 917-5218,(610) 917-7494,Sergei.2.Leonov@gsk.com,An Adaptive Optimal Design for the Emax Model and Its Application in Clinical Trials,1,Sergei,,Leonov,GlaxoSmithKline,Sam,,Miller,GlaxoSmithKline,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"We discuss an adaptive design for a first-time-in-humans dose-escalation study. The project team wished to maximize the efficiency of the study by using doses targeted at maximizing information about the dose-response relationship within certain safety constraints. We have developed a GUI-based adaptive optimal design tool to recommend doses when the response follows an Emax model, with functionality for pre-trial simulation and in-stream analysis. We present the results of a simulation to investigate the operating characteristics of the applied algorithm and discuss potential extensions for other models.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,TRUE,Experimental design,Adaptive design/adaptive randomization,lin@bios.unc.edu,,Danyu Lin,,UNC,Dept of Biostatistics,(919) 843-5134,,lin@bios.unc.edu,Estimating Genetic Effects and Gene-Environment Interactions With Missing Data,1,Danyu,,Lin,University of North Carolina,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Missing data arise in genetic association studies when genotypes areunknown or when haplotypes are of direct interest. We provide ageneral likelihood-based framework for estimating genetic effects andgene-environment interactions with such missing data. We allow geneticand environmental variables to be correlated while leaving thedistribution of environmental variables completely unspecified. Weconsider three major study designs --- cross-sectional, case-control,and cohort designs --- and construct appropriate likelihood functionsfor all common phenotypes (e.g., case-control status, quantitativetraits, and potentially censored ages at onset of disease). Thelikelihood functions involve both finite- and infinite-dimensionalparameters. The maximum likelihood estimators are shown to beconsistent, asymptotically normal, and asymptotically efficient. Fastand stable numerical algorithms are developed to implement thecorresponding inference procedures. Extensive simulation studiesdemonstrate that the proposed inferential and numerical methodsperform well in practical settings. Applications to two genomewideassociation studies are provided. ",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Statistical genetics,Genomics,wengu@u.washington.edu,,Wen Gu,,University of Washington,"2341 NE 55th Ave, Apt 104",2067241917,,wengu@u.washington.edu,Estimating the capacity for improvement in risk prediction with a marker,1,Wen,,Gu,"University of Washington andFred Hutchinson Cancer Research Center",Margaret,,Pepe,"University of Washington andFred Hutchinson Cancer Research Center",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Consider a set of baseline predictors X to predict a binary outcome D and let Y be a novel marker or predictor. This talk is concerned with evaluating the performance of the augmented risk model P(D = 1|Y,X) compared with the baseline model P(D = 1|X). The diagnostic likelihood ratio, DLRX(y), quantifies the change in risk obtained with knowledge of Y = y for a subject with baseline risk factors X. The notion is commonly used in clinical medicine to quantify the increment in risk prediction due to Y. It is contrasted here with the notion of covariate adjusted effect of Y in the augmented risk model. We also propose methods for making inference about DLRX(y). Case-control study designs are accommodated. The methods provide a mechanism to investigate if the predictive information in Y varies with baseline covariates. In addition, we show that when combined with a baseline risk model and information about the population distribution of Y given X, covariate specific predictiveness curves can be estimated. These curves are useful to an individual in deciding if ascertainment of Y is likely to be informative or not for him. Data from two studies, one is a study of the performance of hearing screening tests for infants; the other concerns the value of serum creatinine in diagnosing renal artery stenosis, will be used to illustrate the methodology.",TRUE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Biomarkers/surrogate markers,ROC analysis,chattern@mail.nih.gov,,Nilanjan Chatterjee,,National Cancer Institute,6120 Executive Blvd,301-402-7933,,chattern@mail.nih.gov,Detecting gene-gene interactions using genome-wide association studies in presence of population stratification,1,Samsiddhi,,Bhattacharjee,National Cancer Institute,Nilanjan,,Chatterjee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Some existing approaches to measure gene-gene interaction such as the case-only approach make a crucial assumption of gene-gene independence for physically distant genes. While this strategy is known to increase power considerably, it is prone to significant bias when the independence assumption is violated. To solve this problem, we use the idea of genetic matching, which has been proposed recently to control for stratification in a different context. Cases and controls are matched based on ancestry inferred from genome-wide null markers. Matched sets are then analyzed using extensions of conditional logistic regression that derive additional power from the gene-gene independence assumption. We compare our approach to some of the existing methods in terms of bias and efficiency, both using a GWAS data on prostate cancer from the CGEMS study and simulations under varying degrees of stratification.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Genomics,Cancer applications,heagerty@u.washington.edu,,Patrick Heagerty,Professor,University of Washington,Box 357232,206-616-2720,,heagerty@u.washington.edu, Longitudinal analysis of surgical trials with non-compliance,1,Patrick,J,Heagerty,"Department of BiostatisticsUniversity of Washington",Colleen,,Sitlani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Randomized surgical trials with the goal of evaluating the long-termbenefit of surgical intervention as compared to a non-surgical treatment are often faced with serious patient non-compliance.  Frequently subjectsassignedto surgery delay or subsequently refuse surgery, while non-surgicalsubjectsmay ultimately seek and receive surgery.  There are several statistical challenges associated with longitudinal 'as-treated' analyses thatseek to estimate average causal effects attributable to surgery.  In thispresentationwe adopt an underlying longitudinal causal mixed model that is a natural example of a structural nested mean model, and then compare theperformance of alternative analysis methods when endogenous processeslead to patient crossover (e.g. from non-surgical to surgical). Standard linear mixed models may not be valid yet can perform surprisingly well when selection bias is modest and non-differential.  In contrast, Causal estimation methods such as G-estimation and instrumental variable approached can be valid and their implementation in this setting will be reviewed.",FALSE,FALSE,,FALSE,FALSE,FALSE,,invitedpaper,,,,FALSE,FALSE,FALSE,FALSE,Longitudinal data,"Biologics, pharmaceuticals, medical devices",